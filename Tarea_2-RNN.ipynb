{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Z0tyIsliieNr"
   },
   "source": [
    "# Tarea 2 - Named Entity Recognition\n",
    "\n",
    "----------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-22T15:21:15.416464Z",
     "start_time": "2020-06-22T15:21:15.411478Z"
    },
    "colab_type": "text",
    "id": "X3QUWWoWaSE3"
   },
   "source": [
    "- **Nombres:** Mario Vicuña, Miguel Videla\n",
    "\n",
    "- **Usuario o nombre de equipo en Codalab:** TeamChalla\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "W_dxGEs3iiau"
   },
   "source": [
    "\n",
    "## Introducción a la tarea\n",
    "\n",
    "### Objetivo\n",
    "\n",
    "\n",
    "El objetivo de esta tarea es resolver una de las tasks mas importantes de Sequence Labelling: [Named Entity Recognition (NER)](http://www.cs.columbia.edu/~mcollins/cs4705-spring2019/slides/tagging.pdf). \n",
    "\n",
    "En particular, deberán participar, al igual que en la tarea anterior, en una competencia en donde deberán crear distintos modelos que apunten a resolver NER en español. Para esto, les proveeremos un dataset de NER de noticias etiquetadas en español mas este baseline en donde podrán comenzar a trabajar. \n",
    "\n",
    "Esperamos que (por lo menos) utilizen Redes Neuronales Recurrentes (RNN) para resolverla. Nuevamente, hay total libertad para utilizar software y los modelos que deseen, siempre y cuando estos no traigan los modelos ya implementados (como el caso de spacy).\n",
    "\n",
    "\n",
    "**¿Qué es Sequence Labelling?** \n",
    "\n",
    "En breves palabras, dada una secuencia de tokens (frase u oración) sequence labelling tiene por objetivo asignar una etiqueta a cada token de dicha secuencia.\n",
    "\n",
    "**Named Entity Recognition (NER)**\n",
    "\n",
    "Esta tarea consiste en localizar y clasificar los tokens de una oración que representen entidades nombradas. Es decir, tokens que simbolicen (1) **personas**, (2) **organizaciones**, (3) **lugares** y (4) **adjetivos, eventos y otras entidades que no entren en las categorías anteriores** deberán ser taggeados como (1) **PER**, (2) **ORG**, (3) **LOC** y (4) **MISC** respectivamente. Adicionalmente, dado que existen entidades representadas en más de un token (como La Serena), se utiliza la notación BIO como prefijo al tag: Beginning, Inside, Outside. Es decir, si encuentro una entidad, el primer token etiquetado será precedido por B, el segundo por I y los n restantes por I. Por otra parte, si el token no representa ninguna entidad nombrada, se representa por O. Un ejemplo de esto es:\n",
    "\n",
    "Por ejemplo:\n",
    "\n",
    "```\n",
    "Felipe B-PER\n",
    "Bravo I-PER\n",
    "es O\n",
    "el O\n",
    "profesor O\n",
    "de O\n",
    "PLN B-MISC\n",
    "de O\n",
    "la O\n",
    "Universidad B-ORG\n",
    "de I-ORG\n",
    "Chile I-ORG\n",
    ". O\n",
    "```\n",
    "\n",
    "Estos links son los más indicados para comenzar:\n",
    "\n",
    "-  [Tagging, and Hidden Markov Models ](http://www.cs.columbia.edu/~mcollins/cs4705-spring2019/slides/tagging.pdf) (slides by Michael Collins), [notes](http://www.cs.columbia.edu/~mcollins/hmms-spring2013.pdf), [video 1](https://youtu.be/-ngfOZz8yK0), [video 2](https://youtu.be/PLoLKQwkONw), [video 3](https://youtu.be/aaa5Qoi8Vco), [video 4](https://youtu.be/4pKWIDkF_6Y)       \n",
    "-  [Recurrent Neural Networks](slides/NLP-RNN.pdf) | [video 1](https://youtu.be/BmhjUkzz3nk), [video 2](https://youtu.be/z43YFR1iIvk), [video 3](https://youtu.be/7L5JxQdwNJk)\n",
    "\n",
    "\n",
    "Recuerden que todo el material se encuentra disponible en el [github del curso](https://github.com/dccuchile/CC6205)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iWlfabmkaSE7"
   },
   "source": [
    "### Reglas de la tarea"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-22T15:33:49.808401Z",
     "start_time": "2020-06-22T15:33:49.798428Z"
    },
    "colab_type": "text",
    "id": "3w9Dw4CSaSE8"
   },
   "source": [
    "Algunos detalles de la competencia:\n",
    "\n",
    "- Para que su tarea sea evaluada, deben participar en la competencia como también, enviar este notebook con su informe.\n",
    "- Para participar, deben registrarse en la competencia en Codalab en grupos de máximo 2 alumnos. Cada grupo debe tener un nombre de equipo. (¡Y deben reportarlo en su informe!)\n",
    "- Las métricas usadas serán Precisión, Recall y F1.\n",
    "- En esta tarea se recomienda usar GPU. Pueden ejecutar su tarea en colab (lo cual trae todo instalado) o pueden intentar correrlo en su computador. en este caso, deberá ser compatible con cuda y deberán instalar todo por su cuenta.\n",
    "- En total pueden hacer un **máximo de 4 envíos**.\n",
    "- Por favor, todas sus dudas haganlas en el hilo de U-cursos de la tarea. Los emails que lleguen al equipo docente serán remitidos a ese medio. Recuerden el ánimo colaborativo del curso!!\n",
    "- Estar top 5 en alguna métrica equivale a 1 punto extra en la nota final.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "V5BBxJWQaSE-"
   },
   "source": [
    "**Link a la competencia:  https://competitions.codalab.org/competitions/25302?secret_key=690406c7-b3b0-4092-8694-d08d7991ca94**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9spX-Hkh8YJg"
   },
   "source": [
    "### Modelos\n",
    "\n",
    "La RNN del baseline adjunto a este notebook está programado en [`pytorch`](https://pytorch.org/) y contiene:\n",
    "\n",
    "- La carga los datasets, creación de batches de texto y padding. En resumen, carga los datos y los deja listo para entrenar la red.\n",
    "- La implementación básica de una red `LSTM` simple de solo un nivel y sin bi-direccionalidad. \n",
    "- La construcción un output para que lo puedan probar en la tarea en codelab.\n",
    "\n",
    "\n",
    "\n",
    "roponer algunos experimentos a hacer:\n",
    "(cambiar el batch size, dimensiones de las capas, cambiar el tipo de\n",
    "RNN, cambiar el optimizer, usar una CRF loss, usar embeddings\n",
    "pre-entrenados, usar BERT??). Quizás podemos sugerir usar algo como\n",
    "https://github.com/flairNLP/flair\n",
    "\n",
    "Se espera que ustedes experimenten con el baseline utilizando (pero no limitándose) estas sugerencias:\n",
    "\n",
    "*   Probar Early stopping\n",
    "*   Variar la cantidad de parámetros de la capa de embeddings.\n",
    "*   Variar la cantidad de capas RNN.\n",
    "*   Variar la cantidad de parámetros de las capas de RNN.\n",
    "*   Inicializar la capa de embeddings con modelos pre-entrenados. (word2vec, glove, conceptnet, etc...).[Guía breve aquí](https://github.com/dccuchile/spanish-word-embeddings), [Embeddings en español aquí](https://github.com/dccuchile/spanish-word-embeddings).\n",
    "*   Variar la cantidad de épocas de entrenamiento.\n",
    "*   Variar el optimizador, learning rate, batch size, usar CRF loss, etc...\n",
    "*   Probar bi-direccionalidad.\n",
    "*   Probar teacher forcing.\n",
    "*   Incluir dropout.\n",
    "*   Probar modelos de tipo GRU\n",
    "*   Probar Embedding Contextuales (les puede ser de utilidad [flair](https://github.com/flairNLP/flair))\n",
    "*   Probar modelos de transformers en español usando [Huggingface](https://github.com/huggingface/transformers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4HfZqQ-_aSFE"
   },
   "source": [
    "### Reporte\n",
    "\n",
    "Este debe cumplir la siguiente estructura:\n",
    "\n",
    "1.\t**Introducción**: Presentar brevemente el problema a resolver, los modelos utilizados en el desarrollo de la tarea y conclusiones obtenidas. (0.5 Puntos)\n",
    "\n",
    "2.\t**Modelos**: Describir brevemente los modelos, métodos y hiperparámetros utilizados. (1.0 puntos)\n",
    "\n",
    "4.\t**Métricas de evaluación**: Describir las métricas utilizadas en la evaluación indicando que miden y cuál es su interpretación en este problema en particular. (0.5 puntos)\n",
    "\n",
    "5.\t**Experimentos**: Reportar todos sus experimentos y código en esta sección. Comparar los resultados obtenidos utilizando diferentes modelos. ¡Es vital haber realizado varios experimentos para sacar una buena nota! (3.0 puntos)\n",
    "\n",
    "6.\t**Conclusiones**: Discutir resultados, proponer trabajo futuro. (1.0 punto)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fGM-hn87aSFF"
   },
   "source": [
    "(Pueden eliminar cualquier celda con instrucciones...)\n",
    "\n",
    "**Importante**: Recuerden poner su nombre y el de su usuario o de equipo (en caso de que aplique) tanto en el reporte. NO serán evaluados Notebooks sin nombre."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "f7X2FruyaSFG"
   },
   "source": [
    "\n",
    "-----------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PzQlYlmGaSFH"
   },
   "source": [
    "## Introducción\n",
    "\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UbA1EmhCaSFI"
   },
   "source": [
    "## Modelos \n",
    "\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AaVhZ5iaaSFK"
   },
   "source": [
    "## Métricas de evaluación\n",
    "\n",
    "- **Precision:** ...\n",
    "- **Recall:** ...\n",
    "- **F1 score:** ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-22T15:44:52.175773Z",
     "start_time": "2020-06-22T15:44:52.172782Z"
    },
    "colab_type": "text",
    "id": "uFM-wNt8aSFM"
   },
   "source": [
    "## Experimentos\n",
    "\n",
    "\n",
    "El código que les entregaremos servirá de baseline para luego implementar mejores modelos. \n",
    "En general, el código asociado a la carga de los datos, las funciones de entrenamiento, de evaluación y la predicción de los datos de la competencia no deberían cambiar. \n",
    "Solo deben preocuparse de cambiar la arquitectura del modelo, sus hiperparámetros y reportar, lo cual lo pueden hacer en las subsecciones *modelos*.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 357
    },
    "colab_type": "code",
    "id": "hX-D0-BmN_KG",
    "outputId": "3b2d0768-5db7-4412-b491-94940e2c1d48"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Jul 30 21:47:12 2020       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 450.51.05    Driver Version: 418.67       CUDA Version: 10.1     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
      "| N/A   50C    P8    10W /  70W |      0MiB / 15079MiB |      0%      Default |\n",
      "|                               |                      |                 ERR! |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LMgKjfYC_Go-"
   },
   "source": [
    "###  Carga de datos y Preprocesamiento\n",
    "\n",
    "Para cargar los datos y preprocesarlos usaremos la librería [`torchtext`](https://github.com/pytorch/text).\n",
    "En particular usaremos su módulo `data`, el cual según su documentación original provee: \n",
    "\n",
    "    - Ability to describe declaratively how to load a custom NLP dataset that's in a \"normal\" format\n",
    "    - Ability to define a preprocessing pipeline\n",
    "    - Batching, padding, and numericalizing (including building a vocabulary object)\n",
    "    - Wrapper for dataset splits (train, validation, test)\n",
    "\n",
    "\n",
    "El proceso será el siguiente: \n",
    "\n",
    "1. Descargar los datos desde github y examinarlos.\n",
    "2. Definir los campos (`fields`) que cargaremos desde los archivos.\n",
    "3. Cargar los datasets.\n",
    "4. Crear el vocabulario.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-23T22:24:52.392908Z",
     "start_time": "2020-06-23T22:24:50.641641Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 445
    },
    "colab_type": "code",
    "id": "27csY87GaSFO",
    "outputId": "8bdadf1b-439c-4151-c858-8011b7038338",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torchtext==0.6\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f2/17/e7c588245aece7aa93f360894179374830daf60d7ed0bbb59332de3b3b61/torchtext-0.6.0-py3-none-any.whl (64kB)\n",
      "\r",
      "\u001b[K     |█████                           | 10kB 22.2MB/s eta 0:00:01\r",
      "\u001b[K     |██████████▏                     | 20kB 5.0MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████▎                | 30kB 6.4MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████▍           | 40kB 7.0MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████▌      | 51kB 6.6MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████████▋ | 61kB 7.1MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████████| 71kB 5.1MB/s \n",
      "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from torchtext==0.6) (4.41.1)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torchtext==0.6) (1.18.5)\n",
      "Collecting sentencepiece\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/a4/d0a884c4300004a78cca907a6ff9a5e9fe4f090f5d95ab341c53d28cbc58/sentencepiece-0.1.91-cp36-cp36m-manylinux1_x86_64.whl (1.1MB)\n",
      "\r",
      "\u001b[K     |▎                               | 10kB 30.7MB/s eta 0:00:01\r",
      "\u001b[K     |▋                               | 20kB 37.6MB/s eta 0:00:01\r",
      "\u001b[K     |█                               | 30kB 20.9MB/s eta 0:00:01\r",
      "\u001b[K     |█▏                              | 40kB 16.7MB/s eta 0:00:01\r",
      "\u001b[K     |█▌                              | 51kB 13.5MB/s eta 0:00:01\r",
      "\u001b[K     |█▉                              | 61kB 13.0MB/s eta 0:00:01\r",
      "\u001b[K     |██▏                             | 71kB 12.6MB/s eta 0:00:01\r",
      "\u001b[K     |██▍                             | 81kB 12.0MB/s eta 0:00:01\r",
      "\u001b[K     |██▊                             | 92kB 12.3MB/s eta 0:00:01\r",
      "\u001b[K     |███                             | 102kB 12.2MB/s eta 0:00:01\r",
      "\u001b[K     |███▍                            | 112kB 12.2MB/s eta 0:00:01\r",
      "\u001b[K     |███▋                            | 122kB 12.2MB/s eta 0:00:01\r",
      "\u001b[K     |████                            | 133kB 12.2MB/s eta 0:00:01\r",
      "\u001b[K     |████▎                           | 143kB 12.2MB/s eta 0:00:01\r",
      "\u001b[K     |████▋                           | 153kB 12.2MB/s eta 0:00:01\r",
      "\u001b[K     |████▉                           | 163kB 12.2MB/s eta 0:00:01\r",
      "\u001b[K     |█████▏                          | 174kB 12.2MB/s eta 0:00:01\r",
      "\u001b[K     |█████▌                          | 184kB 12.2MB/s eta 0:00:01\r",
      "\u001b[K     |█████▉                          | 194kB 12.2MB/s eta 0:00:01\r",
      "\u001b[K     |██████                          | 204kB 12.2MB/s eta 0:00:01\r",
      "\u001b[K     |██████▍                         | 215kB 12.2MB/s eta 0:00:01\r",
      "\u001b[K     |██████▊                         | 225kB 12.2MB/s eta 0:00:01\r",
      "\u001b[K     |███████                         | 235kB 12.2MB/s eta 0:00:01\r",
      "\u001b[K     |███████▎                        | 245kB 12.2MB/s eta 0:00:01\r",
      "\u001b[K     |███████▋                        | 256kB 12.2MB/s eta 0:00:01\r",
      "\u001b[K     |████████                        | 266kB 12.2MB/s eta 0:00:01\r",
      "\u001b[K     |████████▎                       | 276kB 12.2MB/s eta 0:00:01\r",
      "\u001b[K     |████████▌                       | 286kB 12.2MB/s eta 0:00:01\r",
      "\u001b[K     |████████▉                       | 296kB 12.2MB/s eta 0:00:01\r",
      "\u001b[K     |█████████▏                      | 307kB 12.2MB/s eta 0:00:01\r",
      "\u001b[K     |█████████▌                      | 317kB 12.2MB/s eta 0:00:01\r",
      "\u001b[K     |█████████▊                      | 327kB 12.2MB/s eta 0:00:01\r",
      "\u001b[K     |██████████                      | 337kB 12.2MB/s eta 0:00:01\r",
      "\u001b[K     |██████████▍                     | 348kB 12.2MB/s eta 0:00:01\r",
      "\u001b[K     |██████████▊                     | 358kB 12.2MB/s eta 0:00:01\r",
      "\u001b[K     |███████████                     | 368kB 12.2MB/s eta 0:00:01\r",
      "\u001b[K     |███████████▎                    | 378kB 12.2MB/s eta 0:00:01\r",
      "\u001b[K     |███████████▋                    | 389kB 12.2MB/s eta 0:00:01\r",
      "\u001b[K     |████████████                    | 399kB 12.2MB/s eta 0:00:01\r",
      "\u001b[K     |████████████▏                   | 409kB 12.2MB/s eta 0:00:01\r",
      "\u001b[K     |████████████▌                   | 419kB 12.2MB/s eta 0:00:01\r",
      "\u001b[K     |████████████▉                   | 430kB 12.2MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████▏                  | 440kB 12.2MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████▍                  | 450kB 12.2MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████▊                  | 460kB 12.2MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████                  | 471kB 12.2MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████▍                 | 481kB 12.2MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████▋                 | 491kB 12.2MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████                 | 501kB 12.2MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████▎                | 512kB 12.2MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████▋                | 522kB 12.2MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████▉                | 532kB 12.2MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████▏               | 542kB 12.2MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████▌               | 552kB 12.2MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████▉               | 563kB 12.2MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████               | 573kB 12.2MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████▍              | 583kB 12.2MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████▊              | 593kB 12.2MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████              | 604kB 12.2MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████▎             | 614kB 12.2MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████▋             | 624kB 12.2MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████             | 634kB 12.2MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████▎            | 645kB 12.2MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████▌            | 655kB 12.2MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████▉            | 665kB 12.2MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████▏           | 675kB 12.2MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████▌           | 686kB 12.2MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████▊           | 696kB 12.2MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████           | 706kB 12.2MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████▍          | 716kB 12.2MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████▋          | 727kB 12.2MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████          | 737kB 12.2MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████▎         | 747kB 12.2MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████▋         | 757kB 12.2MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████▉         | 768kB 12.2MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████▏        | 778kB 12.2MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████▌        | 788kB 12.2MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████▉        | 798kB 12.2MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████        | 808kB 12.2MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████▍       | 819kB 12.2MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████▊       | 829kB 12.2MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████       | 839kB 12.2MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████▎      | 849kB 12.2MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████▋      | 860kB 12.2MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████      | 870kB 12.2MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████▎     | 880kB 12.2MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████▌     | 890kB 12.2MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████▉     | 901kB 12.2MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████▏    | 911kB 12.2MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████▌    | 921kB 12.2MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████▊    | 931kB 12.2MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████    | 942kB 12.2MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████▍   | 952kB 12.2MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████▊   | 962kB 12.2MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████████   | 972kB 12.2MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████████▎  | 983kB 12.2MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████████▋  | 993kB 12.2MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████████  | 1.0MB 12.2MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████████▏ | 1.0MB 12.2MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████████▌ | 1.0MB 12.2MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████████▉ | 1.0MB 12.2MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████████▏| 1.0MB 12.2MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████████▍| 1.1MB 12.2MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████████▊| 1.1MB 12.2MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████████| 1.1MB 12.2MB/s \n",
      "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from torchtext==0.6) (2.23.0)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from torchtext==0.6) (1.15.0)\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (from torchtext==0.6) (1.5.1+cu101)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext==0.6) (2020.6.20)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext==0.6) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext==0.6) (1.24.3)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext==0.6) (2.10)\n",
      "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch->torchtext==0.6) (0.16.0)\n",
      "Installing collected packages: sentencepiece, torchtext\n",
      "  Found existing installation: torchtext 0.3.1\n",
      "    Uninstalling torchtext-0.3.1:\n",
      "      Successfully uninstalled torchtext-0.3.1\n",
      "Successfully installed sentencepiece-0.1.91 torchtext-0.6.0\n",
      "Requirement already satisfied: torch==1.5.1 in /usr/local/lib/python3.6/dist-packages (1.5.1+cu101)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch==1.5.1) (1.18.5)\n",
      "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch==1.5.1) (0.16.0)\n"
     ]
    }
   ],
   "source": [
    "# Instalar torchtext (en codalab) - Descomentar.\n",
    "#!pip3 install --upgrade torchtext\n",
    "!pip3 install torchtext==0.6\n",
    "#!pip3 install --upgrade torch\n",
    "!pip3 install torch==1.5.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-23T22:24:53.086354Z",
     "start_time": "2020-06-23T22:24:52.394902Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "ng7wRGEyawjM",
    "outputId": "cea13824-fd8f-4f2e-ce68-218c17ecd532"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchtext import data, datasets\n",
    "\n",
    "\n",
    "# Garantizar reproducibilidad \n",
    "SEED = 1234\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# torch.cuda.device(1)\n",
    "# print(torch.cuda.current_device())\n",
    "print('Using', device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BehSou6rCvwg"
   },
   "source": [
    "#### Obtener datos\n",
    "\n",
    "Descargamos los datos de entrenamiento, validación y prueba en nuestro directorio de trabajo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-23T22:25:49.789218Z",
     "start_time": "2020-06-23T22:24:53.088871Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "lbT0g_kC18Jb"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!wget https://github.com/dccuchile/CC6205/releases/download/Data/train_NER_esp.txt -nc # Dataset de Entrenamiento\n",
    "!wget https://github.com/dccuchile/CC6205/releases/download/Data/val_NER_esp.txt -nc    # Dataset de Validación (Para probar y ajustar el modelo)\n",
    "!wget https://github.com/dccuchile/CC6205/releases/download/Data/test_NER_esp.txt -nc  # Dataset de la Competencia. Estos datos solo contienen los tokens. ¡¡SON LOS QUE DEBEN SER PREDICHOS!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uMud7YGMBZvg"
   },
   "source": [
    "####  Fields\n",
    "\n",
    "Un `field`:\n",
    "\n",
    "* Define un tipo de datos junto con instrucciones para convertir el texto a Tensor.\n",
    "* Contiene un objeto `Vocab` que contiene el vocabulario (palabras posibles que puede tomar ese campo).\n",
    "* Contiene otros parámetros relacionados con la forma en que se debe numericalizar un tipo de datos, como un método de tokenización y el tipo de Tensor que se debe producir.\n",
    "\n",
    "\n",
    "Analizemos el siguiente cuadro el cual contiene un ejemplo cualquiera de entrenamiento:\n",
    "\n",
    "\n",
    "```\n",
    "El O\n",
    "Abogado B-PER\n",
    "General I-PER\n",
    "del I-PER\n",
    "Estado I-PER\n",
    ", O\n",
    "Daryl B-PER\n",
    "Williams I-PER\n",
    "```\n",
    "\n",
    "Cada linea contiene una palabra y su clase. Para que `torchtext` pueda cargar estos datos, debemos definir como va a leer y separar los componentes de cada una de las lineas.\n",
    "Para esto, definiremos un field para cada uno de esos componentes: Las palabras (`TEXT`) y los NER_TAGS (`clase`).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-23T22:25:49.795126Z",
     "start_time": "2020-06-23T22:25:49.791108Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "3DcM_IjgCdzz"
   },
   "outputs": [],
   "source": [
    "# Primer Field: TEXT. Representan los tokens de la secuencia\n",
    "TEXT = data.Field(lower=False) \n",
    "\n",
    "# Segundo Field: NER_TAGS. Representan los Tags asociados a cada palabra.\n",
    "NER_TAGS = data.Field(unk_token=None)\n",
    "\n",
    "fields = ((\"text\", TEXT), (\"nertags\", NER_TAGS))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xCKTJOdgC5eC"
   },
   "source": [
    "####  SequenceTaggingDataset\n",
    "\n",
    "`SequenceTaggingDataset` es una clase de torchtext diseñada para contener datasets de sequence labelling. \n",
    "Los ejemplos que se guarden en una instancia de estos serán arreglos de palabras pareados con sus respectivos tags.\n",
    "Por ejemplo, para Part-of-speech tagging:\n",
    "\n",
    "[I, love, PyTorch, .] estará pareado con [PRON, VERB, PROPN, PUNCT]\n",
    "\n",
    "\n",
    "La idea es que usando los fields que definimos antes, le indiquemos a la clase cómo cargar los datasets de prueba, validación y test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-23T22:25:50.294370Z",
     "start_time": "2020-06-23T22:25:49.797092Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "HsHdGml62J21"
   },
   "outputs": [],
   "source": [
    "train_data, valid_data, test_data = datasets.SequenceTaggingDataset.splits(\n",
    "    path=\"./\",\n",
    "    train=\"train_NER_esp.txt\",\n",
    "    validation=\"val_NER_esp.txt\",\n",
    "    test=\"test_NER_esp.txt\",\n",
    "    fields=fields,\n",
    "    encoding=\"iso-8859-1\",\n",
    "    separator=\" \"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-23T22:25:50.301354Z",
     "start_time": "2020-06-23T22:25:50.296368Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "Hu7q3HCliia5",
    "outputId": "096deb67-f5d7-4589-bc22-a90e9609470b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numero de ejemplos de entrenamiento: 8323\n",
      "Número de ejemplos de validación: 1915\n",
      "Número de ejemplos de test (competencia): 1517\n"
     ]
    }
   ],
   "source": [
    "print(f\"Numero de ejemplos de entrenamiento: {len(train_data)}\")\n",
    "print(f\"Número de ejemplos de validación: {len(valid_data)}\")\n",
    "print(f\"Número de ejemplos de test (competencia): {len(test_data)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fDRnhXAdFGL-"
   },
   "source": [
    "Visualizemos un ejemplo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-23T22:25:50.317313Z",
     "start_time": "2020-06-23T22:25:50.303361Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 748
    },
    "colab_type": "code",
    "id": "T023Ld4RaSF4",
    "outputId": "910ce4b4-c962-4550-cb4b-2f61d7758b79",
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Dentro', 'O'),\n",
       " ('de', 'O'),\n",
       " ('la', 'O'),\n",
       " ('apatía', 'O'),\n",
       " ('y', 'O'),\n",
       " ('el', 'O'),\n",
       " ('desengaño', 'O'),\n",
       " ('con', 'O'),\n",
       " ('la', 'O'),\n",
       " ('oposición', 'O'),\n",
       " ('al', 'O'),\n",
       " ('presidente', 'O'),\n",
       " ('yugoslavo', 'O'),\n",
       " (',', 'O'),\n",
       " ('Slobodan', 'B-PER'),\n",
       " ('Milosevic', 'I-PER'),\n",
       " (',', 'O'),\n",
       " ('la', 'O'),\n",
       " ('población', 'O'),\n",
       " ('ha', 'O'),\n",
       " ('empezado', 'O'),\n",
       " ('a', 'O'),\n",
       " ('jugar', 'O'),\n",
       " ('a', 'O'),\n",
       " ('las', 'O'),\n",
       " ('cartas', 'O'),\n",
       " ('con', 'O'),\n",
       " ('una', 'O'),\n",
       " ('baraja', 'O'),\n",
       " ('en', 'O'),\n",
       " ('la', 'O'),\n",
       " ('que', 'O'),\n",
       " ('sus', 'O'),\n",
       " ('caricaturizados', 'O'),\n",
       " ('líderes', 'O'),\n",
       " ('pintan', 'O'),\n",
       " ('corazones', 'O'),\n",
       " (',', 'O'),\n",
       " ('bastos', 'O'),\n",
       " ('y', 'O'),\n",
       " ('otros', 'O'),\n",
       " ('palos', 'O'),\n",
       " ('.', 'O')]"
      ]
     },
     "execution_count": 8,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "random_item_idx = random.randint(0, len(train_data))\n",
    "random_example = train_data.examples[random_item_idx]\n",
    "list(zip(random_example.text, random_example.nertags))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2l05KYy5FSUy"
   },
   "source": [
    "#### Construir los vocabularios para el texto y las etiquetas\n",
    "\n",
    "Los vocabularios son los obbjetos que contienen todos los tokens (de entrenamiento) posibles para ambos fields.\n",
    "El siguiente paso consiste en construirlos. Para esto, hacemos uso del método `Field.build_vocab` sobre cada uno de nuestros `fields`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-23T22:25:50.445968Z",
     "start_time": "2020-06-23T22:25:50.320305Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "PBhp7WICiibL"
   },
   "outputs": [],
   "source": [
    "TEXT.build_vocab(train_data)\n",
    "NER_TAGS.build_vocab(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-23T22:25:50.453960Z",
     "start_time": "2020-06-23T22:25:50.448987Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "M4OgUKM_iibO",
    "outputId": "e7eb0fe5-29bd-4c81-eed8-f922a4c54265",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens únicos en TEXT: 26101\n",
      "Tokens únicos en NER_TAGS: 10\n"
     ]
    }
   ],
   "source": [
    "print(f\"Tokens únicos en TEXT: {len(TEXT.vocab)}\")\n",
    "print(f\"Tokens únicos en NER_TAGS: {len(NER_TAGS.vocab)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-23T22:25:50.460965Z",
     "start_time": "2020-06-23T22:25:50.455942Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "colab_type": "code",
    "id": "d4FeyL9nFnId",
    "outputId": "39bba466-0288-4a23-bef3-f55836072e3f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<pad>',\n",
       " 'O',\n",
       " 'B-ORG',\n",
       " 'I-ORG',\n",
       " 'B-LOC',\n",
       " 'B-PER',\n",
       " 'I-PER',\n",
       " 'I-MISC',\n",
       " 'B-MISC',\n",
       " 'I-LOC']"
      ]
     },
     "execution_count": 11,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Veamos las posibles etiquetas que hemos cargado:\n",
    "NER_TAGS.vocab.itos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "g_jJ20GqxtuS"
   },
   "outputs": [],
   "source": [
    "%%script false\n",
    "## Verifiquemos los NER_TAGS del conjunto de validación:\n",
    "TEXT.build_vocab(valid_data)\n",
    "NER_TAGS.build_vocab(valid_data)\n",
    "\n",
    "print(f\"Tokens únicos en TEXT: {len(TEXT.vocab)}\")\n",
    "print(f\"Tokens únicos en NER_TAGS: {len(NER_TAGS.vocab)}\")\n",
    "\n",
    "NER_TAGS.vocab.itos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HYQDoUqSHFKj"
   },
   "source": [
    "Observen que ademas de los tags NER, tenemos \\<pad\\>, el cual es generado por el dataloader para cumplir con el padding de cada oración.\n",
    "\n",
    "Veamos ahora los tokens mas frecuentes y especiales:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-23T22:25:50.473893Z",
     "start_time": "2020-06-23T22:25:50.462923Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "colab_type": "code",
    "id": "m5eSLm4diibR",
    "outputId": "f110ceaf-015c-4ef1-d3ac-799db7b68130"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('de', 17657),\n",
       " (',', 14716),\n",
       " ('la', 9571),\n",
       " ('que', 7516),\n",
       " ('.', 7263),\n",
       " ('el', 6905),\n",
       " ('en', 6484),\n",
       " ('\"', 5691),\n",
       " ('y', 5336),\n",
       " ('a', 4304)]"
      ]
     },
     "execution_count": 13,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokens mas frecuentes\n",
    "TEXT.vocab.freqs.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-23T22:25:50.479897Z",
     "start_time": "2020-06-23T22:25:50.475889Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "WtEZq38RuO8D",
    "outputId": "c9e00d69-e21a-43dc-fd90-7bd6f15796d8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "0\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "# Seteamos algunas variables que nos serán de utilidad mas adelante...\n",
    "UNK_IDX = TEXT.vocab.stoi[TEXT.unk_token]\n",
    "PAD_IDX = TEXT.vocab.stoi[TEXT.pad_token]\n",
    "\n",
    "PAD_TAG_IDX = NER_TAGS.vocab.stoi[NER_TAGS.pad_token]\n",
    "O_TAG_IDX = NER_TAGS.vocab.stoi['O']\n",
    "\n",
    "print(UNK_IDX)\n",
    "print(PAD_IDX)\n",
    "print(PAD_TAG_IDX)\n",
    "print(O_TAG_IDX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qrYvF3X0sjWL"
   },
   "source": [
    "#### Frecuencia de los Tags\n",
    "\n",
    "Visualizemos rápidamente las cantidades y frecuencias de cada tag:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-23T22:25:50.490885Z",
     "start_time": "2020-06-23T22:25:50.481873Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "tuXOsbJUiibh",
    "outputId": "9f03225a-9390-48f7-b04b-6b10124beda6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tag Ocurrencia Porcentaje\n",
      "\n",
      "O\t231920\t87.6%\n",
      "B-ORG\t7390\t 2.8%\n",
      "I-ORG\t4992\t 1.9%\n",
      "B-LOC\t4913\t 1.9%\n",
      "B-PER\t4321\t 1.6%\n",
      "I-PER\t3903\t 1.5%\n",
      "I-MISC\t3212\t 1.2%\n",
      "B-MISC\t2173\t 0.8%\n",
      "I-LOC\t1891\t 0.7%\n"
     ]
    }
   ],
   "source": [
    "def tag_percentage(tag_counts):\n",
    "    \n",
    "    total_count = sum([count for tag, count in tag_counts])\n",
    "    tag_counts_percentages = [(tag, count, count/total_count) for tag, count in tag_counts]\n",
    "  \n",
    "    return tag_counts_percentages\n",
    "\n",
    "print(\"Tag Ocurrencia Porcentaje\\n\")\n",
    "\n",
    "for tag, count, percent in tag_percentage(NER_TAGS.vocab.freqs.most_common()):\n",
    "    print(f\"{tag}\\t{count}\\t{percent*100:4.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-22T21:44:17.730460Z",
     "start_time": "2020-06-22T21:44:17.724482Z"
    },
    "colab_type": "text",
    "id": "y4wPiydnaSGs"
   },
   "source": [
    "#### Configuramos pytorch y dividimos los datos.\n",
    "\n",
    "Importante: si tienes problemas con la ram de la gpu, disminuye el tamaño de los batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-23T22:25:51.101455Z",
     "start_time": "2020-06-23T22:25:50.492843Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "uB7cwLWpaSGs",
    "outputId": "6cd23357-8e80-4716-ab1b-fcdc1bf29e00"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 16  # disminuir si hay problemas de ram.\n",
    "\n",
    "# Usar cuda si es que está disponible.\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using', device)\n",
    "\n",
    "# Dividir datos entre entrenamiento y test\n",
    "train_iterator, valid_iterator, test_iterator = data.BucketIterator.splits(\n",
    "    (train_data, valid_data, test_data),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    device=device,\n",
    "    sort=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "B21E1eAFId16"
   },
   "source": [
    "#### Métricas de evaluación\n",
    "\n",
    "Además, definiremos las métricas que serán usadas tanto para la competencia como para evaluar el modelo: `precision`, `recall` y `f1`.\n",
    "**Importante**: Noten que la evaluación solo se hace para las Named Entities (sin contar 'O')."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-23T22:25:51.654826Z",
     "start_time": "2020-06-23T22:25:51.103450Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "9mUOOLEWiicU"
   },
   "outputs": [],
   "source": [
    "# Definimos las métricas\n",
    "\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "import warnings\n",
    "import sklearn.exceptions\n",
    "warnings.filterwarnings(\"ignore\",\n",
    "                        category=sklearn.exceptions.UndefinedMetricWarning)\n",
    "\n",
    "\n",
    "def calculate_metrics(preds, y_true, pad_idx=PAD_TAG_IDX, o_idx=O_TAG_IDX):\n",
    "    \"\"\"\n",
    "    Calcula precision, recall y f1 de cada batch.\n",
    "    \"\"\"\n",
    "\n",
    "    # Obtener el indice de la clase con probabilidad mayor. (clases)\n",
    "    y_pred = preds.argmax(dim=1, keepdim=True)\n",
    "    # Obtenemos los indices distintos de 0.\n",
    "\n",
    "    # filtramos <pad> y O para calcular los scores.\n",
    "    mask = [(y_true != o_idx) & (y_true != pad_idx)]\n",
    "    y_pred = y_pred[mask]\n",
    "    y_true = y_true[mask]\n",
    "\n",
    "    # traemos a la cpu\n",
    "    y_pred = y_pred.view(-1).to('cpu')\n",
    "    y_true = y_true.to('cpu')\n",
    "    \n",
    "    # calcular scores\n",
    "    f1 = f1_score(y_true, y_pred, average='macro')\n",
    "    precision = precision_score(y_true, y_pred, average='macro')\n",
    "    recall = recall_score(y_true, y_pred, average='macro')\n",
    "\n",
    "    return precision, recall, f1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Hod516H1aSG2"
   },
   "source": [
    "-------------------\n",
    "\n",
    "### Modelo Baseline\n",
    "\n",
    "Teniendo ya cargado los datos, toca definir nuestro modelo. Este baseline tendrá una capa de embedding, unas cuantas LSTM y una capa de salida y usará dropout en el entrenamiento.\n",
    "\n",
    "Este constará de los siguientes pasos: \n",
    "\n",
    "1. Definir la clase que contendrá la red.\n",
    "2. Definir los hiperparámetros e inicializar la red. \n",
    "3. Definir la época de entrenamiento\n",
    "3. Definir la función de loss.\n",
    "\n",
    "\n",
    "\n",
    "Recomendamos que para experimentar, encapsules los modelos en una sola variable y luego la fijes en model para entrenarla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-23T22:25:51.666751Z",
     "start_time": "2020-06-23T22:25:51.656778Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "rMPL08XqaSG3"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "# Definir la red\n",
    "class NER_RNN(nn.Module):\n",
    "    def __init__(self, \n",
    "                 input_dim, \n",
    "                 embedding_dim, \n",
    "                 hidden_dim, \n",
    "                 output_dim,\n",
    "                 n_layers, \n",
    "                 bidirectional, \n",
    "                 dropout, \n",
    "                 pad_idx):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        # Capa de embedding\n",
    "        self.embedding = nn.Embedding(input_dim,\n",
    "                                      embedding_dim,\n",
    "                                      padding_idx=pad_idx)\n",
    "\n",
    "        # Capa LSTM\n",
    "        self.lstm = nn.LSTM(embedding_dim,\n",
    "                           hidden_dim,\n",
    "                           num_layers=n_layers,\n",
    "                           bidirectional=bidirectional, \n",
    "                           dropout = dropout if n_layers > 1 else 0)\n",
    "\n",
    "        # Capa de salida\n",
    "        self.fc = nn.Linear(hidden_dim * 2 if bidirectional else hidden_dim,\n",
    "                            output_dim)\n",
    "\n",
    "        # Dropout\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, text):\n",
    "\n",
    "        #text = [sent len, batch size]\n",
    "\n",
    "        # Convertir lo enviado a embedding\n",
    "        embedded = self.dropout(self.embedding(text))\n",
    "        \n",
    "        outputs, (hidden, cell) = self.lstm(embedded)\n",
    "        #embedded = [sent len, batch size, emb dim]\n",
    "\n",
    "        # Pasar los embeddings por la rnn (LSTM)\n",
    "\n",
    "        #output = [sent len, batch size, hid dim * n directions]\n",
    "        #hidden/cell = [n layers * n directions, batch size, hid dim]\n",
    "\n",
    "        # Predecir usando la capa de salida.\n",
    "        predictions = self.fc(self.dropout(outputs))\n",
    "        #predictions = [sent len, batch size, output dim]\n",
    "\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-22T21:43:02.333880Z",
     "start_time": "2020-06-22T21:43:02.329861Z"
    },
    "colab_type": "text",
    "id": "cCl3530VaSG7"
   },
   "source": [
    "#### Hiperparámetros de la red\n",
    "\n",
    "Definimos los hiperparámetros. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-23T22:25:51.705684Z",
     "start_time": "2020-06-23T22:25:51.668746Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "EHdi3QdOaSG8"
   },
   "outputs": [],
   "source": [
    "# tamaño del vocabulario. recuerden que la entrada son vectores bag of word(one-hot).\n",
    "INPUT_DIM = len(TEXT.vocab)\n",
    "EMBEDDING_DIM = 100  # dimensión de los embeddings.\n",
    "HIDDEN_DIM = 128  # dimensión de la capas LSTM\n",
    "OUTPUT_DIM = len(NER_TAGS.vocab)  # número de clases\n",
    "\n",
    "N_LAYERS = 2  # número de capas.\n",
    "DROPOUT = 0.25\n",
    "BIDIRECTIONAL = False\n",
    "\n",
    "# Creamos nuestro modelo.\n",
    "baseline_model = NER_RNN(INPUT_DIM, EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM,\n",
    "                         N_LAYERS, BIDIRECTIONAL, DROPOUT, PAD_IDX)\n",
    "\n",
    "baseline_model_name = 'baseline'  # nombre que tendrá el modelo guardado..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-23T22:25:51.710633Z",
     "start_time": "2020-06-23T22:25:51.706649Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "jlF1DhJeaSHA"
   },
   "outputs": [],
   "source": [
    "baseline_n_epochs = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "s3u4imJGaSHE"
   },
   "source": [
    "#### Definimos la función de loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-23T22:25:51.715637Z",
     "start_time": "2020-06-23T22:25:51.712628Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "6G_4k99_aSHG"
   },
   "outputs": [],
   "source": [
    "# Loss: Cross Entropy\n",
    "TAG_PAD_IDX = NER_TAGS.vocab.stoi[NER_TAGS.pad_token]\n",
    "baseline_criterion = nn.CrossEntropyLoss(ignore_index = TAG_PAD_IDX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lRYOEDiQaSHK"
   },
   "source": [
    "--------------------\n",
    "### Modelo 1\n",
    "\n",
    "En estas secciones pueden implementar nuevas redes al modificar los hiperparámetros, la cantidad de épocas de entrenamiento, el tamaño de los batches, loss, optimizador, etc... como también definir nuevas arquitecturas de red (mediante la creación de clases nuevas)\n",
    "\n",
    "\n",
    "Al final de estas, hay 4 variables, las cuales deben setear con los modelos, épocas de entrenamiento, loss y optimizador que deseen probar.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-23T22:25:51.722604Z",
     "start_time": "2020-06-23T22:25:51.717615Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "c81f8ki5aSHL"
   },
   "outputs": [],
   "source": [
    "# model_1 = ...\n",
    "# model_name_1 = ...\n",
    "# n_epochs_1 = ...\n",
    "# loss_1 = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rV9oLkN1aSHO"
   },
   "source": [
    "---------------\n",
    "\n",
    "### Modelo 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-23T22:25:51.728587Z",
     "start_time": "2020-06-23T22:25:51.724596Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "KWPzETaNaSHP"
   },
   "outputs": [],
   "source": [
    "# model_2 = ...\n",
    "# model_name_2 = ...\n",
    "# n_epochs_2 = ...\n",
    "# loss_2 = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-22T16:07:45.755561Z",
     "start_time": "2020-06-22T16:07:45.751571Z"
    },
    "colab_type": "text",
    "id": "Zpy3p7YaaSHT"
   },
   "source": [
    "---------------\n",
    "\n",
    "\n",
    "### Modelo 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-23T22:25:51.733572Z",
     "start_time": "2020-06-23T22:25:51.730580Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "_w0CFjA8aSHU"
   },
   "outputs": [],
   "source": [
    "# modelo_3 = ...\n",
    "# model_name_3 = ...\n",
    "# n_epochs_3 = ...\n",
    "# loss_3 = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aPGdirx7aSHZ"
   },
   "source": [
    "------\n",
    "### Entrenamos y evaluamos\n",
    "\n",
    "\n",
    "**Importante** : Fijen el modelo, el número de épocas de entrenamiento, la loss y el optimizador que usarán para entrenar y evaluar en las siguientes variables!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-23T22:25:51.748571Z",
     "start_time": "2020-06-23T22:25:51.739556Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "r8YlGnjxaSHZ"
   },
   "outputs": [],
   "source": [
    "model = baseline_model\n",
    "model_name = baseline_model_name\n",
    "criterion = baseline_criterion\n",
    "n_epochs = baseline_n_epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Pu_lXic2aSHd"
   },
   "source": [
    "\n",
    "\n",
    "#### Inicializamos la red\n",
    "\n",
    "iniciamos los pesos de la red de forma aleatoria (Usando una distribución normal).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-23T22:25:51.805380Z",
     "start_time": "2020-06-23T22:25:51.751524Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 119
    },
    "colab_type": "code",
    "id": "Q-G_NWFcaSHe",
    "outputId": "cf05f4b0-d346-4027-db58-ac0248f7290d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NER_RNN(\n",
       "  (embedding): Embedding(26101, 100, padding_idx=1)\n",
       "  (lstm): LSTM(100, 128, num_layers=2, dropout=0.25)\n",
       "  (fc): Linear(in_features=128, out_features=10, bias=True)\n",
       "  (dropout): Dropout(p=0.25, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 26,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def init_weights(m):\n",
    "    # Inicializamos los pesos como aleatorios\n",
    "    for name, param in m.named_parameters():\n",
    "        nn.init.normal_(param.data, mean=0, std=0.1) \n",
    "        \n",
    "    # Seteamos como 0 los embeddings de UNK y PAD.\n",
    "    model.embedding.weight.data[UNK_IDX] = torch.zeros(EMBEDDING_DIM)\n",
    "    model.embedding.weight.data[PAD_IDX] = torch.zeros(EMBEDDING_DIM)\n",
    "        \n",
    "model.apply(init_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-23T22:25:51.812389Z",
     "start_time": "2020-06-23T22:25:51.806377Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "mjWDX2CJaSHh",
    "outputId": "40a2f609-ab05-4cf1-8ae4-183edc04922b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El modelo actual tiene 2,861,246 parámetros entrenables.\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f'El modelo actual tiene {count_parameters(model):,} parámetros entrenables.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UVqBqerlaSHk"
   },
   "source": [
    "Por último, definimos los embeddings que representan a \\<unk\\> y \\<pad\\>  como [0, 0, ..., 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rVZvHtwpaSHq"
   },
   "source": [
    "#### Definimos el optimizador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-23T22:25:51.819370Z",
     "start_time": "2020-06-23T22:25:51.814357Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "AH6o8_cTaSHq"
   },
   "outputs": [],
   "source": [
    "# Optimizador\n",
    "optimizer = optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Fz39wa78wGYR"
   },
   "source": [
    "#### Enviamos el modelo a cuda\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-23T22:25:54.503226Z",
     "start_time": "2020-06-23T22:25:51.821338Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "dqr0AJ6_iicR"
   },
   "outputs": [],
   "source": [
    "# Enviamos el modelo y la loss a cuda (en el caso en que esté disponible)\n",
    "model = model.to(device)\n",
    "criterion = criterion.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8xlq48WjiW6U"
   },
   "source": [
    "#### Definimos el entrenamiento de la red\n",
    "\n",
    "Algunos conceptos previos: \n",
    "\n",
    "- `epoch` : una pasada de entrenamiento completa de una dataset.\n",
    "- `batch`: una fracción de la época. Se utilizan para entrenar mas rápidamente la red. (mas eficiente pasar n datos que uno en cada ejecución del backpropagation)\n",
    "\n",
    "Esta función está encargada de entrenar la red en una época. Para esto, por cada batch de la época actual, predice los tags del texto, calcula su loss y luego hace backpropagation para actualizar los pesos de la red.\"\n",
    "\n",
    "Observación: En algunos comentarios aparecerá el tamaño de los tensores entre corchetes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-23T22:25:54.515194Z",
     "start_time": "2020-06-23T22:25:54.505221Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "DV6YLt0oiicW"
   },
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, criterion):\n",
    "\n",
    "    epoch_loss = 0\n",
    "    epoch_precision = 0\n",
    "    epoch_recall = 0\n",
    "    epoch_f1 = 0\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    # Por cada batch del iterador de la época:\n",
    "    for batch in iterator:\n",
    "\n",
    "        # Extraemos el texto y los tags del batch que estamos procesado\n",
    "        text = batch.text\n",
    "        tags = batch.nertags\n",
    "\n",
    "        # Reiniciamos los gradientes calculados en la iteración anterior\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        #text = [sent len, batch size]\n",
    "\n",
    "        # Predecimos los tags del texto del batch.\n",
    "        predictions = model(text)\n",
    "\n",
    "        #predictions = [sent len, batch size, output dim]\n",
    "        #tags = [sent len, batch size]\n",
    "\n",
    "        # Reordenamos los datos para calcular la loss\n",
    "        predictions = predictions.view(-1, predictions.shape[-1])\n",
    "        tags = tags.view(-1)\n",
    "\n",
    "        #predictions = [sent len * batch size, output dim]\n",
    "        #tags = [sent len * batch size]\n",
    "\n",
    "        # Calculamos el Cross Entropy de las predicciones con respecto a las etiquetas reales\n",
    "        loss = criterion(predictions, tags)\n",
    "        \n",
    "        # Calculamos el accuracy\n",
    "        precision, recall, f1 = calculate_metrics(predictions, tags)\n",
    "\n",
    "        # Calculamos los gradientes\n",
    "        loss.backward()\n",
    "\n",
    "        # Actualizamos los parámetros de la red\n",
    "        optimizer.step()\n",
    "\n",
    "        # Actualizamos el loss y las métricas\n",
    "        epoch_loss += loss.item()\n",
    "        epoch_precision += precision\n",
    "        epoch_recall += recall\n",
    "        epoch_f1 += f1\n",
    "\n",
    "    return epoch_loss / len(iterator), epoch_precision / len(\n",
    "        iterator), epoch_recall / len(iterator), epoch_f1 / len(iterator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PYNcwKnAz5Hf"
   },
   "source": [
    "#### `Definimos la función de evaluación`\n",
    "\n",
    "Evalua el rendimiento actual de la red usando los datos de validación. \n",
    "\n",
    "Por cada batch de estos datos, calcula y reporta el loss y las métricas asociadas al conjunto de validación. \n",
    "Ya que las métricas son calculadas por cada batch, estas son retornadas promediadas por el número de batches entregados. (ver linea del return)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-23T22:25:54.527162Z",
     "start_time": "2020-06-23T22:25:54.518186Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "WsRuiUuHiicY"
   },
   "outputs": [],
   "source": [
    "def evaluate(model, iterator, criterion):\n",
    "\n",
    "    epoch_loss = 0\n",
    "    epoch_precision = 0\n",
    "    epoch_recall = 0\n",
    "    epoch_f1 = 0\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    # Indicamos que ahora no guardaremos los gradientes\n",
    "    with torch.no_grad():\n",
    "        # Por cada batch\n",
    "        for batch in iterator:\n",
    "\n",
    "            text = batch.text\n",
    "            tags = batch.nertags\n",
    "\n",
    "            # Predecimos\n",
    "            predictions = model(text)\n",
    "\n",
    "            predictions = predictions.view(-1, predictions.shape[-1])\n",
    "            tags = tags.view(-1)\n",
    "\n",
    "            # Calculamos el Cross Entropy de las predicciones con respecto a las etiquetas reales\n",
    "            loss = criterion(predictions, tags)\n",
    "\n",
    "            # Calculamos las métricas\n",
    "            precision, recall, f1 = calculate_metrics(predictions, tags)\n",
    "\n",
    "            # Actualizamos el loss y las métricas\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_precision += precision\n",
    "            epoch_recall += recall\n",
    "            epoch_f1 += f1\n",
    "\n",
    "    return epoch_loss / len(iterator), epoch_precision / len(\n",
    "        iterator), epoch_recall / len(iterator), epoch_f1 / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-23T22:25:54.535141Z",
     "start_time": "2020-06-23T22:25:54.529158Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "Xs-n9Y5yiica"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hy3MVf5H0A94"
   },
   "source": [
    "\n",
    "#### Entrenamiento de la red\n",
    "\n",
    "En este cuadro de código ejecutaremos el entrenamiento de la red.\n",
    "Para esto, primero definiremos el número de épocas y luego por cada época, ejecutaremos `train` y `evaluate`.\n",
    "\n",
    "**Importante: Reiniciar los pesos del modelo**\n",
    "\n",
    "Si ejecutas nuevamente esta celda, se seguira entrenando el mismo modelo una y otra vez. \n",
    "Para reiniciar el modelo se debe ejecutar nuevamente la celda que contiene la función `init_weights`\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-23T21:49:02.524817Z",
     "start_time": "2020-06-23T21:47:09.863026Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "iK5lQqpviicf"
   },
   "outputs": [],
   "source": [
    "%%script false\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Recuerdo: train_iterator y valid_iterator contienen el dataset dividido en batches.\n",
    "\n",
    "    # Entrenar\n",
    "    train_loss, train_precision, train_recall, train_f1 = train(\n",
    "        model, train_iterator, optimizer, criterion)\n",
    "\n",
    "    # Evaluar (valid = validación)\n",
    "    valid_loss, valid_precision, valid_recall, valid_f1 = evaluate(\n",
    "        model, valid_iterator, criterion)\n",
    "\n",
    "    end_time = time.time()\n",
    "\n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "\n",
    "    # Si obtuvimos mejores resultados, guardamos este modelo en el almacenamiento (para poder cargarlo luego)\n",
    "    # Si detienen el entrenamiento prematuramente, pueden cargar el modelo en el siguiente recuadro de código.\n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), '{}.pt'.format(model_name))\n",
    "    # Si ya no mejoramos el loss de validación, terminamos de entrenar.\n",
    "\n",
    "    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(\n",
    "        f'\\tTrain Loss: {train_loss:.3f} | Train f1: {train_f1:.2f} | Train precision: {train_precision:.2f} | Train recall: {train_recall:.2f}'\n",
    "    )\n",
    "    print(\n",
    "        f'\\t Val. Loss: {valid_loss:.3f} |  Val. f1: {valid_f1:.2f} |  Val. precision: {valid_precision:.2f} | Val. recall: {valid_recall:.2f}'\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mLul1hVWuO80"
   },
   "source": [
    "**Importante**: Recuerden que el último modelo entrenado no es el mejor (probablemente esté *overfitteado*), si no el que guardamos con la menor loss del conjunto de validación.\n",
    "Para cargar el mejor modelo entrenado, ejecuten la siguiente celda.\n",
    "\n",
    "Este problema lo pueden solucionar con *early stopping*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-23T22:25:58.680706Z",
     "start_time": "2020-06-23T22:25:58.663725Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "y27CNYfrjtQ-"
   },
   "outputs": [],
   "source": [
    "%%script false\n",
    "# cargar el mejor modelo entrenado.\n",
    "model.load_state_dict(torch.load('{}.pt'.format(model_name)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-23T21:52:04.077979Z",
     "start_time": "2020-06-23T21:52:04.072991Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "t-hSYNGJuO82"
   },
   "outputs": [],
   "source": [
    "# Limpiar ram de cuda\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kBctQHTh0lxD"
   },
   "source": [
    "#### Evaluamos el set de validación con el modelo final\n",
    "\n",
    "Estos son los resultados de predecir el dataset de evaluación con el *mejor* modelo entrenado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-23T22:26:01.788742Z",
     "start_time": "2020-06-23T22:26:00.558829Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "s0gVbP8yiicj"
   },
   "outputs": [],
   "source": [
    "%%script false\n",
    "valid_loss, valid_precision, valid_recall, valid_f1 = evaluate(\n",
    "    model, valid_iterator, criterion)\n",
    "\n",
    "print(\n",
    "    f'Val. Loss: {valid_loss:.3f} |  Val. f1: {valid_f1:.2f} | Val. precision: {valid_precision:.2f} | Val. recall: {valid_recall:.2f}'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qip7Mkislq0D"
   },
   "source": [
    "### Modificaciones al Baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GMNC8uNml6A7"
   },
   "source": [
    "En esta sección se desarrollan los distintos experimentos sobre arquitecturas recurrentes, considerando la base ya provista.\n",
    "\n",
    "En primer lugar se propone variar la cantidad de épocas e implementar *Early Stopping*, para aprovechar al máximo la información en los datos y el hecho de que ya se cuenta con las herramientas para recuperar el ```state``` del modelo con mejor desempeño en validación durante el entrenamiento.\n",
    "\n",
    "La idea es poder incrementar el número de épocas para que los modelos no \"se queden cortos\", pero evitar por medio de *Early Stopping* entrenar la red una vez que ya está en *overfitting* y así no entrenar durante las épocas restantes, en las cuales es esperable no encontrar un modelo mejor que alguno de los ya encontrados."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aFW7kd78pmUU"
   },
   "source": [
    "#### Número de Épocas y *Early Stopping*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ibg04VN2pyYx"
   },
   "outputs": [],
   "source": [
    "def optimize_model(model, train_iterator, valid_iterator, optimizer, \n",
    "                   criterion, nEpochs = 100, stopTolerance = 10, useBest = True):\n",
    "  '''stopTolerance denota la cantidad de épocas sin mejoría antes de terminar \\\n",
    "  el entrenamiento mediante Early Stopping'''\n",
    "  '''useBest denota el criterio de Early Stopping a considerar: si useBest es \\\n",
    "  True, tras <stopTolerance> épocas sin mejorar el mejor resultado obtenido se \\\n",
    "  termina el entrenamiento. Si es False, tras <stopTolerance> épocas \\\n",
    "  consecutivas sin una sola mejora de la Loss se termina el entrenamiento.'''\n",
    "\n",
    "  best_valid_loss = float('inf')\n",
    "  prev_valid_loss = float('inf')\n",
    "  counter = 0\n",
    "\n",
    "  for epoch in range(nEpochs):\n",
    "    \n",
    "      start_time = time.time()\n",
    "\n",
    "      # Recuerdo: train_iterator y valid_iterator contienen el dataset dividido en batches.\n",
    "\n",
    "      # Entrenar\n",
    "      train_loss, train_precision, train_recall, train_f1 = train(\n",
    "          model, train_iterator, optimizer, criterion)\n",
    "\n",
    "      # Evaluar (valid = validación)\n",
    "      valid_loss, valid_precision, valid_recall, valid_f1 = evaluate(\n",
    "          model, valid_iterator, criterion)\n",
    "\n",
    "      end_time = time.time()\n",
    "\n",
    "      epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "\n",
    "      print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
    "      print(\n",
    "          f'\\tTrain Loss: {train_loss:.3f} | Train f1: {train_f1:.2f} | Train precision: {train_precision:.2f} | Train recall: {train_recall:.2f}'\n",
    "      )\n",
    "      print(\n",
    "          f'\\t Val. Loss: {valid_loss:.3f} |  Val. f1: {valid_f1:.2f} |  Val. precision: {valid_precision:.2f} | Val. recall: {valid_recall:.2f}'\n",
    "      )\n",
    "\n",
    "      # Si obtuvimos mejores resultados, guardamos este modelo en el almacenamiento (para poder cargarlo luego)\n",
    "      # Si detienen el entrenamiento prematuramente, pueden cargar el modelo en el siguiente recuadro de código.\n",
    "      if valid_loss < best_valid_loss:\n",
    "          best_valid_loss = valid_loss\n",
    "          torch.save(model.state_dict(), '{}.pt'.format(model_name))\n",
    "          counter = 0\n",
    "\n",
    "      # Si ya no mejoramos el loss de validación, terminamos de entrenar.\n",
    "      else:\n",
    "          if useBest:\n",
    "              counter += 1\n",
    "          \n",
    "          else:\n",
    "              if valid_loss >= prev_valid_loss:\n",
    "                  counter += 1\n",
    "          \n",
    "          if counter == stopTolerance:\n",
    "              break\n",
    "\n",
    "      prev_valid_loss = valid_loss\n",
    "\n",
    "  model.load_state_dict(torch.load('{}.pt'.format(model_name)))\n",
    "  torch.cuda.empty_cache()\n",
    "\n",
    "  valid_loss, valid_precision, valid_recall, valid_f1 = evaluate(\n",
    "    model, valid_iterator, criterion)\n",
    "  \n",
    "  print('\\nPerformance of best found Model:')\n",
    "  print(\n",
    "      f'Val. Loss: {valid_loss:.3f} |  Val. f1: {valid_f1:.2f} | Val. precision: {valid_precision:.2f} | Val. recall: {valid_recall:.2f}'\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FiBrMNw_wyU4"
   },
   "outputs": [],
   "source": [
    "%%script false\n",
    "model.apply(init_weights)\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "optimize_model(model, train_iterator, valid_iterator, optimizer, criterion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oP2Y7BM93InW"
   },
   "source": [
    "#### Tipos de RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dGhiZXcdT6bl"
   },
   "source": [
    "Para poder introducir arquitecturas alternativas a LSTM en los experimentos, se pretende definir una clase equivalente a la anterior, pero incluyendo arquitecturas Elman RNN y GRU, aprovechando que las implementaciones nativas de ```Pytorch``` reciben (casi) los mismos parámetros en su construcción."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fqWKK4MPT8Uk"
   },
   "outputs": [],
   "source": [
    "class NER_ELMAN(nn.Module):\n",
    "    def __init__(self, \n",
    "                 input_dim, \n",
    "                 embedding_dim, \n",
    "                 hidden_dim, \n",
    "                 output_dim,\n",
    "                 n_layers, \n",
    "                 bidirectional, \n",
    "                 dropout, \n",
    "                 pad_idx,\n",
    "                 nonlinearity = 'tanh'):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        # Capa de embedding\n",
    "        self.embedding = nn.Embedding(input_dim,\n",
    "                                      embedding_dim,\n",
    "                                      padding_idx=pad_idx)\n",
    "\n",
    "        # Capa Elman RNN\n",
    "        self.rnn = nn.RNN(embedding_dim,\n",
    "                           hidden_dim,\n",
    "                           num_layers=n_layers,\n",
    "                           bidirectional=bidirectional, \n",
    "                           dropout = dropout if n_layers > 1 else 0,\n",
    "                           nonlinearity = nonlinearity)\n",
    "\n",
    "        # Capa de salida\n",
    "        self.fc = nn.Linear(hidden_dim * 2 if bidirectional else hidden_dim,\n",
    "                            output_dim)\n",
    "\n",
    "        # Dropout\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, text):\n",
    "\n",
    "        #text = [sent len, batch size]\n",
    "\n",
    "        # Convertir lo enviado a embedding\n",
    "        embedded = self.dropout(self.embedding(text))\n",
    "        \n",
    "        outputs, hidden = self.rnn(embedded)\n",
    "        #embedded = [sent len, batch size, emb dim]\n",
    "\n",
    "        # Pasar los embeddings por la rnn (LSTM)\n",
    "\n",
    "        #output = [sent len, batch size, hid dim * n directions]\n",
    "        #hidden/cell = [n layers * n directions, batch size, hid dim]\n",
    "\n",
    "        # Predecir usando la capa de salida.\n",
    "        predictions = self.fc(self.dropout(outputs))\n",
    "        #predictions = [sent len, batch size, output dim]\n",
    "\n",
    "        return predictions\n",
    "\n",
    "class NER_GRU(nn.Module):\n",
    "    def __init__(self, \n",
    "                 input_dim, \n",
    "                 embedding_dim, \n",
    "                 hidden_dim, \n",
    "                 output_dim,\n",
    "                 n_layers, \n",
    "                 bidirectional, \n",
    "                 dropout, \n",
    "                 pad_idx):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        # Capa de embedding\n",
    "        self.embedding = nn.Embedding(input_dim,\n",
    "                                      embedding_dim,\n",
    "                                      padding_idx=pad_idx)\n",
    "\n",
    "        # Capa Elman RNN\n",
    "        self.gru = nn.GRU(embedding_dim,\n",
    "                           hidden_dim,\n",
    "                           num_layers=n_layers,\n",
    "                           bidirectional=bidirectional, \n",
    "                           dropout = dropout if n_layers > 1 else 0)\n",
    "\n",
    "        # Capa de salida\n",
    "        self.fc = nn.Linear(hidden_dim * 2 if bidirectional else hidden_dim,\n",
    "                            output_dim)\n",
    "\n",
    "        # Dropout\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, text):\n",
    "\n",
    "        #text = [sent len, batch size]\n",
    "\n",
    "        # Convertir lo enviado a embedding\n",
    "        embedded = self.dropout(self.embedding(text))\n",
    "        \n",
    "        outputs, hidden = self.gru(embedded)\n",
    "        #embedded = [sent len, batch size, emb dim]\n",
    "\n",
    "        # Pasar los embeddings por la rnn (LSTM)\n",
    "\n",
    "        #output = [sent len, batch size, hid dim * n directions]\n",
    "        #hidden/cell = [n layers * n directions, batch size, hid dim]\n",
    "\n",
    "        # Predecir usando la capa de salida.\n",
    "        predictions = self.fc(self.dropout(outputs))\n",
    "        #predictions = [sent len, batch size, output dim]\n",
    "\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4YL5dQm3WHw7"
   },
   "outputs": [],
   "source": [
    "## Parámetros del Experimento ##\n",
    "\n",
    "# tamaño del vocabulario. recuerden que la entrada son vectores bag of word(one-hot).\n",
    "INPUT_DIM = len(TEXT.vocab)\n",
    "EMBEDDING_DIM = 100  # dimensión de los embeddings.\n",
    "HIDDEN_DIM = 128  # dimensión de la capas LSTM\n",
    "OUTPUT_DIM = len(NER_TAGS.vocab)  # número de clases\n",
    "\n",
    "N_LAYERS = 2  # número de capas.\n",
    "DROPOUT = 0.25\n",
    "BIDIRECTIONAL = False\n",
    "\n",
    "TAG_PAD_IDX = NER_TAGS.vocab.stoi[NER_TAGS.pad_token]\n",
    "baseline_criterion = nn.CrossEntropyLoss(ignore_index = TAG_PAD_IDX)\n",
    "\n",
    "# Creamos nuestros modelos.\n",
    "elman_model = NER_ELMAN(INPUT_DIM, EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM,\n",
    "                         N_LAYERS, BIDIRECTIONAL, DROPOUT, PAD_IDX)\n",
    "\n",
    "elman_model_name = 'Elman1'  # nombre que tendrá el modelo guardado...\n",
    "\n",
    "gru_model = NER_GRU(INPUT_DIM, EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM,\n",
    "                         N_LAYERS, BIDIRECTIONAL, DROPOUT, PAD_IDX)\n",
    "\n",
    "gru_model_name = 'GRU1'  # nombre que tendrá el modelo guardado..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 748
    },
    "colab_type": "code",
    "id": "6Jo7M5dPXIED",
    "outputId": "eb7caa63-f994-46ca-f474-c3d350ce22f7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El modelo actual tiene 2,673,854 parámetros entrenables.\n",
      "Epoch: 01 | Epoch Time: 0m 13s\n",
      "\tTrain Loss: 0.420 | Train f1: 0.20 | Train precision: 0.28 | Train recall: 0.19\n",
      "\t Val. Loss: 0.286 |  Val. f1: 0.36 |  Val. precision: 0.46 | Val. recall: 0.35\n",
      "Epoch: 02 | Epoch Time: 0m 13s\n",
      "\tTrain Loss: 0.174 | Train f1: 0.51 | Train precision: 0.58 | Train recall: 0.50\n",
      "\t Val. Loss: 0.240 |  Val. f1: 0.51 |  Val. precision: 0.62 | Val. recall: 0.49\n",
      "Epoch: 03 | Epoch Time: 0m 16s\n",
      "\tTrain Loss: 0.104 | Train f1: 0.66 | Train precision: 0.71 | Train recall: 0.66\n",
      "\t Val. Loss: 0.265 |  Val. f1: 0.55 |  Val. precision: 0.65 | Val. recall: 0.52\n",
      "Epoch: 04 | Epoch Time: 0m 17s\n",
      "\tTrain Loss: 0.071 | Train f1: 0.74 | Train precision: 0.78 | Train recall: 0.74\n",
      "\t Val. Loss: 0.239 |  Val. f1: 0.58 |  Val. precision: 0.65 | Val. recall: 0.56\n",
      "Epoch: 05 | Epoch Time: 0m 16s\n",
      "\tTrain Loss: 0.053 | Train f1: 0.79 | Train precision: 0.82 | Train recall: 0.79\n",
      "\t Val. Loss: 0.280 |  Val. f1: 0.56 |  Val. precision: 0.64 | Val. recall: 0.55\n",
      "Epoch: 06 | Epoch Time: 0m 16s\n",
      "\tTrain Loss: 0.040 | Train f1: 0.83 | Train precision: 0.85 | Train recall: 0.83\n",
      "\t Val. Loss: 0.281 |  Val. f1: 0.58 |  Val. precision: 0.66 | Val. recall: 0.57\n",
      "Epoch: 07 | Epoch Time: 0m 16s\n",
      "\tTrain Loss: 0.033 | Train f1: 0.86 | Train precision: 0.88 | Train recall: 0.86\n",
      "\t Val. Loss: 0.295 |  Val. f1: 0.57 |  Val. precision: 0.65 | Val. recall: 0.56\n",
      "Epoch: 08 | Epoch Time: 0m 16s\n",
      "\tTrain Loss: 0.030 | Train f1: 0.87 | Train precision: 0.88 | Train recall: 0.86\n",
      "\t Val. Loss: 0.327 |  Val. f1: 0.58 |  Val. precision: 0.66 | Val. recall: 0.56\n",
      "Epoch: 09 | Epoch Time: 0m 16s\n",
      "\tTrain Loss: 0.025 | Train f1: 0.89 | Train precision: 0.90 | Train recall: 0.89\n",
      "\t Val. Loss: 0.334 |  Val. f1: 0.57 |  Val. precision: 0.66 | Val. recall: 0.56\n",
      "Epoch: 10 | Epoch Time: 0m 16s\n",
      "\tTrain Loss: 0.022 | Train f1: 0.90 | Train precision: 0.91 | Train recall: 0.90\n",
      "\t Val. Loss: 0.370 |  Val. f1: 0.56 |  Val. precision: 0.64 | Val. recall: 0.54\n",
      "Epoch: 11 | Epoch Time: 0m 16s\n",
      "\tTrain Loss: 0.018 | Train f1: 0.91 | Train precision: 0.92 | Train recall: 0.91\n",
      "\t Val. Loss: 0.409 |  Val. f1: 0.56 |  Val. precision: 0.65 | Val. recall: 0.55\n",
      "Epoch: 12 | Epoch Time: 0m 16s\n",
      "\tTrain Loss: 0.018 | Train f1: 0.91 | Train precision: 0.92 | Train recall: 0.91\n",
      "\t Val. Loss: 0.346 |  Val. f1: 0.57 |  Val. precision: 0.65 | Val. recall: 0.56\n",
      "Epoch: 13 | Epoch Time: 0m 16s\n",
      "\tTrain Loss: 0.017 | Train f1: 0.92 | Train precision: 0.93 | Train recall: 0.92\n",
      "\t Val. Loss: 0.393 |  Val. f1: 0.57 |  Val. precision: 0.65 | Val. recall: 0.55\n",
      "\n",
      "Performance of best found Model:\n",
      "Val. Loss: 0.239 |  Val. f1: 0.58 | Val. precision: 0.65 | Val. recall: 0.56\n"
     ]
    }
   ],
   "source": [
    "model = elman_model\n",
    "model_name = baseline_model_name\n",
    "criterion = baseline_criterion\n",
    "\n",
    "model.apply(init_weights)\n",
    "print(f'El modelo actual tiene {count_parameters(model):,} parámetros entrenables.')\n",
    "\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "model = model.to(device)\n",
    "criterion = criterion.to(device)\n",
    "\n",
    "optimize_model(model, train_iterator, valid_iterator, optimizer, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 748
    },
    "colab_type": "code",
    "id": "2b3UNl3LhxVo",
    "outputId": "f387068c-04f5-4c03-fbcd-235670621f94"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El modelo actual tiene 2,798,782 parámetros entrenables.\n",
      "Epoch: 01 | Epoch Time: 0m 18s\n",
      "\tTrain Loss: 0.374 | Train f1: 0.25 | Train precision: 0.33 | Train recall: 0.24\n",
      "\t Val. Loss: 0.259 |  Val. f1: 0.46 |  Val. precision: 0.56 | Val. recall: 0.43\n",
      "Epoch: 02 | Epoch Time: 0m 16s\n",
      "\tTrain Loss: 0.141 | Train f1: 0.60 | Train precision: 0.66 | Train recall: 0.59\n",
      "\t Val. Loss: 0.216 |  Val. f1: 0.56 |  Val. precision: 0.65 | Val. recall: 0.54\n",
      "Epoch: 03 | Epoch Time: 0m 15s\n",
      "\tTrain Loss: 0.081 | Train f1: 0.72 | Train precision: 0.76 | Train recall: 0.72\n",
      "\t Val. Loss: 0.199 |  Val. f1: 0.60 |  Val. precision: 0.66 | Val. recall: 0.59\n",
      "Epoch: 04 | Epoch Time: 0m 15s\n",
      "\tTrain Loss: 0.057 | Train f1: 0.78 | Train precision: 0.81 | Train recall: 0.78\n",
      "\t Val. Loss: 0.214 |  Val. f1: 0.59 |  Val. precision: 0.66 | Val. recall: 0.59\n",
      "Epoch: 05 | Epoch Time: 0m 15s\n",
      "\tTrain Loss: 0.041 | Train f1: 0.83 | Train precision: 0.85 | Train recall: 0.83\n",
      "\t Val. Loss: 0.244 |  Val. f1: 0.59 |  Val. precision: 0.67 | Val. recall: 0.58\n",
      "Epoch: 06 | Epoch Time: 0m 14s\n",
      "\tTrain Loss: 0.032 | Train f1: 0.86 | Train precision: 0.87 | Train recall: 0.85\n",
      "\t Val. Loss: 0.244 |  Val. f1: 0.59 |  Val. precision: 0.65 | Val. recall: 0.59\n",
      "Epoch: 07 | Epoch Time: 0m 14s\n",
      "\tTrain Loss: 0.027 | Train f1: 0.88 | Train precision: 0.89 | Train recall: 0.88\n",
      "\t Val. Loss: 0.288 |  Val. f1: 0.58 |  Val. precision: 0.66 | Val. recall: 0.57\n",
      "Epoch: 08 | Epoch Time: 0m 14s\n",
      "\tTrain Loss: 0.024 | Train f1: 0.89 | Train precision: 0.90 | Train recall: 0.89\n",
      "\t Val. Loss: 0.268 |  Val. f1: 0.58 |  Val. precision: 0.65 | Val. recall: 0.58\n",
      "Epoch: 09 | Epoch Time: 0m 14s\n",
      "\tTrain Loss: 0.020 | Train f1: 0.91 | Train precision: 0.91 | Train recall: 0.90\n",
      "\t Val. Loss: 0.273 |  Val. f1: 0.58 |  Val. precision: 0.65 | Val. recall: 0.57\n",
      "Epoch: 10 | Epoch Time: 0m 15s\n",
      "\tTrain Loss: 0.018 | Train f1: 0.92 | Train precision: 0.92 | Train recall: 0.91\n",
      "\t Val. Loss: 0.266 |  Val. f1: 0.60 |  Val. precision: 0.66 | Val. recall: 0.59\n",
      "Epoch: 11 | Epoch Time: 0m 15s\n",
      "\tTrain Loss: 0.016 | Train f1: 0.92 | Train precision: 0.93 | Train recall: 0.92\n",
      "\t Val. Loss: 0.283 |  Val. f1: 0.59 |  Val. precision: 0.65 | Val. recall: 0.59\n",
      "Epoch: 12 | Epoch Time: 0m 14s\n",
      "\tTrain Loss: 0.014 | Train f1: 0.93 | Train precision: 0.94 | Train recall: 0.93\n",
      "\t Val. Loss: 0.306 |  Val. f1: 0.59 |  Val. precision: 0.66 | Val. recall: 0.57\n",
      "Epoch: 13 | Epoch Time: 0m 15s\n",
      "\tTrain Loss: 0.014 | Train f1: 0.93 | Train precision: 0.94 | Train recall: 0.93\n",
      "\t Val. Loss: 0.306 |  Val. f1: 0.59 |  Val. precision: 0.65 | Val. recall: 0.58\n",
      "\n",
      "Performance of best found Model:\n",
      "Val. Loss: 0.199 |  Val. f1: 0.60 | Val. precision: 0.66 | Val. recall: 0.59\n"
     ]
    }
   ],
   "source": [
    "model = gru_model\n",
    "model_name = baseline_model_name\n",
    "criterion = baseline_criterion\n",
    "\n",
    "model.apply(init_weights)\n",
    "print(f'El modelo actual tiene {count_parameters(model):,} parámetros entrenables.')\n",
    "\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "model = model.to(device)\n",
    "criterion = criterion.to(device)\n",
    "\n",
    "optimize_model(model, train_iterator, valid_iterator, optimizer, criterion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1TL7uR4up7D-"
   },
   "source": [
    "La Elman RNN de Pytorch acepta cambiar la función de activación por ReLU, por lo que probaremos el mismo modelo anteriormente visto, pero con esta nueva no-linealidad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 799
    },
    "colab_type": "code",
    "id": "1_HGhLGOp0bw",
    "outputId": "63fd10f9-9a13-47ae-887e-757515b38b1c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El modelo actual tiene 2,673,854 parámetros entrenables.\n",
      "Epoch: 01 | Epoch Time: 0m 16s\n",
      "\tTrain Loss: 0.426 | Train f1: 0.15 | Train precision: 0.20 | Train recall: 0.14\n",
      "\t Val. Loss: 0.290 |  Val. f1: 0.33 |  Val. precision: 0.38 | Val. recall: 0.34\n",
      "Epoch: 02 | Epoch Time: 0m 16s\n",
      "\tTrain Loss: 0.187 | Train f1: 0.42 | Train precision: 0.47 | Train recall: 0.43\n",
      "\t Val. Loss: 0.246 |  Val. f1: 0.46 |  Val. precision: 0.56 | Val. recall: 0.46\n",
      "Epoch: 03 | Epoch Time: 0m 16s\n",
      "\tTrain Loss: 0.119 | Train f1: 0.60 | Train precision: 0.65 | Train recall: 0.60\n",
      "\t Val. Loss: 0.215 |  Val. f1: 0.53 |  Val. precision: 0.61 | Val. recall: 0.52\n",
      "Epoch: 04 | Epoch Time: 0m 16s\n",
      "\tTrain Loss: 0.085 | Train f1: 0.70 | Train precision: 0.73 | Train recall: 0.70\n",
      "\t Val. Loss: 0.209 |  Val. f1: 0.57 |  Val. precision: 0.64 | Val. recall: 0.56\n",
      "Epoch: 05 | Epoch Time: 0m 16s\n",
      "\tTrain Loss: 0.065 | Train f1: 0.76 | Train precision: 0.79 | Train recall: 0.76\n",
      "\t Val. Loss: 0.215 |  Val. f1: 0.58 |  Val. precision: 0.65 | Val. recall: 0.58\n",
      "Epoch: 06 | Epoch Time: 0m 16s\n",
      "\tTrain Loss: 0.052 | Train f1: 0.79 | Train precision: 0.82 | Train recall: 0.79\n",
      "\t Val. Loss: 0.212 |  Val. f1: 0.60 |  Val. precision: 0.65 | Val. recall: 0.59\n",
      "Epoch: 07 | Epoch Time: 0m 16s\n",
      "\tTrain Loss: 0.042 | Train f1: 0.83 | Train precision: 0.85 | Train recall: 0.83\n",
      "\t Val. Loss: 0.258 |  Val. f1: 0.57 |  Val. precision: 0.65 | Val. recall: 0.56\n",
      "Epoch: 08 | Epoch Time: 0m 16s\n",
      "\tTrain Loss: 0.036 | Train f1: 0.85 | Train precision: 0.86 | Train recall: 0.85\n",
      "\t Val. Loss: 0.261 |  Val. f1: 0.58 |  Val. precision: 0.66 | Val. recall: 0.57\n",
      "Epoch: 09 | Epoch Time: 0m 16s\n",
      "\tTrain Loss: 0.031 | Train f1: 0.86 | Train precision: 0.87 | Train recall: 0.86\n",
      "\t Val. Loss: 0.233 |  Val. f1: 0.59 |  Val. precision: 0.64 | Val. recall: 0.59\n",
      "Epoch: 10 | Epoch Time: 0m 16s\n",
      "\tTrain Loss: 0.026 | Train f1: 0.88 | Train precision: 0.89 | Train recall: 0.88\n",
      "\t Val. Loss: 0.285 |  Val. f1: 0.58 |  Val. precision: 0.65 | Val. recall: 0.58\n",
      "Epoch: 11 | Epoch Time: 0m 16s\n",
      "\tTrain Loss: 0.025 | Train f1: 0.89 | Train precision: 0.90 | Train recall: 0.88\n",
      "\t Val. Loss: 0.243 |  Val. f1: 0.60 |  Val. precision: 0.65 | Val. recall: 0.60\n",
      "Epoch: 12 | Epoch Time: 0m 16s\n",
      "\tTrain Loss: 0.021 | Train f1: 0.90 | Train precision: 0.91 | Train recall: 0.90\n",
      "\t Val. Loss: 0.299 |  Val. f1: 0.59 |  Val. precision: 0.66 | Val. recall: 0.59\n",
      "Epoch: 13 | Epoch Time: 0m 16s\n",
      "\tTrain Loss: 0.019 | Train f1: 0.90 | Train precision: 0.91 | Train recall: 0.90\n",
      "\t Val. Loss: 0.273 |  Val. f1: 0.59 |  Val. precision: 0.64 | Val. recall: 0.60\n",
      "Epoch: 14 | Epoch Time: 0m 16s\n",
      "\tTrain Loss: 0.020 | Train f1: 0.90 | Train precision: 0.91 | Train recall: 0.90\n",
      "\t Val. Loss: 0.279 |  Val. f1: 0.59 |  Val. precision: 0.66 | Val. recall: 0.58\n",
      "\n",
      "Performance of best found Model:\n",
      "Val. Loss: 0.209 |  Val. f1: 0.57 | Val. precision: 0.64 | Val. recall: 0.56\n"
     ]
    }
   ],
   "source": [
    "elman_model = NER_ELMAN(INPUT_DIM, EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM,\n",
    "                         N_LAYERS, BIDIRECTIONAL, DROPOUT, PAD_IDX, 'relu')\n",
    "\n",
    "elman_model_name = 'Elman2'  # nombre que tendrá el modelo guardado...\n",
    "\n",
    "model = elman_model\n",
    "model_name = baseline_model_name\n",
    "criterion = baseline_criterion\n",
    "\n",
    "model.apply(init_weights)\n",
    "print(f'El modelo actual tiene {count_parameters(model):,} parámetros entrenables.')\n",
    "\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "model = model.to(device)\n",
    "criterion = criterion.to(device)\n",
    "\n",
    "optimize_model(model, train_iterator, valid_iterator, optimizer, criterion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jXeRp2SsxdJ9"
   },
   "source": [
    "#### Bidireccionalidad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Jb_HrwaNxfIC"
   },
   "source": [
    "A continuación, se incluye en los modelos anteriores bidireccionalidad para estudiar cómo afecta esto en particular a cada uno. Para le red Elman RNN se considerará solo la arquitectura con ```ReLU``` como función de activación dado que ya se verificó su mejor *performance* con esta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LvSbJ1ROxp7F"
   },
   "outputs": [],
   "source": [
    "## Parámetros del Experimento ##\n",
    "\n",
    "# tamaño del vocabulario. recuerden que la entrada son vectores bag of word(one-hot).\n",
    "INPUT_DIM = len(TEXT.vocab)\n",
    "EMBEDDING_DIM = 100  # dimensión de los embeddings.\n",
    "HIDDEN_DIM = 128  # dimensión de la capas LSTM\n",
    "OUTPUT_DIM = len(NER_TAGS.vocab)  # número de clases\n",
    "\n",
    "N_LAYERS = 2  # número de capas.\n",
    "DROPOUT = 0.25\n",
    "BIDIRECTIONAL = True\n",
    "\n",
    "TAG_PAD_IDX = NER_TAGS.vocab.stoi[NER_TAGS.pad_token]\n",
    "baseline_criterion = nn.CrossEntropyLoss(ignore_index = TAG_PAD_IDX)\n",
    "\n",
    "# Creamos nuestros modelos.\n",
    "lstm_model = NER_RNN(INPUT_DIM, EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM,\n",
    "                         N_LAYERS, BIDIRECTIONAL, DROPOUT, PAD_IDX)\n",
    "\n",
    "lstm_model_name = 'LSTM'  # nombre que tendrá el modelo guardado...\n",
    "\n",
    "elman_model = NER_ELMAN(INPUT_DIM, EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM,\n",
    "                         N_LAYERS, BIDIRECTIONAL, DROPOUT, PAD_IDX, 'relu')\n",
    "\n",
    "elman_model_name = 'Elman'  # nombre que tendrá el modelo guardado...\n",
    "\n",
    "gru_model = NER_GRU(INPUT_DIM, EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM,\n",
    "                         N_LAYERS, BIDIRECTIONAL, DROPOUT, PAD_IDX)\n",
    "\n",
    "gru_model_name = 'GRU1'  # nombre que tendrá el modelo guardado..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 748
    },
    "colab_type": "code",
    "id": "FWBANZxryVpS",
    "outputId": "7b7bdaf6-b933-4d00-997e-3467a12674f8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El modelo actual tiene 3,243,454 parámetros entrenables.\n",
      "Epoch: 01 | Epoch Time: 0m 29s\n",
      "\tTrain Loss: 0.357 | Train f1: 0.26 | Train precision: 0.33 | Train recall: 0.25\n",
      "\t Val. Loss: 0.243 |  Val. f1: 0.47 |  Val. precision: 0.58 | Val. recall: 0.46\n",
      "Epoch: 02 | Epoch Time: 0m 25s\n",
      "\tTrain Loss: 0.123 | Train f1: 0.61 | Train precision: 0.66 | Train recall: 0.61\n",
      "\t Val. Loss: 0.190 |  Val. f1: 0.58 |  Val. precision: 0.64 | Val. recall: 0.58\n",
      "Epoch: 03 | Epoch Time: 0m 24s\n",
      "\tTrain Loss: 0.066 | Train f1: 0.75 | Train precision: 0.78 | Train recall: 0.75\n",
      "\t Val. Loss: 0.194 |  Val. f1: 0.58 |  Val. precision: 0.65 | Val. recall: 0.58\n",
      "Epoch: 04 | Epoch Time: 0m 24s\n",
      "\tTrain Loss: 0.042 | Train f1: 0.82 | Train precision: 0.84 | Train recall: 0.82\n",
      "\t Val. Loss: 0.186 |  Val. f1: 0.62 |  Val. precision: 0.66 | Val. recall: 0.63\n",
      "Epoch: 05 | Epoch Time: 0m 24s\n",
      "\tTrain Loss: 0.029 | Train f1: 0.87 | Train precision: 0.89 | Train recall: 0.87\n",
      "\t Val. Loss: 0.216 |  Val. f1: 0.61 |  Val. precision: 0.67 | Val. recall: 0.61\n",
      "Epoch: 06 | Epoch Time: 0m 24s\n",
      "\tTrain Loss: 0.022 | Train f1: 0.90 | Train precision: 0.91 | Train recall: 0.90\n",
      "\t Val. Loss: 0.212 |  Val. f1: 0.62 |  Val. precision: 0.67 | Val. recall: 0.62\n",
      "Epoch: 07 | Epoch Time: 0m 24s\n",
      "\tTrain Loss: 0.017 | Train f1: 0.92 | Train precision: 0.93 | Train recall: 0.92\n",
      "\t Val. Loss: 0.224 |  Val. f1: 0.62 |  Val. precision: 0.67 | Val. recall: 0.62\n",
      "Epoch: 08 | Epoch Time: 0m 24s\n",
      "\tTrain Loss: 0.014 | Train f1: 0.93 | Train precision: 0.94 | Train recall: 0.93\n",
      "\t Val. Loss: 0.233 |  Val. f1: 0.61 |  Val. precision: 0.67 | Val. recall: 0.61\n",
      "Epoch: 09 | Epoch Time: 0m 24s\n",
      "\tTrain Loss: 0.012 | Train f1: 0.94 | Train precision: 0.95 | Train recall: 0.94\n",
      "\t Val. Loss: 0.244 |  Val. f1: 0.62 |  Val. precision: 0.68 | Val. recall: 0.62\n",
      "Epoch: 10 | Epoch Time: 0m 24s\n",
      "\tTrain Loss: 0.010 | Train f1: 0.95 | Train precision: 0.96 | Train recall: 0.95\n",
      "\t Val. Loss: 0.253 |  Val. f1: 0.61 |  Val. precision: 0.68 | Val. recall: 0.61\n",
      "Epoch: 11 | Epoch Time: 0m 24s\n",
      "\tTrain Loss: 0.009 | Train f1: 0.96 | Train precision: 0.96 | Train recall: 0.96\n",
      "\t Val. Loss: 0.279 |  Val. f1: 0.60 |  Val. precision: 0.67 | Val. recall: 0.60\n",
      "Epoch: 12 | Epoch Time: 0m 24s\n",
      "\tTrain Loss: 0.008 | Train f1: 0.96 | Train precision: 0.96 | Train recall: 0.96\n",
      "\t Val. Loss: 0.266 |  Val. f1: 0.62 |  Val. precision: 0.68 | Val. recall: 0.62\n",
      "Epoch: 13 | Epoch Time: 0m 24s\n",
      "\tTrain Loss: 0.007 | Train f1: 0.97 | Train precision: 0.97 | Train recall: 0.96\n",
      "\t Val. Loss: 0.259 |  Val. f1: 0.62 |  Val. precision: 0.68 | Val. recall: 0.61\n",
      "\n",
      "Performance of best found Model:\n",
      "Val. Loss: 0.186 |  Val. f1: 0.62 | Val. precision: 0.66 | Val. recall: 0.63\n"
     ]
    }
   ],
   "source": [
    "model = lstm_model\n",
    "model_name = lstm_model_name\n",
    "criterion = baseline_criterion\n",
    "\n",
    "model.apply(init_weights)\n",
    "print(f'El modelo actual tiene {count_parameters(model):,} parámetros entrenables.')\n",
    "\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "model = model.to(device)\n",
    "criterion = criterion.to(device)\n",
    "\n",
    "optimize_model(model, train_iterator, valid_iterator, optimizer, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 887
    },
    "colab_type": "code",
    "id": "hxaQYCSB7n3n",
    "outputId": "a74ef0fb-d245-4e2f-cb16-444e353b5ad5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El modelo actual tiene 2,770,366 parámetros entrenables.\n",
      "Epoch: 01 | Epoch Time: 0m 8s\n",
      "\tTrain Loss: 0.375 | Train f1: 0.23 | Train precision: 0.30 | Train recall: 0.21\n",
      "\t Val. Loss: 0.255 |  Val. f1: 0.43 |  Val. precision: 0.50 | Val. recall: 0.43\n",
      "Epoch: 02 | Epoch Time: 0m 8s\n",
      "\tTrain Loss: 0.150 | Train f1: 0.54 | Train precision: 0.60 | Train recall: 0.53\n",
      "\t Val. Loss: 0.203 |  Val. f1: 0.54 |  Val. precision: 0.61 | Val. recall: 0.55\n",
      "Epoch: 03 | Epoch Time: 0m 8s\n",
      "\tTrain Loss: 0.088 | Train f1: 0.69 | Train precision: 0.73 | Train recall: 0.69\n",
      "\t Val. Loss: 0.191 |  Val. f1: 0.59 |  Val. precision: 0.65 | Val. recall: 0.60\n",
      "Epoch: 04 | Epoch Time: 0m 8s\n",
      "\tTrain Loss: 0.059 | Train f1: 0.77 | Train precision: 0.80 | Train recall: 0.77\n",
      "\t Val. Loss: 0.180 |  Val. f1: 0.61 |  Val. precision: 0.65 | Val. recall: 0.62\n",
      "Epoch: 05 | Epoch Time: 0m 8s\n",
      "\tTrain Loss: 0.042 | Train f1: 0.83 | Train precision: 0.85 | Train recall: 0.83\n",
      "\t Val. Loss: 0.199 |  Val. f1: 0.62 |  Val. precision: 0.67 | Val. recall: 0.63\n",
      "Epoch: 06 | Epoch Time: 0m 8s\n",
      "\tTrain Loss: 0.032 | Train f1: 0.86 | Train precision: 0.87 | Train recall: 0.86\n",
      "\t Val. Loss: 0.207 |  Val. f1: 0.61 |  Val. precision: 0.66 | Val. recall: 0.63\n",
      "Epoch: 07 | Epoch Time: 0m 8s\n",
      "\tTrain Loss: 0.026 | Train f1: 0.88 | Train precision: 0.89 | Train recall: 0.88\n",
      "\t Val. Loss: 0.221 |  Val. f1: 0.60 |  Val. precision: 0.66 | Val. recall: 0.61\n",
      "Epoch: 08 | Epoch Time: 0m 8s\n",
      "\tTrain Loss: 0.020 | Train f1: 0.90 | Train precision: 0.91 | Train recall: 0.90\n",
      "\t Val. Loss: 0.226 |  Val. f1: 0.62 |  Val. precision: 0.66 | Val. recall: 0.63\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/numpy/lib/function_base.py:393: RuntimeWarning: Mean of empty slice.\n",
      "  avg = a.mean(axis)\n",
      "/usr/local/lib/python3.6/dist-packages/numpy/core/_methods.py:161: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 09 | Epoch Time: 0m 8s\n",
      "\tTrain Loss: 0.017 | Train f1: nan | Train precision: nan | Train recall: nan\n",
      "\t Val. Loss: 0.234 |  Val. f1: 0.63 |  Val. precision: 0.67 | Val. recall: 0.64\n",
      "Epoch: 10 | Epoch Time: 0m 8s\n",
      "\tTrain Loss: 0.015 | Train f1: 0.93 | Train precision: 0.93 | Train recall: 0.93\n",
      "\t Val. Loss: 0.257 |  Val. f1: 0.61 |  Val. precision: 0.67 | Val. recall: 0.62\n",
      "Epoch: 11 | Epoch Time: 0m 8s\n",
      "\tTrain Loss: 0.013 | Train f1: 0.94 | Train precision: 0.94 | Train recall: 0.94\n",
      "\t Val. Loss: 0.252 |  Val. f1: 0.61 |  Val. precision: 0.66 | Val. recall: 0.62\n",
      "Epoch: 12 | Epoch Time: 0m 8s\n",
      "\tTrain Loss: 0.012 | Train f1: 0.94 | Train precision: 0.94 | Train recall: 0.94\n",
      "\t Val. Loss: 0.273 |  Val. f1: 0.61 |  Val. precision: 0.66 | Val. recall: 0.61\n",
      "Epoch: 13 | Epoch Time: 0m 8s\n",
      "\tTrain Loss: 0.012 | Train f1: 0.95 | Train precision: 0.95 | Train recall: 0.95\n",
      "\t Val. Loss: 0.259 |  Val. f1: 0.61 |  Val. precision: 0.65 | Val. recall: 0.63\n",
      "Epoch: 14 | Epoch Time: 0m 8s\n",
      "\tTrain Loss: 0.010 | Train f1: 0.95 | Train precision: 0.96 | Train recall: 0.95\n",
      "\t Val. Loss: 0.254 |  Val. f1: 0.63 |  Val. precision: 0.68 | Val. recall: 0.63\n",
      "\n",
      "Performance of best found Model:\n",
      "Val. Loss: 0.180 |  Val. f1: 0.61 | Val. precision: 0.65 | Val. recall: 0.62\n"
     ]
    }
   ],
   "source": [
    "model = elman_model\n",
    "model_name = elman_model_name\n",
    "criterion = baseline_criterion\n",
    "\n",
    "model.apply(init_weights)\n",
    "print(f'El modelo actual tiene {count_parameters(model):,} parámetros entrenables.')\n",
    "\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "model = model.to(device)\n",
    "criterion = criterion.to(device)\n",
    "\n",
    "optimize_model(model, train_iterator, valid_iterator, optimizer, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 748
    },
    "colab_type": "code",
    "id": "ZQfhXntp74kt",
    "outputId": "1155d48d-0b05-44d4-daee-e843f820b56b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El modelo actual tiene 3,085,758 parámetros entrenables.\n",
      "Epoch: 01 | Epoch Time: 0m 25s\n",
      "\tTrain Loss: 0.325 | Train f1: 0.33 | Train precision: 0.42 | Train recall: 0.30\n",
      "\t Val. Loss: 0.225 |  Val. f1: 0.51 |  Val. precision: 0.60 | Val. recall: 0.49\n",
      "Epoch: 02 | Epoch Time: 0m 26s\n",
      "\tTrain Loss: 0.111 | Train f1: 0.67 | Train precision: 0.72 | Train recall: 0.66\n",
      "\t Val. Loss: 0.186 |  Val. f1: 0.58 |  Val. precision: 0.65 | Val. recall: 0.56\n",
      "Epoch: 03 | Epoch Time: 0m 25s\n",
      "\tTrain Loss: 0.058 | Train f1: 0.78 | Train precision: 0.81 | Train recall: 0.78\n",
      "\t Val. Loss: 0.180 |  Val. f1: 0.62 |  Val. precision: 0.68 | Val. recall: 0.61\n",
      "Epoch: 04 | Epoch Time: 0m 24s\n",
      "\tTrain Loss: 0.036 | Train f1: 0.85 | Train precision: 0.87 | Train recall: 0.85\n",
      "\t Val. Loss: 0.202 |  Val. f1: 0.62 |  Val. precision: 0.69 | Val. recall: 0.61\n",
      "Epoch: 05 | Epoch Time: 0m 24s\n",
      "\tTrain Loss: 0.025 | Train f1: 0.88 | Train precision: 0.89 | Train recall: 0.88\n",
      "\t Val. Loss: 0.218 |  Val. f1: 0.62 |  Val. precision: 0.68 | Val. recall: 0.61\n",
      "Epoch: 06 | Epoch Time: 0m 25s\n",
      "\tTrain Loss: 0.018 | Train f1: 0.92 | Train precision: 0.92 | Train recall: 0.91\n",
      "\t Val. Loss: 0.247 |  Val. f1: 0.60 |  Val. precision: 0.67 | Val. recall: 0.61\n",
      "Epoch: 07 | Epoch Time: 0m 24s\n",
      "\tTrain Loss: 0.015 | Train f1: 0.93 | Train precision: 0.93 | Train recall: 0.93\n",
      "\t Val. Loss: 0.258 |  Val. f1: 0.60 |  Val. precision: 0.66 | Val. recall: 0.61\n",
      "Epoch: 08 | Epoch Time: 0m 26s\n",
      "\tTrain Loss: 0.012 | Train f1: 0.94 | Train precision: 0.95 | Train recall: 0.94\n",
      "\t Val. Loss: 0.256 |  Val. f1: 0.62 |  Val. precision: 0.68 | Val. recall: 0.61\n",
      "Epoch: 09 | Epoch Time: 0m 23s\n",
      "\tTrain Loss: 0.011 | Train f1: 0.95 | Train precision: 0.95 | Train recall: 0.95\n",
      "\t Val. Loss: 0.254 |  Val. f1: 0.62 |  Val. precision: 0.68 | Val. recall: 0.62\n",
      "Epoch: 10 | Epoch Time: 0m 25s\n",
      "\tTrain Loss: 0.009 | Train f1: 0.96 | Train precision: 0.96 | Train recall: 0.96\n",
      "\t Val. Loss: 0.270 |  Val. f1: 0.63 |  Val. precision: 0.69 | Val. recall: 0.63\n",
      "Epoch: 11 | Epoch Time: 0m 24s\n",
      "\tTrain Loss: 0.008 | Train f1: 0.96 | Train precision: 0.97 | Train recall: 0.96\n",
      "\t Val. Loss: 0.316 |  Val. f1: 0.61 |  Val. precision: 0.67 | Val. recall: 0.60\n",
      "Epoch: 12 | Epoch Time: 0m 25s\n",
      "\tTrain Loss: 0.008 | Train f1: 0.96 | Train precision: 0.97 | Train recall: 0.96\n",
      "\t Val. Loss: 0.343 |  Val. f1: 0.61 |  Val. precision: 0.69 | Val. recall: 0.59\n",
      "Epoch: 13 | Epoch Time: 0m 25s\n",
      "\tTrain Loss: 0.007 | Train f1: 0.96 | Train precision: 0.97 | Train recall: 0.96\n",
      "\t Val. Loss: 0.317 |  Val. f1: 0.62 |  Val. precision: 0.68 | Val. recall: 0.61\n",
      "\n",
      "Performance of best found Model:\n",
      "Val. Loss: 0.180 |  Val. f1: 0.62 | Val. precision: 0.68 | Val. recall: 0.61\n"
     ]
    }
   ],
   "source": [
    "model = gru_model\n",
    "model_name = gru_model_name\n",
    "criterion = baseline_criterion\n",
    "\n",
    "model.apply(init_weights)\n",
    "print(f'El modelo actual tiene {count_parameters(model):,} parámetros entrenables.')\n",
    "\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "model = model.to(device)\n",
    "criterion = criterion.to(device)\n",
    "\n",
    "optimize_model(model, train_iterator, valid_iterator, optimizer, criterion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WFBccaBwcl7D"
   },
   "source": [
    "#### Tamaño de las Redes Recurrentes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PFZui57CcvdF"
   },
   "source": [
    "Habiendo constatado que los modelos estudiados mejoran consistentemente con la inclusión de bidireccionalidad, variaremos el tamaño (```HIDDEN_DIM```) de las capas ocultas que conforman las redes recurrentes de cada modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HOdLaxZQcN5P"
   },
   "outputs": [],
   "source": [
    "## Parámetros del Experimento ##\n",
    "\n",
    "# tamaño del vocabulario. recuerden que la entrada son vectores bag of word(one-hot).\n",
    "INPUT_DIM = len(TEXT.vocab)\n",
    "EMBEDDING_DIM = 100  # dimensión de los embeddings.\n",
    "HIDDEN_DIM = 64  # dimensión de la capas RNN\n",
    "OUTPUT_DIM = len(NER_TAGS.vocab)  # número de clases\n",
    "\n",
    "N_LAYERS = 2  # número de capas.\n",
    "DROPOUT = 0.25\n",
    "BIDIRECTIONAL = True\n",
    "\n",
    "TAG_PAD_IDX = NER_TAGS.vocab.stoi[NER_TAGS.pad_token]\n",
    "baseline_criterion = nn.CrossEntropyLoss(ignore_index = TAG_PAD_IDX)\n",
    "\n",
    "# Creamos nuestros modelos.\n",
    "lstm_model = NER_RNN(INPUT_DIM, EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM,\n",
    "                         N_LAYERS, BIDIRECTIONAL, DROPOUT, PAD_IDX)\n",
    "\n",
    "lstm_model_name = 'LSTM'  # nombre que tendrá el modelo guardado...\n",
    "\n",
    "elman_model = NER_ELMAN(INPUT_DIM, EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM,\n",
    "                         N_LAYERS, BIDIRECTIONAL, DROPOUT, PAD_IDX, 'relu')\n",
    "\n",
    "elman_model_name = 'Elman'  # nombre que tendrá el modelo guardado...\n",
    "\n",
    "gru_model = NER_GRU(INPUT_DIM, EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM,\n",
    "                         N_LAYERS, BIDIRECTIONAL, DROPOUT, PAD_IDX)\n",
    "\n",
    "gru_model_name = 'GRU'  # nombre que tendrá el modelo guardado..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 799
    },
    "colab_type": "code",
    "id": "TgystZb8dd4l",
    "outputId": "78a006aa-84df-465c-cf8e-d994c4068fce"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El modelo actual tiene 2,795,710 parámetros entrenables.\n",
      "Epoch: 01 | Epoch Time: 0m 8s\n",
      "\tTrain Loss: 0.420 | Train f1: 0.17 | Train precision: 0.20 | Train recall: 0.17\n",
      "\t Val. Loss: 0.273 |  Val. f1: 0.35 |  Val. precision: 0.41 | Val. recall: 0.35\n",
      "Epoch: 02 | Epoch Time: 0m 8s\n",
      "\tTrain Loss: 0.155 | Train f1: 0.52 | Train precision: 0.57 | Train recall: 0.52\n",
      "\t Val. Loss: 0.231 |  Val. f1: 0.52 |  Val. precision: 0.60 | Val. recall: 0.51\n",
      "Epoch: 03 | Epoch Time: 0m 8s\n",
      "\tTrain Loss: 0.085 | Train f1: 0.71 | Train precision: 0.75 | Train recall: 0.71\n",
      "\t Val. Loss: 0.208 |  Val. f1: 0.57 |  Val. precision: 0.64 | Val. recall: 0.57\n",
      "Epoch: 04 | Epoch Time: 0m 8s\n",
      "\tTrain Loss: 0.055 | Train f1: 0.79 | Train precision: 0.81 | Train recall: 0.79\n",
      "\t Val. Loss: 0.203 |  Val. f1: 0.60 |  Val. precision: 0.66 | Val. recall: 0.59\n",
      "Epoch: 05 | Epoch Time: 0m 8s\n",
      "\tTrain Loss: 0.038 | Train f1: 0.85 | Train precision: 0.86 | Train recall: 0.85\n",
      "\t Val. Loss: 0.226 |  Val. f1: 0.60 |  Val. precision: 0.66 | Val. recall: 0.61\n",
      "Epoch: 06 | Epoch Time: 0m 8s\n",
      "\tTrain Loss: 0.029 | Train f1: 0.88 | Train precision: 0.89 | Train recall: 0.88\n",
      "\t Val. Loss: 0.238 |  Val. f1: 0.60 |  Val. precision: 0.66 | Val. recall: 0.60\n",
      "Epoch: 07 | Epoch Time: 0m 8s\n",
      "\tTrain Loss: 0.023 | Train f1: 0.90 | Train precision: 0.91 | Train recall: 0.90\n",
      "\t Val. Loss: 0.232 |  Val. f1: 0.61 |  Val. precision: 0.66 | Val. recall: 0.61\n",
      "Epoch: 08 | Epoch Time: 0m 8s\n",
      "\tTrain Loss: 0.019 | Train f1: 0.92 | Train precision: 0.92 | Train recall: 0.92\n",
      "\t Val. Loss: 0.256 |  Val. f1: 0.60 |  Val. precision: 0.66 | Val. recall: 0.61\n",
      "Epoch: 09 | Epoch Time: 0m 8s\n",
      "\tTrain Loss: 0.015 | Train f1: 0.93 | Train precision: 0.94 | Train recall: 0.93\n",
      "\t Val. Loss: 0.245 |  Val. f1: 0.61 |  Val. precision: 0.67 | Val. recall: 0.62\n",
      "Epoch: 10 | Epoch Time: 0m 8s\n",
      "\tTrain Loss: 0.013 | Train f1: 0.94 | Train precision: 0.95 | Train recall: 0.94\n",
      "\t Val. Loss: 0.265 |  Val. f1: 0.60 |  Val. precision: 0.66 | Val. recall: 0.60\n",
      "Epoch: 11 | Epoch Time: 0m 8s\n",
      "\tTrain Loss: 0.011 | Train f1: 0.95 | Train precision: 0.95 | Train recall: 0.95\n",
      "\t Val. Loss: 0.275 |  Val. f1: 0.61 |  Val. precision: 0.67 | Val. recall: 0.61\n",
      "Epoch: 12 | Epoch Time: 0m 8s\n",
      "\tTrain Loss: 0.010 | Train f1: 0.95 | Train precision: 0.95 | Train recall: 0.95\n",
      "\t Val. Loss: 0.270 |  Val. f1: 0.61 |  Val. precision: 0.66 | Val. recall: 0.62\n",
      "Epoch: 13 | Epoch Time: 0m 8s\n",
      "\tTrain Loss: 0.008 | Train f1: 0.96 | Train precision: 0.96 | Train recall: 0.96\n",
      "\t Val. Loss: 0.277 |  Val. f1: 0.62 |  Val. precision: 0.66 | Val. recall: 0.63\n",
      "Epoch: 14 | Epoch Time: 0m 8s\n",
      "\tTrain Loss: 0.008 | Train f1: 0.96 | Train precision: 0.97 | Train recall: 0.96\n",
      "\t Val. Loss: 0.291 |  Val. f1: 0.61 |  Val. precision: 0.66 | Val. recall: 0.62\n",
      "\n",
      "Performance of best found Model:\n",
      "Val. Loss: 0.203 |  Val. f1: 0.60 | Val. precision: 0.66 | Val. recall: 0.59\n"
     ]
    }
   ],
   "source": [
    "model = lstm_model\n",
    "model_name = lstm_model_name\n",
    "criterion = baseline_criterion\n",
    "\n",
    "model.apply(init_weights)\n",
    "print(f'El modelo actual tiene {count_parameters(model):,} parámetros entrenables.')\n",
    "\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "model = model.to(device)\n",
    "criterion = criterion.to(device)\n",
    "\n",
    "optimize_model(model, train_iterator, valid_iterator, optimizer, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 697
    },
    "colab_type": "code",
    "id": "mw6jAfr3g-mV",
    "outputId": "243094c1-a506-4d1c-b543-3631e6326acb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El modelo actual tiene 2,657,470 parámetros entrenables.\n",
      "Epoch: 01 | Epoch Time: 0m 8s\n",
      "\tTrain Loss: 0.375 | Train f1: 0.21 | Train precision: 0.26 | Train recall: 0.20\n",
      "\t Val. Loss: 0.248 |  Val. f1: 0.37 |  Val. precision: 0.42 | Val. recall: 0.39\n",
      "Epoch: 02 | Epoch Time: 0m 8s\n",
      "\tTrain Loss: 0.150 | Train f1: 0.51 | Train precision: 0.57 | Train recall: 0.52\n",
      "\t Val. Loss: 0.198 |  Val. f1: 0.53 |  Val. precision: 0.60 | Val. recall: 0.53\n",
      "Epoch: 03 | Epoch Time: 0m 8s\n",
      "\tTrain Loss: 0.090 | Train f1: 0.68 | Train precision: 0.71 | Train recall: 0.68\n",
      "\t Val. Loss: 0.208 |  Val. f1: 0.58 |  Val. precision: 0.63 | Val. recall: 0.59\n",
      "Epoch: 04 | Epoch Time: 0m 8s\n",
      "\tTrain Loss: 0.063 | Train f1: 0.76 | Train precision: 0.78 | Train recall: 0.76\n",
      "\t Val. Loss: 0.218 |  Val. f1: 0.58 |  Val. precision: 0.64 | Val. recall: 0.58\n",
      "Epoch: 05 | Epoch Time: 0m 8s\n",
      "\tTrain Loss: 0.046 | Train f1: 0.81 | Train precision: 0.83 | Train recall: 0.81\n",
      "\t Val. Loss: 0.203 |  Val. f1: 0.60 |  Val. precision: 0.65 | Val. recall: 0.61\n",
      "Epoch: 06 | Epoch Time: 0m 8s\n",
      "\tTrain Loss: 0.035 | Train f1: 0.85 | Train precision: 0.86 | Train recall: 0.85\n",
      "\t Val. Loss: 0.210 |  Val. f1: 0.62 |  Val. precision: 0.67 | Val. recall: 0.63\n",
      "Epoch: 07 | Epoch Time: 0m 8s\n",
      "\tTrain Loss: 0.028 | Train f1: 0.88 | Train precision: 0.89 | Train recall: 0.88\n",
      "\t Val. Loss: 0.210 |  Val. f1: 0.62 |  Val. precision: 0.66 | Val. recall: 0.63\n",
      "Epoch: 08 | Epoch Time: 0m 8s\n",
      "\tTrain Loss: 0.022 | Train f1: 0.89 | Train precision: 0.90 | Train recall: 0.89\n",
      "\t Val. Loss: 0.223 |  Val. f1: 0.62 |  Val. precision: 0.67 | Val. recall: 0.63\n",
      "Epoch: 09 | Epoch Time: 0m 8s\n",
      "\tTrain Loss: 0.019 | Train f1: 0.91 | Train precision: 0.92 | Train recall: 0.91\n",
      "\t Val. Loss: 0.231 |  Val. f1: 0.63 |  Val. precision: 0.67 | Val. recall: 0.64\n",
      "Epoch: 10 | Epoch Time: 0m 8s\n",
      "\tTrain Loss: 0.016 | Train f1: 0.92 | Train precision: 0.93 | Train recall: 0.92\n",
      "\t Val. Loss: 0.244 |  Val. f1: 0.61 |  Val. precision: 0.66 | Val. recall: 0.61\n",
      "Epoch: 11 | Epoch Time: 0m 8s\n",
      "\tTrain Loss: 0.015 | Train f1: 0.93 | Train precision: 0.93 | Train recall: 0.93\n",
      "\t Val. Loss: 0.285 |  Val. f1: 0.59 |  Val. precision: 0.64 | Val. recall: 0.61\n",
      "Epoch: 12 | Epoch Time: 0m 8s\n",
      "\tTrain Loss: 0.012 | Train f1: 0.94 | Train precision: 0.95 | Train recall: 0.94\n",
      "\t Val. Loss: 0.266 |  Val. f1: 0.61 |  Val. precision: 0.66 | Val. recall: 0.61\n",
      "\n",
      "Performance of best found Model:\n",
      "Val. Loss: 0.198 |  Val. f1: 0.53 | Val. precision: 0.60 | Val. recall: 0.53\n"
     ]
    }
   ],
   "source": [
    "model = elman_model\n",
    "model_name = elman_model_name\n",
    "criterion = baseline_criterion\n",
    "\n",
    "model.apply(init_weights)\n",
    "print(f'El modelo actual tiene {count_parameters(model):,} parámetros entrenables.')\n",
    "\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "model = model.to(device)\n",
    "criterion = criterion.to(device)\n",
    "\n",
    "optimize_model(model, train_iterator, valid_iterator, optimizer, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 697
    },
    "colab_type": "code",
    "id": "siTm8PfukGxt",
    "outputId": "4119aaad-ecef-42cc-8398-1ee6e7c165a9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El modelo actual tiene 2,749,630 parámetros entrenables.\n",
      "Epoch: 01 | Epoch Time: 0m 8s\n",
      "\tTrain Loss: 0.359 | Train f1: 0.26 | Train precision: 0.32 | Train recall: 0.25\n",
      "\t Val. Loss: 0.247 |  Val. f1: 0.44 |  Val. precision: 0.52 | Val. recall: 0.43\n",
      "Epoch: 02 | Epoch Time: 0m 8s\n",
      "\tTrain Loss: 0.125 | Train f1: 0.61 | Train precision: 0.67 | Train recall: 0.61\n",
      "\t Val. Loss: 0.188 |  Val. f1: 0.57 |  Val. precision: 0.63 | Val. recall: 0.57\n",
      "Epoch: 03 | Epoch Time: 0m 8s\n",
      "\tTrain Loss: 0.068 | Train f1: 0.75 | Train precision: 0.78 | Train recall: 0.75\n",
      "\t Val. Loss: 0.202 |  Val. f1: 0.59 |  Val. precision: 0.66 | Val. recall: 0.58\n",
      "Epoch: 04 | Epoch Time: 0m 8s\n",
      "\tTrain Loss: 0.042 | Train f1: 0.83 | Train precision: 0.85 | Train recall: 0.83\n",
      "\t Val. Loss: 0.210 |  Val. f1: 0.60 |  Val. precision: 0.67 | Val. recall: 0.60\n",
      "Epoch: 05 | Epoch Time: 0m 8s\n",
      "\tTrain Loss: 0.030 | Train f1: 0.87 | Train precision: 0.89 | Train recall: 0.87\n",
      "\t Val. Loss: 0.212 |  Val. f1: 0.61 |  Val. precision: 0.67 | Val. recall: 0.61\n",
      "Epoch: 06 | Epoch Time: 0m 8s\n",
      "\tTrain Loss: 0.022 | Train f1: 0.90 | Train precision: 0.91 | Train recall: 0.90\n",
      "\t Val. Loss: 0.238 |  Val. f1: 0.61 |  Val. precision: 0.66 | Val. recall: 0.61\n",
      "Epoch: 07 | Epoch Time: 0m 8s\n",
      "\tTrain Loss: 0.017 | Train f1: 0.92 | Train precision: 0.93 | Train recall: 0.92\n",
      "\t Val. Loss: 0.240 |  Val. f1: 0.61 |  Val. precision: 0.67 | Val. recall: 0.61\n",
      "Epoch: 08 | Epoch Time: 0m 8s\n",
      "\tTrain Loss: 0.014 | Train f1: 0.93 | Train precision: 0.94 | Train recall: 0.93\n",
      "\t Val. Loss: 0.254 |  Val. f1: 0.61 |  Val. precision: 0.67 | Val. recall: 0.61\n",
      "Epoch: 09 | Epoch Time: 0m 8s\n",
      "\tTrain Loss: 0.012 | Train f1: 0.95 | Train precision: 0.95 | Train recall: 0.95\n",
      "\t Val. Loss: 0.263 |  Val. f1: 0.62 |  Val. precision: 0.67 | Val. recall: 0.62\n",
      "Epoch: 10 | Epoch Time: 0m 8s\n",
      "\tTrain Loss: 0.010 | Train f1: 0.95 | Train precision: 0.96 | Train recall: 0.95\n",
      "\t Val. Loss: 0.279 |  Val. f1: 0.61 |  Val. precision: 0.67 | Val. recall: 0.60\n",
      "Epoch: 11 | Epoch Time: 0m 8s\n",
      "\tTrain Loss: 0.010 | Train f1: 0.96 | Train precision: 0.96 | Train recall: 0.95\n",
      "\t Val. Loss: 0.254 |  Val. f1: 0.63 |  Val. precision: 0.68 | Val. recall: 0.62\n",
      "Epoch: 12 | Epoch Time: 0m 8s\n",
      "\tTrain Loss: 0.009 | Train f1: 0.96 | Train precision: 0.96 | Train recall: 0.96\n",
      "\t Val. Loss: 0.271 |  Val. f1: 0.62 |  Val. precision: 0.68 | Val. recall: 0.62\n",
      "\n",
      "Performance of best found Model:\n",
      "Val. Loss: 0.188 |  Val. f1: 0.57 | Val. precision: 0.63 | Val. recall: 0.57\n"
     ]
    }
   ],
   "source": [
    "model = gru_model\n",
    "model_name = gru_model_name\n",
    "criterion = baseline_criterion\n",
    "\n",
    "model.apply(init_weights)\n",
    "print(f'El modelo actual tiene {count_parameters(model):,} parámetros entrenables.')\n",
    "\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "model = model.to(device)\n",
    "criterion = criterion.to(device)\n",
    "\n",
    "optimize_model(model, train_iterator, valid_iterator, optimizer, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bJhkpAdkdTzB"
   },
   "outputs": [],
   "source": [
    "## Parámetros del Experimento ##\n",
    "\n",
    "# tamaño del vocabulario. recuerden que la entrada son vectores bag of word(one-hot).\n",
    "INPUT_DIM = len(TEXT.vocab)\n",
    "EMBEDDING_DIM = 100  # dimensión de los embeddings.\n",
    "HIDDEN_DIM = 256  # dimensión de la capas RNN\n",
    "OUTPUT_DIM = len(NER_TAGS.vocab)  # número de clases\n",
    "\n",
    "N_LAYERS = 2  # número de capas.\n",
    "DROPOUT = 0.25\n",
    "BIDIRECTIONAL = True\n",
    "\n",
    "TAG_PAD_IDX = NER_TAGS.vocab.stoi[NER_TAGS.pad_token]\n",
    "baseline_criterion = nn.CrossEntropyLoss(ignore_index = TAG_PAD_IDX)\n",
    "\n",
    "# Creamos nuestros modelos.\n",
    "lstm_model = NER_RNN(INPUT_DIM, EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM,\n",
    "                         N_LAYERS, BIDIRECTIONAL, DROPOUT, PAD_IDX)\n",
    "\n",
    "lstm_model_name = 'LSTM'  # nombre que tendrá el modelo guardado...\n",
    "\n",
    "elman_model = NER_ELMAN(INPUT_DIM, EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM,\n",
    "                         N_LAYERS, BIDIRECTIONAL, DROPOUT, PAD_IDX, 'relu')\n",
    "\n",
    "elman_model_name = 'Elman'  # nombre que tendrá el modelo guardado...\n",
    "\n",
    "gru_model = NER_GRU(INPUT_DIM, EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM,\n",
    "                         N_LAYERS, BIDIRECTIONAL, DROPOUT, PAD_IDX)\n",
    "\n",
    "gru_model_name = 'GRU'  # nombre que tendrá el modelo guardado..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 799
    },
    "colab_type": "code",
    "id": "2M-gdsqJm-tK",
    "outputId": "b359b92d-655a-46ac-9be0-6eab19d649d7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El modelo actual tiene 4,925,374 parámetros entrenables.\n",
      "Epoch: 01 | Epoch Time: 0m 11s\n",
      "\tTrain Loss: 0.341 | Train f1: 0.29 | Train precision: 0.37 | Train recall: 0.27\n",
      "\t Val. Loss: 0.246 |  Val. f1: 0.46 |  Val. precision: 0.56 | Val. recall: 0.45\n",
      "Epoch: 02 | Epoch Time: 0m 11s\n",
      "\tTrain Loss: 0.118 | Train f1: 0.63 | Train precision: 0.68 | Train recall: 0.62\n",
      "\t Val. Loss: 0.205 |  Val. f1: 0.54 |  Val. precision: 0.62 | Val. recall: 0.54\n",
      "Epoch: 03 | Epoch Time: 0m 11s\n",
      "\tTrain Loss: 0.063 | Train f1: 0.76 | Train precision: 0.79 | Train recall: 0.76\n",
      "\t Val. Loss: 0.194 |  Val. f1: 0.59 |  Val. precision: 0.66 | Val. recall: 0.58\n",
      "Epoch: 04 | Epoch Time: 0m 11s\n",
      "\tTrain Loss: 0.041 | Train f1: 0.83 | Train precision: 0.85 | Train recall: 0.83\n",
      "\t Val. Loss: 0.182 |  Val. f1: 0.62 |  Val. precision: 0.67 | Val. recall: 0.63\n",
      "Epoch: 05 | Epoch Time: 0m 11s\n",
      "\tTrain Loss: 0.028 | Train f1: 0.87 | Train precision: 0.89 | Train recall: 0.87\n",
      "\t Val. Loss: 0.210 |  Val. f1: 0.61 |  Val. precision: 0.67 | Val. recall: 0.60\n",
      "Epoch: 06 | Epoch Time: 0m 11s\n",
      "\tTrain Loss: 0.019 | Train f1: 0.91 | Train precision: 0.92 | Train recall: 0.91\n",
      "\t Val. Loss: 0.219 |  Val. f1: 0.63 |  Val. precision: 0.69 | Val. recall: 0.62\n",
      "Epoch: 07 | Epoch Time: 0m 11s\n",
      "\tTrain Loss: 0.015 | Train f1: 0.92 | Train precision: 0.93 | Train recall: 0.92\n",
      "\t Val. Loss: 0.215 |  Val. f1: 0.62 |  Val. precision: 0.68 | Val. recall: 0.62\n",
      "Epoch: 08 | Epoch Time: 0m 11s\n",
      "\tTrain Loss: 0.012 | Train f1: 0.94 | Train precision: 0.95 | Train recall: 0.94\n",
      "\t Val. Loss: 0.256 |  Val. f1: 0.61 |  Val. precision: 0.67 | Val. recall: 0.61\n",
      "Epoch: 09 | Epoch Time: 0m 11s\n",
      "\tTrain Loss: 0.011 | Train f1: 0.95 | Train precision: 0.96 | Train recall: 0.95\n",
      "\t Val. Loss: 0.234 |  Val. f1: 0.62 |  Val. precision: 0.68 | Val. recall: 0.61\n",
      "Epoch: 10 | Epoch Time: 0m 11s\n",
      "\tTrain Loss: 0.009 | Train f1: 0.96 | Train precision: 0.96 | Train recall: 0.96\n",
      "\t Val. Loss: 0.259 |  Val. f1: 0.62 |  Val. precision: 0.68 | Val. recall: 0.62\n",
      "Epoch: 11 | Epoch Time: 0m 11s\n",
      "\tTrain Loss: 0.008 | Train f1: 0.97 | Train precision: 0.97 | Train recall: 0.96\n",
      "\t Val. Loss: 0.234 |  Val. f1: 0.63 |  Val. precision: 0.68 | Val. recall: 0.64\n",
      "Epoch: 12 | Epoch Time: 0m 11s\n",
      "\tTrain Loss: 0.008 | Train f1: 0.97 | Train precision: 0.97 | Train recall: 0.97\n",
      "\t Val. Loss: 0.292 |  Val. f1: 0.62 |  Val. precision: 0.66 | Val. recall: 0.63\n",
      "Epoch: 13 | Epoch Time: 0m 11s\n",
      "\tTrain Loss: 0.009 | Train f1: 0.95 | Train precision: 0.96 | Train recall: 0.95\n",
      "\t Val. Loss: 0.241 |  Val. f1: 0.62 |  Val. precision: 0.67 | Val. recall: 0.63\n",
      "Epoch: 14 | Epoch Time: 0m 11s\n",
      "\tTrain Loss: 0.007 | Train f1: 0.97 | Train precision: 0.97 | Train recall: 0.97\n",
      "\t Val. Loss: 0.278 |  Val. f1: 0.61 |  Val. precision: 0.68 | Val. recall: 0.61\n",
      "\n",
      "Performance of best found Model:\n",
      "Val. Loss: 0.182 |  Val. f1: 0.62 | Val. precision: 0.67 | Val. recall: 0.63\n"
     ]
    }
   ],
   "source": [
    "model = lstm_model\n",
    "model_name = lstm_model_name\n",
    "criterion = baseline_criterion\n",
    "\n",
    "model.apply(init_weights)\n",
    "print(f'El modelo actual tiene {count_parameters(model):,} parámetros entrenables.')\n",
    "\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "model = model.to(device)\n",
    "criterion = criterion.to(device)\n",
    "\n",
    "optimize_model(model, train_iterator, valid_iterator, optimizer, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "Jdi-anAerNZn",
    "outputId": "98920a57-967f-4bf0-aa50-41ed27a5e0e1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El modelo actual tiene 3,192,766 parámetros entrenables.\n",
      "Epoch: 01 | Epoch Time: 0m 8s\n",
      "\tTrain Loss: 294306.830 | Train f1: 0.02 | Train precision: 0.06 | Train recall: 0.01\n",
      "\t Val. Loss: 0.928 |  Val. f1: 0.00 |  Val. precision: 0.00 | Val. recall: 0.00\n",
      "Epoch: 02 | Epoch Time: 0m 8s\n",
      "\tTrain Loss: 0.877 | Train f1: 0.01 | Train precision: 0.02 | Train recall: 0.00\n",
      "\t Val. Loss: 0.774 |  Val. f1: 0.00 |  Val. precision: 0.00 | Val. recall: 0.00\n",
      "Epoch: 03 | Epoch Time: 0m 8s\n",
      "\tTrain Loss: 0.758 | Train f1: 0.00 | Train precision: 0.01 | Train recall: 0.00\n",
      "\t Val. Loss: 0.766 |  Val. f1: 0.00 |  Val. precision: 0.00 | Val. recall: 0.00\n",
      "Epoch: 04 | Epoch Time: 0m 8s\n",
      "\tTrain Loss: 0.705 | Train f1: 0.00 | Train precision: 0.00 | Train recall: 0.00\n",
      "\t Val. Loss: 0.775 |  Val. f1: 0.00 |  Val. precision: 0.00 | Val. recall: 0.00\n",
      "Epoch: 05 | Epoch Time: 0m 8s\n",
      "\tTrain Loss: 0.700 | Train f1: 0.00 | Train precision: 0.00 | Train recall: 0.00\n",
      "\t Val. Loss: 0.754 |  Val. f1: 0.00 |  Val. precision: 0.00 | Val. recall: 0.00\n",
      "Epoch: 06 | Epoch Time: 0m 8s\n",
      "\tTrain Loss: 0.668 | Train f1: 0.00 | Train precision: 0.00 | Train recall: 0.00\n",
      "\t Val. Loss: 0.750 |  Val. f1: 0.00 |  Val. precision: 0.00 | Val. recall: 0.00\n",
      "Epoch: 07 | Epoch Time: 0m 8s\n",
      "\tTrain Loss: 0.648 | Train f1: 0.00 | Train precision: 0.00 | Train recall: 0.00\n",
      "\t Val. Loss: 0.736 |  Val. f1: 0.00 |  Val. precision: 0.00 | Val. recall: 0.00\n",
      "Epoch: 08 | Epoch Time: 0m 8s\n",
      "\tTrain Loss: 0.635 | Train f1: 0.00 | Train precision: 0.00 | Train recall: 0.00\n",
      "\t Val. Loss: 0.733 |  Val. f1: 0.00 |  Val. precision: 0.00 | Val. recall: 0.00\n",
      "Epoch: 09 | Epoch Time: 0m 9s\n",
      "\tTrain Loss: 0.628 | Train f1: 0.00 | Train precision: 0.00 | Train recall: 0.00\n",
      "\t Val. Loss: 0.719 |  Val. f1: 0.00 |  Val. precision: 0.00 | Val. recall: 0.00\n",
      "Epoch: 10 | Epoch Time: 0m 8s\n",
      "\tTrain Loss: 0.618 | Train f1: 0.00 | Train precision: 0.00 | Train recall: 0.00\n",
      "\t Val. Loss: 0.715 |  Val. f1: 0.00 |  Val. precision: 0.00 | Val. recall: 0.00\n",
      "Epoch: 11 | Epoch Time: 0m 8s\n",
      "\tTrain Loss: 0.616 | Train f1: 0.00 | Train precision: 0.00 | Train recall: 0.00\n",
      "\t Val. Loss: 0.712 |  Val. f1: 0.00 |  Val. precision: 0.00 | Val. recall: 0.00\n",
      "Epoch: 12 | Epoch Time: 0m 8s\n",
      "\tTrain Loss: 0.604 | Train f1: 0.00 | Train precision: 0.01 | Train recall: 0.00\n",
      "\t Val. Loss: 0.696 |  Val. f1: 0.00 |  Val. precision: 0.00 | Val. recall: 0.00\n",
      "Epoch: 13 | Epoch Time: 0m 8s\n",
      "\tTrain Loss: 0.586 | Train f1: 0.00 | Train precision: 0.01 | Train recall: 0.00\n",
      "\t Val. Loss: 0.671 |  Val. f1: 0.00 |  Val. precision: 0.01 | Val. recall: 0.00\n",
      "Epoch: 14 | Epoch Time: 0m 8s\n",
      "\tTrain Loss: 0.560 | Train f1: 0.01 | Train precision: 0.02 | Train recall: 0.00\n",
      "\t Val. Loss: 0.646 |  Val. f1: 0.00 |  Val. precision: 0.02 | Val. recall: 0.00\n",
      "Epoch: 15 | Epoch Time: 0m 8s\n",
      "\tTrain Loss: 0.527 | Train f1: 0.01 | Train precision: 0.04 | Train recall: 0.00\n",
      "\t Val. Loss: 0.627 |  Val. f1: 0.01 |  Val. precision: 0.02 | Val. recall: 0.00\n",
      "Epoch: 16 | Epoch Time: 0m 8s\n",
      "\tTrain Loss: 0.495 | Train f1: 0.03 | Train precision: 0.10 | Train recall: 0.02\n",
      "\t Val. Loss: 0.584 |  Val. f1: 0.02 |  Val. precision: 0.06 | Val. recall: 0.01\n",
      "Epoch: 17 | Epoch Time: 0m 8s\n",
      "\tTrain Loss: 0.456 | Train f1: 0.06 | Train precision: 0.18 | Train recall: 0.04\n",
      "\t Val. Loss: 0.546 |  Val. f1: 0.07 |  Val. precision: 0.18 | Val. recall: 0.05\n",
      "Epoch: 18 | Epoch Time: 0m 8s\n",
      "\tTrain Loss: 0.413 | Train f1: 0.12 | Train precision: 0.25 | Train recall: 0.09\n",
      "\t Val. Loss: 0.511 |  Val. f1: 0.13 |  Val. precision: 0.28 | Val. recall: 0.10\n",
      "Epoch: 19 | Epoch Time: 0m 8s\n",
      "\tTrain Loss: 0.373 | Train f1: 0.18 | Train precision: 0.30 | Train recall: 0.15\n",
      "\t Val. Loss: 0.478 |  Val. f1: 0.20 |  Val. precision: 0.33 | Val. recall: 0.17\n",
      "Epoch: 20 | Epoch Time: 0m 8s\n",
      "\tTrain Loss: 0.335 | Train f1: 0.22 | Train precision: 0.32 | Train recall: 0.20\n",
      "\t Val. Loss: 0.494 |  Val. f1: 0.22 |  Val. precision: 0.36 | Val. recall: 0.18\n",
      "Epoch: 21 | Epoch Time: 0m 9s\n",
      "\tTrain Loss: 0.301 | Train f1: 0.25 | Train precision: 0.34 | Train recall: 0.24\n",
      "\t Val. Loss: 0.459 |  Val. f1: 0.25 |  Val. precision: 0.38 | Val. recall: 0.22\n",
      "Epoch: 22 | Epoch Time: 0m 9s\n",
      "\tTrain Loss: 0.271 | Train f1: 0.30 | Train precision: 0.39 | Train recall: 0.29\n",
      "\t Val. Loss: 0.428 |  Val. f1: 0.29 |  Val. precision: 0.40 | Val. recall: 0.27\n",
      "Epoch: 23 | Epoch Time: 0m 9s\n",
      "\tTrain Loss: 0.244 | Train f1: 0.33 | Train precision: 0.40 | Train recall: 0.32\n",
      "\t Val. Loss: 0.440 |  Val. f1: 0.29 |  Val. precision: 0.39 | Val. recall: 0.27\n",
      "Epoch: 24 | Epoch Time: 0m 9s\n",
      "\tTrain Loss: 0.219 | Train f1: 0.37 | Train precision: 0.43 | Train recall: 0.36\n",
      "\t Val. Loss: 0.421 |  Val. f1: 0.34 |  Val. precision: 0.44 | Val. recall: 0.32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/numpy/lib/function_base.py:393: RuntimeWarning: Mean of empty slice.\n",
      "  avg = a.mean(axis)\n",
      "/usr/local/lib/python3.6/dist-packages/numpy/core/_methods.py:161: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 25 | Epoch Time: 0m 9s\n",
      "\tTrain Loss: 0.199 | Train f1: nan | Train precision: nan | Train recall: nan\n",
      "\t Val. Loss: 0.432 |  Val. f1: 0.36 |  Val. precision: 0.47 | Val. recall: 0.34\n",
      "Epoch: 26 | Epoch Time: 0m 9s\n",
      "\tTrain Loss: 0.180 | Train f1: 0.45 | Train precision: 0.51 | Train recall: 0.45\n",
      "\t Val. Loss: 0.424 |  Val. f1: 0.38 |  Val. precision: 0.48 | Val. recall: 0.36\n",
      "Epoch: 27 | Epoch Time: 0m 9s\n",
      "\tTrain Loss: 0.162 | Train f1: 0.50 | Train precision: 0.55 | Train recall: 0.49\n",
      "\t Val. Loss: 0.472 |  Val. f1: 0.39 |  Val. precision: 0.51 | Val. recall: 0.37\n",
      "Epoch: 28 | Epoch Time: 0m 8s\n",
      "\tTrain Loss: 0.144 | Train f1: 0.54 | Train precision: 0.59 | Train recall: 0.54\n",
      "\t Val. Loss: 0.439 |  Val. f1: 0.44 |  Val. precision: 0.56 | Val. recall: 0.42\n",
      "Epoch: 29 | Epoch Time: 0m 8s\n",
      "\tTrain Loss: 0.131 | Train f1: 0.58 | Train precision: 0.63 | Train recall: 0.58\n",
      "\t Val. Loss: 0.435 |  Val. f1: 0.46 |  Val. precision: 0.58 | Val. recall: 0.42\n",
      "Epoch: 30 | Epoch Time: 0m 8s\n",
      "\tTrain Loss: 0.116 | Train f1: 0.62 | Train precision: 0.66 | Train recall: 0.62\n",
      "\t Val. Loss: 0.436 |  Val. f1: 0.48 |  Val. precision: 0.59 | Val. recall: 0.45\n",
      "Epoch: 31 | Epoch Time: 0m 8s\n",
      "\tTrain Loss: 0.115 | Train f1: 0.63 | Train precision: 0.68 | Train recall: 0.63\n",
      "\t Val. Loss: 0.419 |  Val. f1: 0.49 |  Val. precision: 0.60 | Val. recall: 0.47\n",
      "Epoch: 32 | Epoch Time: 0m 8s\n",
      "\tTrain Loss: 119953.460 | Train f1: 0.52 | Train precision: 0.58 | Train recall: 0.52\n",
      "\t Val. Loss: 0.474 |  Val. f1: 0.49 |  Val. precision: 0.59 | Val. recall: 0.47\n",
      "\n",
      "Performance of best found Model:\n",
      "Val. Loss: 0.419 |  Val. f1: 0.49 | Val. precision: 0.60 | Val. recall: 0.47\n"
     ]
    }
   ],
   "source": [
    "model = elman_model\n",
    "model_name = elman_model_name\n",
    "criterion = baseline_criterion\n",
    "\n",
    "model.apply(init_weights)\n",
    "print(f'El modelo actual tiene {count_parameters(model):,} parámetros entrenables.')\n",
    "\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "model = model.to(device)\n",
    "criterion = criterion.to(device)\n",
    "\n",
    "optimize_model(model, train_iterator, valid_iterator, optimizer, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 748
    },
    "colab_type": "code",
    "id": "-S2nDoEwt3OX",
    "outputId": "b0e4db7f-7a20-4fe2-8285-bfc789325833"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El modelo actual tiene 4,347,838 parámetros entrenables.\n",
      "Epoch: 01 | Epoch Time: 0m 10s\n",
      "\tTrain Loss: 0.343 | Train f1: 0.32 | Train precision: 0.42 | Train recall: 0.29\n",
      "\t Val. Loss: 0.229 |  Val. f1: 0.49 |  Val. precision: 0.57 | Val. recall: 0.49\n",
      "Epoch: 02 | Epoch Time: 0m 10s\n",
      "\tTrain Loss: 0.116 | Train f1: 0.64 | Train precision: 0.70 | Train recall: 0.63\n",
      "\t Val. Loss: 0.218 |  Val. f1: 0.56 |  Val. precision: 0.65 | Val. recall: 0.54\n",
      "Epoch: 03 | Epoch Time: 0m 10s\n",
      "\tTrain Loss: 0.058 | Train f1: 0.78 | Train precision: 0.81 | Train recall: 0.78\n",
      "\t Val. Loss: 0.195 |  Val. f1: 0.61 |  Val. precision: 0.68 | Val. recall: 0.59\n",
      "Epoch: 04 | Epoch Time: 0m 10s\n",
      "\tTrain Loss: 0.034 | Train f1: 0.85 | Train precision: 0.87 | Train recall: 0.85\n",
      "\t Val. Loss: 0.210 |  Val. f1: 0.63 |  Val. precision: 0.69 | Val. recall: 0.62\n",
      "Epoch: 05 | Epoch Time: 0m 10s\n",
      "\tTrain Loss: 0.025 | Train f1: 0.89 | Train precision: 0.90 | Train recall: 0.89\n",
      "\t Val. Loss: 0.218 |  Val. f1: 0.63 |  Val. precision: 0.69 | Val. recall: 0.61\n",
      "Epoch: 06 | Epoch Time: 0m 10s\n",
      "\tTrain Loss: 0.018 | Train f1: 0.92 | Train precision: 0.93 | Train recall: 0.92\n",
      "\t Val. Loss: 0.237 |  Val. f1: 0.62 |  Val. precision: 0.68 | Val. recall: 0.62\n",
      "Epoch: 07 | Epoch Time: 0m 10s\n",
      "\tTrain Loss: 0.015 | Train f1: 0.93 | Train precision: 0.94 | Train recall: 0.93\n",
      "\t Val. Loss: 0.238 |  Val. f1: 0.63 |  Val. precision: 0.69 | Val. recall: 0.62\n",
      "Epoch: 08 | Epoch Time: 0m 10s\n",
      "\tTrain Loss: 0.012 | Train f1: 0.95 | Train precision: 0.95 | Train recall: 0.94\n",
      "\t Val. Loss: 0.273 |  Val. f1: 0.62 |  Val. precision: 0.68 | Val. recall: 0.61\n",
      "Epoch: 09 | Epoch Time: 0m 10s\n",
      "\tTrain Loss: 0.013 | Train f1: 0.94 | Train precision: 0.95 | Train recall: 0.94\n",
      "\t Val. Loss: 0.250 |  Val. f1: 0.62 |  Val. precision: 0.68 | Val. recall: 0.61\n",
      "Epoch: 10 | Epoch Time: 0m 10s\n",
      "\tTrain Loss: 0.010 | Train f1: 0.95 | Train precision: 0.95 | Train recall: 0.95\n",
      "\t Val. Loss: 0.291 |  Val. f1: 0.61 |  Val. precision: 0.68 | Val. recall: 0.61\n",
      "Epoch: 11 | Epoch Time: 0m 10s\n",
      "\tTrain Loss: 0.009 | Train f1: 0.96 | Train precision: 0.96 | Train recall: 0.96\n",
      "\t Val. Loss: 0.270 |  Val. f1: 0.63 |  Val. precision: 0.69 | Val. recall: 0.62\n",
      "Epoch: 12 | Epoch Time: 0m 10s\n",
      "\tTrain Loss: 0.009 | Train f1: 0.96 | Train precision: 0.96 | Train recall: 0.96\n",
      "\t Val. Loss: 0.259 |  Val. f1: 0.64 |  Val. precision: 0.70 | Val. recall: 0.63\n",
      "Epoch: 13 | Epoch Time: 0m 10s\n",
      "\tTrain Loss: 0.008 | Train f1: 0.96 | Train precision: 0.97 | Train recall: 0.96\n",
      "\t Val. Loss: 0.294 |  Val. f1: 0.62 |  Val. precision: 0.68 | Val. recall: 0.62\n",
      "\n",
      "Performance of best found Model:\n",
      "Val. Loss: 0.195 |  Val. f1: 0.61 | Val. precision: 0.68 | Val. recall: 0.59\n"
     ]
    }
   ],
   "source": [
    "model = gru_model\n",
    "model_name = gru_model_name\n",
    "criterion = baseline_criterion\n",
    "\n",
    "model.apply(init_weights)\n",
    "print(f'El modelo actual tiene {count_parameters(model):,} parámetros entrenables.')\n",
    "\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "model = model.to(device)\n",
    "criterion = criterion.to(device)  \n",
    "\n",
    "optimize_model(model, train_iterator, valid_iterator, optimizer, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "590UhjSxdVot"
   },
   "outputs": [],
   "source": [
    "## Parámetros del Experimento ##\n",
    "\n",
    "# tamaño del vocabulario. recuerden que la entrada son vectores bag of word(one-hot).\n",
    "INPUT_DIM = len(TEXT.vocab)\n",
    "EMBEDDING_DIM = 100  # dimensión de los embeddings.\n",
    "HIDDEN_DIM = 512  # dimensión de la capas RNN\n",
    "OUTPUT_DIM = len(NER_TAGS.vocab)  # número de clases\n",
    "\n",
    "N_LAYERS = 2  # número de capas.\n",
    "DROPOUT = 0.25\n",
    "BIDIRECTIONAL = True\n",
    "\n",
    "TAG_PAD_IDX = NER_TAGS.vocab.stoi[NER_TAGS.pad_token]\n",
    "baseline_criterion = nn.CrossEntropyLoss(ignore_index = TAG_PAD_IDX)\n",
    "\n",
    "# Creamos nuestros modelos.\n",
    "lstm_model = NER_RNN(INPUT_DIM, EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM,\n",
    "                         N_LAYERS, BIDIRECTIONAL, DROPOUT, PAD_IDX)\n",
    "\n",
    "lstm_model_name = 'LSTM'  # nombre que tendrá el modelo guardado...\n",
    "\n",
    "elman_model = NER_ELMAN(INPUT_DIM, EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM,\n",
    "                         N_LAYERS, BIDIRECTIONAL, DROPOUT, PAD_IDX, 'relu')\n",
    "\n",
    "elman_model_name = 'Elman'  # nombre que tendrá el modelo guardado...\n",
    "\n",
    "gru_model = NER_GRU(INPUT_DIM, EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM,\n",
    "                         N_LAYERS, BIDIRECTIONAL, DROPOUT, PAD_IDX)\n",
    "\n",
    "gru_model_name = 'GRU'  # nombre que tendrá el modelo guardado..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 748
    },
    "colab_type": "code",
    "id": "_S1hISzPKA1Q",
    "outputId": "b98b945f-6832-4c29-c137-ca0990002985"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El modelo actual tiene 11,434,942 parámetros entrenables.\n",
      "Epoch: 01 | Epoch Time: 0m 20s\n",
      "\tTrain Loss: 0.389 | Train f1: 0.23 | Train precision: 0.30 | Train recall: 0.22\n",
      "\t Val. Loss: 0.270 |  Val. f1: 0.45 |  Val. precision: 0.55 | Val. recall: 0.45\n",
      "Epoch: 02 | Epoch Time: 0m 20s\n",
      "\tTrain Loss: 0.131 | Train f1: 0.59 | Train precision: 0.65 | Train recall: 0.59\n",
      "\t Val. Loss: 0.208 |  Val. f1: 0.56 |  Val. precision: 0.64 | Val. recall: 0.56\n",
      "Epoch: 03 | Epoch Time: 0m 20s\n",
      "\tTrain Loss: 0.070 | Train f1: 0.74 | Train precision: 0.77 | Train recall: 0.74\n",
      "\t Val. Loss: 0.184 |  Val. f1: 0.61 |  Val. precision: 0.68 | Val. recall: 0.61\n",
      "Epoch: 04 | Epoch Time: 0m 20s\n",
      "\tTrain Loss: 0.042 | Train f1: 0.83 | Train precision: 0.85 | Train recall: 0.83\n",
      "\t Val. Loss: 0.209 |  Val. f1: 0.60 |  Val. precision: 0.67 | Val. recall: 0.59\n",
      "Epoch: 05 | Epoch Time: 0m 20s\n",
      "\tTrain Loss: 0.028 | Train f1: 0.87 | Train precision: 0.89 | Train recall: 0.87\n",
      "\t Val. Loss: 0.236 |  Val. f1: 0.59 |  Val. precision: 0.65 | Val. recall: 0.60\n",
      "Epoch: 06 | Epoch Time: 0m 20s\n",
      "\tTrain Loss: 0.021 | Train f1: 0.90 | Train precision: 0.91 | Train recall: 0.90\n",
      "\t Val. Loss: 0.220 |  Val. f1: 0.62 |  Val. precision: 0.68 | Val. recall: 0.61\n",
      "Epoch: 07 | Epoch Time: 0m 20s\n",
      "\tTrain Loss: 0.015 | Train f1: 0.92 | Train precision: 0.93 | Train recall: 0.92\n",
      "\t Val. Loss: 0.222 |  Val. f1: 0.63 |  Val. precision: 0.68 | Val. recall: 0.64\n",
      "Epoch: 08 | Epoch Time: 0m 20s\n",
      "\tTrain Loss: 0.013 | Train f1: 0.93 | Train precision: 0.94 | Train recall: 0.93\n",
      "\t Val. Loss: 0.252 |  Val. f1: 0.60 |  Val. precision: 0.67 | Val. recall: 0.58\n",
      "Epoch: 09 | Epoch Time: 0m 20s\n",
      "\tTrain Loss: 0.012 | Train f1: 0.94 | Train precision: 0.95 | Train recall: 0.94\n",
      "\t Val. Loss: 0.245 |  Val. f1: 0.62 |  Val. precision: 0.68 | Val. recall: 0.61\n",
      "Epoch: 10 | Epoch Time: 0m 20s\n",
      "\tTrain Loss: 0.010 | Train f1: 0.95 | Train precision: 0.95 | Train recall: 0.95\n",
      "\t Val. Loss: 0.248 |  Val. f1: 0.62 |  Val. precision: 0.68 | Val. recall: 0.62\n",
      "Epoch: 11 | Epoch Time: 0m 20s\n",
      "\tTrain Loss: 0.009 | Train f1: 0.96 | Train precision: 0.96 | Train recall: 0.96\n",
      "\t Val. Loss: 0.261 |  Val. f1: 0.60 |  Val. precision: 0.66 | Val. recall: 0.60\n",
      "Epoch: 12 | Epoch Time: 0m 20s\n",
      "\tTrain Loss: 0.008 | Train f1: 0.96 | Train precision: 0.97 | Train recall: 0.96\n",
      "\t Val. Loss: 0.288 |  Val. f1: 0.60 |  Val. precision: 0.67 | Val. recall: 0.60\n",
      "Epoch: 13 | Epoch Time: 0m 20s\n",
      "\tTrain Loss: 0.009 | Train f1: 0.96 | Train precision: 0.96 | Train recall: 0.96\n",
      "\t Val. Loss: 0.284 |  Val. f1: 0.61 |  Val. precision: 0.68 | Val. recall: 0.60\n",
      "\n",
      "Performance of best found Model:\n",
      "Val. Loss: 0.184 |  Val. f1: 0.61 | Val. precision: 0.68 | Val. recall: 0.61\n"
     ]
    }
   ],
   "source": [
    "model = lstm_model\n",
    "model_name = lstm_model_name\n",
    "criterion = baseline_criterion\n",
    "\n",
    "model.apply(init_weights)\n",
    "print(f'El modelo actual tiene {count_parameters(model):,} parámetros entrenables.')\n",
    "\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "model = model.to(device)\n",
    "criterion = criterion.to(device)\n",
    "\n",
    "optimize_model(model, train_iterator, valid_iterator, optimizer, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 476
    },
    "colab_type": "code",
    "id": "t4iNjYXfKD9l",
    "outputId": "9f7761a7-8f40-4e7a-802b-a1d2a0788892"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El modelo actual tiene 4,823,998 parámetros entrenables.\n",
      "Epoch: 01 | Epoch Time: 0m 11s\n",
      "\tTrain Loss: nan | Train f1: 0.00 | Train precision: 0.00 | Train recall: 0.00\n",
      "\t Val. Loss: nan |  Val. f1: 0.00 |  Val. precision: 0.00 | Val. recall: 0.00\n",
      "Epoch: 02 | Epoch Time: 0m 11s\n",
      "\tTrain Loss: nan | Train f1: 0.00 | Train precision: 0.00 | Train recall: 0.00\n",
      "\t Val. Loss: nan |  Val. f1: 0.00 |  Val. precision: 0.00 | Val. recall: 0.00\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-105-422cac2996da>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mcriterion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0moptimize_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_iterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_iterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-36-67b9338279ee>\u001b[0m in \u001b[0;36moptimize_model\u001b[0;34m(model, train_iterator, valid_iterator, optimizer, criterion, nEpochs, stopTolerance, useBest)\u001b[0m\n\u001b[1;32m     16\u001b[0m       \u001b[0;31m# Entrenar\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m       train_loss, train_precision, train_recall, train_f1 = train(\n\u001b[0;32m---> 18\u001b[0;31m           model, train_iterator, optimizer, criterion)\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m       \u001b[0;31m# Evaluar (valid = validación)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-29-cdad43c8acbd>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, iterator, optimizer, criterion)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0;31m# Calculamos los gradientes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0;31m# Actualizamos los parámetros de la red\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    196\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m         \"\"\"\n\u001b[0;32m--> 198\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     98\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     99\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = elman_model\n",
    "model_name = elman_model_name\n",
    "criterion = baseline_criterion\n",
    "\n",
    "model.apply(init_weights)\n",
    "print(f'El modelo actual tiene {count_parameters(model):,} parámetros entrenables.')\n",
    "\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "model = model.to(device)\n",
    "criterion = criterion.to(device)\n",
    "\n",
    "optimize_model(model, train_iterator, valid_iterator, optimizer, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 785
    },
    "colab_type": "code",
    "id": "LWwLN-UJKGqG",
    "outputId": "b121ec49-352b-4be0-d2a8-ef187ba58c5f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El modelo actual tiene 9,231,294 parámetros entrenables.\n",
      "Epoch: 01 | Epoch Time: 0m 18s\n",
      "\tTrain Loss: 0.565 | Train f1: 0.19 | Train precision: 0.26 | Train recall: 0.18\n",
      "\t Val. Loss: 0.284 |  Val. f1: 0.42 |  Val. precision: 0.49 | Val. recall: 0.43\n",
      "Epoch: 02 | Epoch Time: 0m 18s\n",
      "\tTrain Loss: 0.165 | Train f1: 0.55 | Train precision: 0.61 | Train recall: 0.55\n",
      "\t Val. Loss: 0.197 |  Val. f1: 0.57 |  Val. precision: 0.66 | Val. recall: 0.56\n",
      "Epoch: 03 | Epoch Time: 0m 18s\n",
      "\tTrain Loss: 0.082 | Train f1: 0.72 | Train precision: 0.75 | Train recall: 0.72\n",
      "\t Val. Loss: 0.217 |  Val. f1: 0.61 |  Val. precision: 0.67 | Val. recall: 0.61\n",
      "Epoch: 04 | Epoch Time: 0m 18s\n",
      "\tTrain Loss: 0.048 | Train f1: 0.80 | Train precision: 0.83 | Train recall: 0.80\n",
      "\t Val. Loss: 0.228 |  Val. f1: 0.62 |  Val. precision: 0.69 | Val. recall: 0.61\n",
      "Epoch: 05 | Epoch Time: 0m 18s\n",
      "\tTrain Loss: 0.032 | Train f1: 0.86 | Train precision: 0.88 | Train recall: 0.86\n",
      "\t Val. Loss: 0.227 |  Val. f1: 0.62 |  Val. precision: 0.68 | Val. recall: 0.63\n",
      "Epoch: 06 | Epoch Time: 0m 18s\n",
      "\tTrain Loss: 0.022 | Train f1: 0.90 | Train precision: 0.91 | Train recall: 0.90\n",
      "\t Val. Loss: 0.252 |  Val. f1: 0.63 |  Val. precision: 0.69 | Val. recall: 0.63\n",
      "Epoch: 07 | Epoch Time: 0m 18s\n",
      "\tTrain Loss: 0.018 | Train f1: 0.91 | Train precision: 0.92 | Train recall: 0.91\n",
      "\t Val. Loss: 0.301 |  Val. f1: 0.61 |  Val. precision: 0.67 | Val. recall: 0.60\n",
      "Epoch: 08 | Epoch Time: 0m 18s\n",
      "\tTrain Loss: 0.016 | Train f1: 0.92 | Train precision: 0.93 | Train recall: 0.92\n",
      "\t Val. Loss: 0.313 |  Val. f1: 0.62 |  Val. precision: 0.70 | Val. recall: 0.61\n",
      "Epoch: 09 | Epoch Time: 0m 18s\n",
      "\tTrain Loss: 0.014 | Train f1: 0.94 | Train precision: 0.95 | Train recall: 0.94\n",
      "\t Val. Loss: 0.284 |  Val. f1: 0.62 |  Val. precision: 0.69 | Val. recall: 0.62\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/numpy/lib/function_base.py:393: RuntimeWarning: Mean of empty slice.\n",
      "  avg = a.mean(axis)\n",
      "/usr/local/lib/python3.6/dist-packages/numpy/core/_methods.py:161: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10 | Epoch Time: 0m 18s\n",
      "\tTrain Loss: 0.013 | Train f1: nan | Train precision: nan | Train recall: nan\n",
      "\t Val. Loss: 0.323 |  Val. f1: 0.60 |  Val. precision: 0.67 | Val. recall: 0.60\n",
      "Epoch: 11 | Epoch Time: 0m 18s\n",
      "\tTrain Loss: 0.011 | Train f1: 0.95 | Train precision: 0.95 | Train recall: 0.95\n",
      "\t Val. Loss: 0.339 |  Val. f1: 0.62 |  Val. precision: 0.70 | Val. recall: 0.61\n",
      "Epoch: 12 | Epoch Time: 0m 18s\n",
      "\tTrain Loss: 0.011 | Train f1: 0.95 | Train precision: 0.95 | Train recall: 0.95\n",
      "\t Val. Loss: 0.308 |  Val. f1: 0.62 |  Val. precision: 0.68 | Val. recall: 0.62\n",
      "\n",
      "Performance of best found Model:\n",
      "Val. Loss: 0.197 |  Val. f1: 0.57 | Val. precision: 0.66 | Val. recall: 0.56\n"
     ]
    }
   ],
   "source": [
    "model = gru_model\n",
    "model_name = gru_model_name\n",
    "criterion = baseline_criterion\n",
    "\n",
    "model.apply(init_weights)\n",
    "print(f'El modelo actual tiene {count_parameters(model):,} parámetros entrenables.')\n",
    "\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "model = model.to(device)\n",
    "criterion = criterion.to(device)  \n",
    "\n",
    "optimize_model(model, train_iterator, valid_iterator, optimizer, criterion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0oOvVF_cbkp2"
   },
   "source": [
    "Pareciera que aumentar la cantidad de parámetros en las capas ocultas de los modelos recurrentes no mejora la capacidad de la red, incluso al contrario. No obstante, cabe destacar que hasta ahora el modelo con mejor pérdida en validación se encuentra generalmente tras 2-4 épocas. Esto podría deberse a un optimizador con tasa de aprendizaje muy alta.\n",
    "\n",
    "Por esta razón, se experimentará con los modelos más grandes para verificar si su desempeño puede mejorar al modificar los parámetros del optimizador."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mguQS3_Tcyjh"
   },
   "source": [
    "#### Efecto del Optimizador"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xn10YgQHdUUn"
   },
   "source": [
    "Primeramente, se probará disminuir el *learning rate* del optimizador usado (```Adam```) e incluyendo un ```Scheduler``` para variar el mismo durante el entrenamiento.\n",
    "\n",
    "Pero antes, es necesario modificar las funciones de optimización y entrenamiento definidas anteriormente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_lAWfWZWmqO4"
   },
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, criterion, scheduler = None):\n",
    "\n",
    "    epoch_loss = 0\n",
    "    epoch_precision = 0\n",
    "    epoch_recall = 0\n",
    "    epoch_f1 = 0\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    # Por cada batch del iterador de la época:\n",
    "    for batch in iterator:\n",
    "\n",
    "        # Extraemos el texto y los tags del batch que estamos procesado\n",
    "        text = batch.text\n",
    "        tags = batch.nertags\n",
    "\n",
    "        # Reiniciamos los gradientes calculados en la iteración anterior\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        #text = [sent len, batch size]\n",
    "\n",
    "        # Predecimos los tags del texto del batch.\n",
    "        predictions = model(text)\n",
    "\n",
    "        #predictions = [sent len, batch size, output dim]\n",
    "        #tags = [sent len, batch size]\n",
    "\n",
    "        # Reordenamos los datos para calcular la loss\n",
    "        predictions = predictions.view(-1, predictions.shape[-1])\n",
    "        tags = tags.view(-1)\n",
    "\n",
    "        #predictions = [sent len * batch size, output dim]\n",
    "        #tags = [sent len * batch size]\n",
    "\n",
    "        # Calculamos el Cross Entropy de las predicciones con respecto a las etiquetas reales\n",
    "        loss = criterion(predictions, tags)\n",
    "        \n",
    "        # Calculamos el accuracy\n",
    "        precision, recall, f1 = calculate_metrics(predictions, tags)\n",
    "\n",
    "        # Calculamos los gradientes\n",
    "        loss.backward()\n",
    "\n",
    "        # Actualizamos los parámetros de la red\n",
    "        optimizer.step()\n",
    "\n",
    "        # Actualizamos el loss y las métricas\n",
    "        epoch_loss += loss.item()\n",
    "        epoch_precision += precision\n",
    "        epoch_recall += recall\n",
    "        epoch_f1 += f1\n",
    "\n",
    "    # Actualizamos el optimizador\n",
    "    if scheduler is not None:\n",
    "        scheduler.step()\n",
    "\n",
    "    return epoch_loss / len(iterator), epoch_precision / len(\n",
    "        iterator), epoch_recall / len(iterator), epoch_f1 / len(iterator)\n",
    "\n",
    "def optimize_model(model, train_iterator, valid_iterator, optimizer, \n",
    "                   criterion, scheduler = None, \n",
    "                   nEpochs = 100, stopTolerance = 10, useBest = True):\n",
    "  '''stopTolerance denota la cantidad de épocas sin mejoría antes de terminar \\\n",
    "  el entrenamiento mediante Early Stopping'''\n",
    "  '''useBest denota el criterio de Early Stopping a considerar: si useBest es \\\n",
    "  True, tras <stopTolerance> épocas sin mejorar el mejor resultado obtenido se \\\n",
    "  termina el entrenamiento. Si es False, tras <stopTolerance> épocas \\\n",
    "  consecutivas sin una sola mejora de la Loss se termina el entrenamiento.'''\n",
    "\n",
    "  best_valid_loss = float('inf')\n",
    "  prev_valid_loss = float('inf')\n",
    "  counter = 0\n",
    "\n",
    "  for epoch in range(nEpochs):\n",
    "    \n",
    "      start_time = time.time()\n",
    "\n",
    "      # Recuerdo: train_iterator y valid_iterator contienen el dataset dividido en batches.\n",
    "\n",
    "      # Entrenar\n",
    "      train_loss, train_precision, train_recall, train_f1 = train(\n",
    "          model, train_iterator, optimizer, criterion, scheduler)\n",
    "\n",
    "      # Evaluar (valid = validación)\n",
    "      valid_loss, valid_precision, valid_recall, valid_f1 = evaluate(\n",
    "          model, valid_iterator, criterion)\n",
    "\n",
    "      end_time = time.time()\n",
    "\n",
    "      epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "\n",
    "      print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
    "      print(\n",
    "          f'\\tTrain Loss: {train_loss:.3f} | Train f1: {train_f1:.2f} | Train precision: {train_precision:.2f} | Train recall: {train_recall:.2f}'\n",
    "      )\n",
    "      print(\n",
    "          f'\\t Val. Loss: {valid_loss:.3f} |  Val. f1: {valid_f1:.2f} |  Val. precision: {valid_precision:.2f} | Val. recall: {valid_recall:.2f}'\n",
    "      )\n",
    "\n",
    "      # Si obtuvimos mejores resultados, guardamos este modelo en el almacenamiento (para poder cargarlo luego)\n",
    "      # Si detienen el entrenamiento prematuramente, pueden cargar el modelo en el siguiente recuadro de código.\n",
    "      if valid_loss < best_valid_loss:\n",
    "          best_valid_loss = valid_loss\n",
    "          torch.save(model.state_dict(), '{}.pt'.format(model_name))\n",
    "          counter = 0\n",
    "\n",
    "      # Si ya no mejoramos el loss de validación, terminamos de entrenar.\n",
    "      else:\n",
    "          if useBest:\n",
    "              counter += 1\n",
    "          \n",
    "          else:\n",
    "              if valid_loss >= prev_valid_loss:\n",
    "                  counter += 1\n",
    "          \n",
    "          if counter == stopTolerance:\n",
    "              break\n",
    "\n",
    "      prev_valid_loss = valid_loss\n",
    "\n",
    "  model.load_state_dict(torch.load('{}.pt'.format(model_name)))\n",
    "  torch.cuda.empty_cache()\n",
    "\n",
    "  valid_loss, valid_precision, valid_recall, valid_f1 = evaluate(\n",
    "    model, valid_iterator, criterion)\n",
    "  \n",
    "  print('\\nPerformance of best found Model:')\n",
    "  print(\n",
    "      f'Val. Loss: {valid_loss:.3f} |  Val. f1: {valid_f1:.2f} | Val. precision: {valid_precision:.2f} | Val. recall: {valid_recall:.2f}'\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "41aYE8lDd7nz"
   },
   "outputs": [],
   "source": [
    "## Parámetros del Experimento ##\n",
    "\n",
    "# tamaño del vocabulario. recuerden que la entrada son vectores bag of word(one-hot).\n",
    "INPUT_DIM = len(TEXT.vocab)\n",
    "EMBEDDING_DIM = 100  # dimensión de los embeddings.\n",
    "HIDDEN_DIM = 256  # dimensión de la capas RNN\n",
    "OUTPUT_DIM = len(NER_TAGS.vocab)  # número de clases\n",
    "\n",
    "N_LAYERS = 2  # número de capas.\n",
    "DROPOUT = 0.25\n",
    "BIDIRECTIONAL = True\n",
    "\n",
    "TAG_PAD_IDX = NER_TAGS.vocab.stoi[NER_TAGS.pad_token]\n",
    "baseline_criterion = nn.CrossEntropyLoss(ignore_index = TAG_PAD_IDX)\n",
    "\n",
    "# Creamos nuestros modelos.\n",
    "lstm_model = NER_RNN(INPUT_DIM, EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM,\n",
    "                         N_LAYERS, BIDIRECTIONAL, DROPOUT, PAD_IDX)\n",
    "\n",
    "lstm_model_name = 'LSTM'  # nombre que tendrá el modelo guardado...\n",
    "\n",
    "elman_model = NER_ELMAN(INPUT_DIM, EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM,\n",
    "                         N_LAYERS, BIDIRECTIONAL, DROPOUT, PAD_IDX, 'relu')\n",
    "\n",
    "elman_model_name = 'Elman'  # nombre que tendrá el modelo guardado...\n",
    "\n",
    "gru_model = NER_GRU(INPUT_DIM, EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM,\n",
    "                         N_LAYERS, BIDIRECTIONAL, DROPOUT, PAD_IDX)\n",
    "\n",
    "gru_model_name = 'GRU'  # nombre que tendrá el modelo guardado..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 799
    },
    "colab_type": "code",
    "id": "myyrvKuNeCcr",
    "outputId": "24392fbb-955d-4d3c-f459-49a797c4184e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El modelo actual tiene 4,925,374 parámetros entrenables.\n",
      "Epoch: 01 | Epoch Time: 0m 11s\n",
      "\tTrain Loss: 0.418 | Train f1: 0.19 | Train precision: 0.27 | Train recall: 0.17\n",
      "\t Val. Loss: 0.296 |  Val. f1: 0.38 |  Val. precision: 0.49 | Val. recall: 0.36\n",
      "Epoch: 02 | Epoch Time: 0m 11s\n",
      "\tTrain Loss: 0.179 | Train f1: 0.50 | Train precision: 0.57 | Train recall: 0.49\n",
      "\t Val. Loss: 0.234 |  Val. f1: 0.50 |  Val. precision: 0.60 | Val. recall: 0.48\n",
      "Epoch: 03 | Epoch Time: 0m 11s\n",
      "\tTrain Loss: 0.106 | Train f1: 0.65 | Train precision: 0.70 | Train recall: 0.64\n",
      "\t Val. Loss: 0.195 |  Val. f1: 0.56 |  Val. precision: 0.63 | Val. recall: 0.56\n",
      "Epoch: 04 | Epoch Time: 0m 11s\n",
      "\tTrain Loss: 0.071 | Train f1: 0.74 | Train precision: 0.77 | Train recall: 0.73\n",
      "\t Val. Loss: 0.193 |  Val. f1: 0.59 |  Val. precision: 0.65 | Val. recall: 0.58\n",
      "Epoch: 05 | Epoch Time: 0m 11s\n",
      "\tTrain Loss: 0.049 | Train f1: 0.80 | Train precision: 0.82 | Train recall: 0.79\n",
      "\t Val. Loss: 0.219 |  Val. f1: 0.58 |  Val. precision: 0.66 | Val. recall: 0.57\n",
      "Epoch: 06 | Epoch Time: 0m 11s\n",
      "\tTrain Loss: 0.037 | Train f1: 0.83 | Train precision: 0.85 | Train recall: 0.83\n",
      "\t Val. Loss: 0.227 |  Val. f1: 0.59 |  Val. precision: 0.65 | Val. recall: 0.59\n",
      "Epoch: 07 | Epoch Time: 0m 11s\n",
      "\tTrain Loss: 0.028 | Train f1: 0.87 | Train precision: 0.88 | Train recall: 0.87\n",
      "\t Val. Loss: 0.222 |  Val. f1: 0.61 |  Val. precision: 0.67 | Val. recall: 0.61\n",
      "Epoch: 08 | Epoch Time: 0m 11s\n",
      "\tTrain Loss: 0.023 | Train f1: 0.89 | Train precision: 0.90 | Train recall: 0.89\n",
      "\t Val. Loss: 0.230 |  Val. f1: 0.61 |  Val. precision: 0.67 | Val. recall: 0.61\n",
      "Epoch: 09 | Epoch Time: 0m 11s\n",
      "\tTrain Loss: 0.019 | Train f1: 0.91 | Train precision: 0.92 | Train recall: 0.91\n",
      "\t Val. Loss: 0.233 |  Val. f1: 0.62 |  Val. precision: 0.68 | Val. recall: 0.62\n",
      "Epoch: 10 | Epoch Time: 0m 11s\n",
      "\tTrain Loss: 0.015 | Train f1: 0.92 | Train precision: 0.93 | Train recall: 0.92\n",
      "\t Val. Loss: 0.244 |  Val. f1: 0.61 |  Val. precision: 0.67 | Val. recall: 0.61\n",
      "Epoch: 11 | Epoch Time: 0m 11s\n",
      "\tTrain Loss: 0.012 | Train f1: 0.94 | Train precision: 0.94 | Train recall: 0.94\n",
      "\t Val. Loss: 0.261 |  Val. f1: 0.61 |  Val. precision: 0.67 | Val. recall: 0.61\n",
      "Epoch: 12 | Epoch Time: 0m 11s\n",
      "\tTrain Loss: 0.011 | Train f1: 0.94 | Train precision: 0.94 | Train recall: 0.94\n",
      "\t Val. Loss: 0.266 |  Val. f1: 0.61 |  Val. precision: 0.68 | Val. recall: 0.61\n",
      "Epoch: 13 | Epoch Time: 0m 11s\n",
      "\tTrain Loss: 0.010 | Train f1: 0.95 | Train precision: 0.95 | Train recall: 0.95\n",
      "\t Val. Loss: 0.274 |  Val. f1: 0.61 |  Val. precision: 0.67 | Val. recall: 0.60\n",
      "Epoch: 14 | Epoch Time: 0m 11s\n",
      "\tTrain Loss: 0.009 | Train f1: 0.95 | Train precision: 0.96 | Train recall: 0.95\n",
      "\t Val. Loss: 0.280 |  Val. f1: 0.61 |  Val. precision: 0.68 | Val. recall: 0.61\n",
      "\n",
      "Performance of best found Model:\n",
      "Val. Loss: 0.193 |  Val. f1: 0.59 | Val. precision: 0.65 | Val. recall: 0.58\n"
     ]
    }
   ],
   "source": [
    "model = lstm_model\n",
    "model_name = lstm_model_name\n",
    "criterion = baseline_criterion\n",
    "\n",
    "model.apply(init_weights)\n",
    "print(f'El modelo actual tiene {count_parameters(model):,} parámetros entrenables.')\n",
    "\n",
    "lRate = 0.0005\n",
    "#decayRate = 3\n",
    "decayRate = 2\n",
    "decayFactor = 0.9\n",
    "#decayFactor = 0.75\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr = lRate)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size = decayRate, \n",
    "                                      gamma = decayFactor)\n",
    "model = model.to(device)\n",
    "criterion = criterion.to(device)\n",
    "\n",
    "optimize_model(model, train_iterator, valid_iterator, optimizer, \n",
    "               criterion, scheduler = scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 646
    },
    "colab_type": "code",
    "id": "9EIpFpqrfGut",
    "outputId": "c790c9e8-b2b1-438a-b06e-d4d33679d888"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El modelo actual tiene 3,192,766 parámetros entrenables.\n",
      "Epoch: 01 | Epoch Time: 0m 8s\n",
      "\tTrain Loss: 62189147986614.891 | Train f1: 0.04 | Train precision: 0.07 | Train recall: 0.04\n",
      "\t Val. Loss: 6.421 |  Val. f1: 0.01 |  Val. precision: 0.02 | Val. recall: 0.00\n",
      "Epoch: 02 | Epoch Time: 0m 8s\n",
      "\tTrain Loss: nan | Train f1: 0.02 | Train precision: 0.06 | Train recall: 0.02\n",
      "\t Val. Loss: nan |  Val. f1: 0.00 |  Val. precision: 0.00 | Val. recall: 0.00\n",
      "Epoch: 03 | Epoch Time: 0m 8s\n",
      "\tTrain Loss: nan | Train f1: 0.00 | Train precision: 0.00 | Train recall: 0.00\n",
      "\t Val. Loss: nan |  Val. f1: 0.00 |  Val. precision: 0.00 | Val. recall: 0.00\n",
      "Epoch: 04 | Epoch Time: 0m 9s\n",
      "\tTrain Loss: nan | Train f1: 0.00 | Train precision: 0.00 | Train recall: 0.00\n",
      "\t Val. Loss: nan |  Val. f1: 0.00 |  Val. precision: 0.00 | Val. recall: 0.00\n",
      "Epoch: 05 | Epoch Time: 0m 9s\n",
      "\tTrain Loss: nan | Train f1: 0.00 | Train precision: 0.00 | Train recall: 0.00\n",
      "\t Val. Loss: nan |  Val. f1: 0.00 |  Val. precision: 0.00 | Val. recall: 0.00\n",
      "Epoch: 06 | Epoch Time: 0m 8s\n",
      "\tTrain Loss: nan | Train f1: 0.00 | Train precision: 0.00 | Train recall: 0.00\n",
      "\t Val. Loss: nan |  Val. f1: 0.00 |  Val. precision: 0.00 | Val. recall: 0.00\n",
      "Epoch: 07 | Epoch Time: 0m 8s\n",
      "\tTrain Loss: nan | Train f1: 0.00 | Train precision: 0.00 | Train recall: 0.00\n",
      "\t Val. Loss: nan |  Val. f1: 0.00 |  Val. precision: 0.00 | Val. recall: 0.00\n",
      "Epoch: 08 | Epoch Time: 0m 8s\n",
      "\tTrain Loss: nan | Train f1: 0.00 | Train precision: 0.00 | Train recall: 0.00\n",
      "\t Val. Loss: nan |  Val. f1: 0.00 |  Val. precision: 0.00 | Val. recall: 0.00\n",
      "Epoch: 09 | Epoch Time: 0m 8s\n",
      "\tTrain Loss: nan | Train f1: 0.00 | Train precision: 0.00 | Train recall: 0.00\n",
      "\t Val. Loss: nan |  Val. f1: 0.00 |  Val. precision: 0.00 | Val. recall: 0.00\n",
      "Epoch: 10 | Epoch Time: 0m 8s\n",
      "\tTrain Loss: nan | Train f1: 0.00 | Train precision: 0.00 | Train recall: 0.00\n",
      "\t Val. Loss: nan |  Val. f1: 0.00 |  Val. precision: 0.00 | Val. recall: 0.00\n",
      "Epoch: 11 | Epoch Time: 0m 8s\n",
      "\tTrain Loss: nan | Train f1: 0.00 | Train precision: 0.00 | Train recall: 0.00\n",
      "\t Val. Loss: nan |  Val. f1: 0.00 |  Val. precision: 0.00 | Val. recall: 0.00\n",
      "\n",
      "Performance of best found Model:\n",
      "Val. Loss: 6.421 |  Val. f1: 0.01 | Val. precision: 0.02 | Val. recall: 0.00\n"
     ]
    }
   ],
   "source": [
    "model = elman_model\n",
    "model_name = elman_model_name\n",
    "criterion = baseline_criterion\n",
    "\n",
    "model.apply(init_weights)\n",
    "print(f'El modelo actual tiene {count_parameters(model):,} parámetros entrenables.')\n",
    "\n",
    "lRate = 0.0005\n",
    "#decayRate = 3\n",
    "decayRate = 2\n",
    "decayFactor = 0.9\n",
    "#decayFactor = 0.75\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr = lRate)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size = decayRate, \n",
    "                                      gamma = decayFactor)\n",
    "model = model.to(device)\n",
    "criterion = criterion.to(device)\n",
    "\n",
    "optimize_model(model, train_iterator, valid_iterator, optimizer, \n",
    "               criterion, scheduler = scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 799
    },
    "colab_type": "code",
    "id": "R5F2tduufKo7",
    "outputId": "fa2e1d0c-d760-408d-f654-ba7be7ae2f3b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El modelo actual tiene 4,347,838 parámetros entrenables.\n",
      "Epoch: 01 | Epoch Time: 0m 10s\n",
      "\tTrain Loss: 0.411 | Train f1: 0.22 | Train precision: 0.34 | Train recall: 0.19\n",
      "\t Val. Loss: 0.284 |  Val. f1: 0.41 |  Val. precision: 0.54 | Val. recall: 0.39\n",
      "Epoch: 02 | Epoch Time: 0m 10s\n",
      "\tTrain Loss: 0.176 | Train f1: 0.54 | Train precision: 0.62 | Train recall: 0.52\n",
      "\t Val. Loss: 0.218 |  Val. f1: 0.53 |  Val. precision: 0.62 | Val. recall: 0.52\n",
      "Epoch: 03 | Epoch Time: 0m 10s\n",
      "\tTrain Loss: 0.102 | Train f1: 0.67 | Train precision: 0.73 | Train recall: 0.67\n",
      "\t Val. Loss: 0.214 |  Val. f1: 0.58 |  Val. precision: 0.66 | Val. recall: 0.56\n",
      "Epoch: 04 | Epoch Time: 0m 10s\n",
      "\tTrain Loss: 0.067 | Train f1: 0.75 | Train precision: 0.78 | Train recall: 0.75\n",
      "\t Val. Loss: 0.215 |  Val. f1: 0.59 |  Val. precision: 0.67 | Val. recall: 0.58\n",
      "Epoch: 05 | Epoch Time: 0m 10s\n",
      "\tTrain Loss: 0.045 | Train f1: 0.81 | Train precision: 0.83 | Train recall: 0.81\n",
      "\t Val. Loss: 0.202 |  Val. f1: 0.62 |  Val. precision: 0.69 | Val. recall: 0.61\n",
      "Epoch: 06 | Epoch Time: 0m 10s\n",
      "\tTrain Loss: 0.033 | Train f1: 0.85 | Train precision: 0.87 | Train recall: 0.85\n",
      "\t Val. Loss: 0.205 |  Val. f1: 0.63 |  Val. precision: 0.69 | Val. recall: 0.62\n",
      "Epoch: 07 | Epoch Time: 0m 10s\n",
      "\tTrain Loss: 0.027 | Train f1: 0.87 | Train precision: 0.89 | Train recall: 0.87\n",
      "\t Val. Loss: 0.220 |  Val. f1: 0.63 |  Val. precision: 0.69 | Val. recall: 0.63\n",
      "Epoch: 08 | Epoch Time: 0m 10s\n",
      "\tTrain Loss: 0.022 | Train f1: 0.90 | Train precision: 0.91 | Train recall: 0.89\n",
      "\t Val. Loss: 0.268 |  Val. f1: 0.60 |  Val. precision: 0.68 | Val. recall: 0.58\n",
      "Epoch: 09 | Epoch Time: 0m 10s\n",
      "\tTrain Loss: 0.017 | Train f1: 0.91 | Train precision: 0.92 | Train recall: 0.91\n",
      "\t Val. Loss: 0.251 |  Val. f1: 0.61 |  Val. precision: 0.68 | Val. recall: 0.61\n",
      "Epoch: 10 | Epoch Time: 0m 10s\n",
      "\tTrain Loss: 0.014 | Train f1: 0.93 | Train precision: 0.93 | Train recall: 0.93\n",
      "\t Val. Loss: 0.247 |  Val. f1: 0.63 |  Val. precision: 0.70 | Val. recall: 0.62\n",
      "Epoch: 11 | Epoch Time: 0m 10s\n",
      "\tTrain Loss: 0.012 | Train f1: 0.94 | Train precision: 0.94 | Train recall: 0.93\n",
      "\t Val. Loss: 0.245 |  Val. f1: 0.63 |  Val. precision: 0.69 | Val. recall: 0.62\n",
      "Epoch: 12 | Epoch Time: 0m 10s\n",
      "\tTrain Loss: 0.012 | Train f1: 0.94 | Train precision: 0.95 | Train recall: 0.94\n",
      "\t Val. Loss: 0.256 |  Val. f1: 0.62 |  Val. precision: 0.68 | Val. recall: 0.60\n",
      "Epoch: 13 | Epoch Time: 0m 10s\n",
      "\tTrain Loss: 0.009 | Train f1: 0.95 | Train precision: 0.95 | Train recall: 0.95\n",
      "\t Val. Loss: 0.289 |  Val. f1: 0.62 |  Val. precision: 0.69 | Val. recall: 0.61\n",
      "Epoch: 14 | Epoch Time: 0m 10s\n",
      "\tTrain Loss: 0.009 | Train f1: 0.96 | Train precision: 0.96 | Train recall: 0.95\n",
      "\t Val. Loss: 0.308 |  Val. f1: 0.61 |  Val. precision: 0.69 | Val. recall: 0.60\n",
      "\n",
      "Performance of best found Model:\n",
      "Val. Loss: 0.202 |  Val. f1: 0.62 | Val. precision: 0.69 | Val. recall: 0.61\n"
     ]
    }
   ],
   "source": [
    "model = gru_model\n",
    "model_name = gru_model_name\n",
    "criterion = baseline_criterion\n",
    "\n",
    "model.apply(init_weights)\n",
    "print(f'El modelo actual tiene {count_parameters(model):,} parámetros entrenables.')\n",
    "\n",
    "lRate = 0.0005\n",
    "#decayRate = 3\n",
    "decayRate = 2\n",
    "decayFactor = 0.9\n",
    "#decayFactor = 0.75\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr = lRate)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size = decayRate, \n",
    "                                      gamma = decayFactor)\n",
    "model = model.to(device)\n",
    "criterion = criterion.to(device)\n",
    "\n",
    "optimize_model(model, train_iterator, valid_iterator, optimizer, \n",
    "               criterion, scheduler = scheduler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dY4IdlA1DLnv"
   },
   "source": [
    "En términos generales, no se aprecian mayores diferencias al incorporar un ```Scheduler``` para el *learning rate* y, de momento, el valor por defecto de este para el optimizador ```Adam``` parece adecuado."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gfscR0MQEQFi"
   },
   "source": [
    "#### Profundidad de la red"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Vn6gY1EdESjJ"
   },
   "source": [
    "Hasta el momento, al haber variado la cantidad de parámetros en las capas de las redes se ha encontrado que las arquitecturas con compuertas se comportan mejor con 256 unidades por capa, mientras que la `Elmann RNN` ofrece un desempeño comparable para 128 unidades en la capa oculta, deteriorándose al aumentar esta cantidad.\n",
    "\n",
    "Con esto en consideración, se propone variar la cantidad de capas de la red, más precisamente considerar modelos `Stacked RNN`. Dado que los modelos probados hasta ahora constan de dos capas de profundidad se hace una prueba con tan solo una a modo de *sanity check* para luego aumentar su número y observar el efecto percibido en el desempeño."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Da2noFkQFtna"
   },
   "outputs": [],
   "source": [
    "## Parámetros del Experimento ##\n",
    "\n",
    "# tamaño del vocabulario. recuerden que la entrada son vectores bag of word(one-hot).\n",
    "INPUT_DIM = len(TEXT.vocab)\n",
    "EMBEDDING_DIM = 100  # dimensión de los embeddings.\n",
    "HIDDEN_DIM = 256  # dimensión de la capas RNN\n",
    "OUTPUT_DIM = len(NER_TAGS.vocab)  # número de clases\n",
    "\n",
    "N_LAYERS = 1  # número de capas.\n",
    "DROPOUT = 0.25\n",
    "BIDIRECTIONAL = True\n",
    "\n",
    "TAG_PAD_IDX = NER_TAGS.vocab.stoi[NER_TAGS.pad_token]\n",
    "baseline_criterion = nn.CrossEntropyLoss(ignore_index = TAG_PAD_IDX)\n",
    "\n",
    "# Creamos nuestros modelos.\n",
    "lstm_model = NER_RNN(INPUT_DIM, EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM,\n",
    "                         N_LAYERS, BIDIRECTIONAL, DROPOUT, PAD_IDX)\n",
    "\n",
    "lstm_model_name = 'LSTM'  # nombre que tendrá el modelo guardado...\n",
    "\n",
    "elman_model = NER_ELMAN(INPUT_DIM, EMBEDDING_DIM, int(HIDDEN_DIM/2), OUTPUT_DIM,\n",
    "                         N_LAYERS, BIDIRECTIONAL, DROPOUT, PAD_IDX, 'relu')\n",
    "\n",
    "elman_model_name = 'Elman'  # nombre que tendrá el modelo guardado...\n",
    "\n",
    "gru_model = NER_GRU(INPUT_DIM, EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM,\n",
    "                         N_LAYERS, BIDIRECTIONAL, DROPOUT, PAD_IDX)\n",
    "\n",
    "gru_model_name = 'GRU'  # nombre que tendrá el modelo guardado..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 697
    },
    "colab_type": "code",
    "id": "v9LrFOt5GF-0",
    "outputId": "3eb3df00-e773-45de-a951-7aca4028a385"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El modelo actual tiene 3,348,414 parámetros entrenables.\n",
      "Epoch: 01 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.335 | Train f1: 0.29 | Train precision: 0.37 | Train recall: 0.27\n",
      "\t Val. Loss: 0.237 |  Val. f1: 0.45 |  Val. precision: 0.55 | Val. recall: 0.44\n",
      "Epoch: 02 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.117 | Train f1: 0.63 | Train precision: 0.68 | Train recall: 0.62\n",
      "\t Val. Loss: 0.184 |  Val. f1: 0.59 |  Val. precision: 0.65 | Val. recall: 0.58\n",
      "Epoch: 03 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.065 | Train f1: 0.76 | Train precision: 0.79 | Train recall: 0.76\n",
      "\t Val. Loss: 0.188 |  Val. f1: 0.60 |  Val. precision: 0.67 | Val. recall: 0.60\n",
      "Epoch: 04 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.041 | Train f1: 0.83 | Train precision: 0.85 | Train recall: 0.83\n",
      "\t Val. Loss: 0.204 |  Val. f1: 0.61 |  Val. precision: 0.69 | Val. recall: 0.59\n",
      "Epoch: 05 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.028 | Train f1: 0.88 | Train precision: 0.89 | Train recall: 0.88\n",
      "\t Val. Loss: 0.197 |  Val. f1: 0.62 |  Val. precision: 0.69 | Val. recall: 0.61\n",
      "Epoch: 06 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.019 | Train f1: 0.91 | Train precision: 0.92 | Train recall: 0.91\n",
      "\t Val. Loss: 0.213 |  Val. f1: 0.61 |  Val. precision: 0.68 | Val. recall: 0.60\n",
      "Epoch: 07 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.015 | Train f1: 0.93 | Train precision: 0.94 | Train recall: 0.93\n",
      "\t Val. Loss: 0.256 |  Val. f1: 0.59 |  Val. precision: 0.65 | Val. recall: 0.60\n",
      "Epoch: 08 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.012 | Train f1: 0.94 | Train precision: 0.95 | Train recall: 0.94\n",
      "\t Val. Loss: 0.261 |  Val. f1: 0.60 |  Val. precision: 0.66 | Val. recall: 0.59\n",
      "Epoch: 09 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.011 | Train f1: 0.95 | Train precision: 0.95 | Train recall: 0.95\n",
      "\t Val. Loss: 0.239 |  Val. f1: 0.61 |  Val. precision: 0.67 | Val. recall: 0.62\n",
      "Epoch: 10 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.009 | Train f1: 0.96 | Train precision: 0.96 | Train recall: 0.96\n",
      "\t Val. Loss: 0.274 |  Val. f1: 0.61 |  Val. precision: 0.68 | Val. recall: 0.59\n",
      "Epoch: 11 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.007 | Train f1: 0.96 | Train precision: 0.97 | Train recall: 0.96\n",
      "\t Val. Loss: 0.244 |  Val. f1: 0.61 |  Val. precision: 0.66 | Val. recall: 0.61\n",
      "Epoch: 12 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.007 | Train f1: 0.96 | Train precision: 0.97 | Train recall: 0.96\n",
      "\t Val. Loss: 0.273 |  Val. f1: 0.61 |  Val. precision: 0.67 | Val. recall: 0.60\n",
      "\n",
      "Performance of best found Model:\n",
      "Val. Loss: 0.184 |  Val. f1: 0.59 | Val. precision: 0.65 | Val. recall: 0.58\n"
     ]
    }
   ],
   "source": [
    "model = lstm_model\n",
    "model_name = lstm_model_name\n",
    "criterion = baseline_criterion\n",
    "\n",
    "model.apply(init_weights)\n",
    "print(f'El modelo actual tiene {count_parameters(model):,} parámetros entrenables.')\n",
    "\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "model = model.to(device)\n",
    "criterion = criterion.to(device)\n",
    "\n",
    "optimize_model(model, train_iterator, valid_iterator, optimizer, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 748
    },
    "colab_type": "code",
    "id": "vbn0FxZoG3iE",
    "outputId": "c813a6fb-ac17-4ae8-9890-ec7c5346c102"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El modelo actual tiene 2,671,550 parámetros entrenables.\n",
      "Epoch: 01 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.392 | Train f1: 0.21 | Train precision: 0.28 | Train recall: 0.19\n",
      "\t Val. Loss: 0.266 |  Val. f1: 0.41 |  Val. precision: 0.49 | Val. recall: 0.41\n",
      "Epoch: 02 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.148 | Train f1: 0.54 | Train precision: 0.60 | Train recall: 0.53\n",
      "\t Val. Loss: 0.201 |  Val. f1: 0.55 |  Val. precision: 0.63 | Val. recall: 0.53\n",
      "Epoch: 03 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.085 | Train f1: 0.70 | Train precision: 0.74 | Train recall: 0.70\n",
      "\t Val. Loss: 0.174 |  Val. f1: 0.60 |  Val. precision: 0.66 | Val. recall: 0.60\n",
      "Epoch: 04 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.055 | Train f1: 0.79 | Train precision: 0.81 | Train recall: 0.79\n",
      "\t Val. Loss: 0.180 |  Val. f1: 0.63 |  Val. precision: 0.69 | Val. recall: 0.63\n",
      "Epoch: 05 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.037 | Train f1: 0.84 | Train precision: 0.85 | Train recall: 0.84\n",
      "\t Val. Loss: 0.201 |  Val. f1: 0.62 |  Val. precision: 0.68 | Val. recall: 0.62\n",
      "Epoch: 06 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.028 | Train f1: 0.87 | Train precision: 0.89 | Train recall: 0.87\n",
      "\t Val. Loss: 0.202 |  Val. f1: 0.62 |  Val. precision: 0.69 | Val. recall: 0.61\n",
      "Epoch: 07 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.022 | Train f1: 0.90 | Train precision: 0.91 | Train recall: 0.90\n",
      "\t Val. Loss: 0.210 |  Val. f1: 0.61 |  Val. precision: 0.68 | Val. recall: 0.60\n",
      "Epoch: 08 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.016 | Train f1: 0.92 | Train precision: 0.93 | Train recall: 0.92\n",
      "\t Val. Loss: 0.216 |  Val. f1: 0.62 |  Val. precision: 0.68 | Val. recall: 0.61\n",
      "Epoch: 09 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.014 | Train f1: 0.93 | Train precision: 0.94 | Train recall: 0.93\n",
      "\t Val. Loss: 0.257 |  Val. f1: 0.58 |  Val. precision: 0.65 | Val. recall: 0.58\n",
      "Epoch: 10 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.013 | Train f1: 0.93 | Train precision: 0.94 | Train recall: 0.93\n",
      "\t Val. Loss: 0.221 |  Val. f1: 0.62 |  Val. precision: 0.68 | Val. recall: 0.62\n",
      "Epoch: 11 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.011 | Train f1: 0.95 | Train precision: 0.95 | Train recall: 0.95\n",
      "\t Val. Loss: 0.211 |  Val. f1: 0.63 |  Val. precision: 0.68 | Val. recall: 0.64\n",
      "Epoch: 12 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.011 | Train f1: 0.95 | Train precision: 0.95 | Train recall: 0.95\n",
      "\t Val. Loss: 0.244 |  Val. f1: 0.60 |  Val. precision: 0.67 | Val. recall: 0.60\n",
      "Epoch: 13 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.009 | Train f1: 0.96 | Train precision: 0.96 | Train recall: 0.96\n",
      "\t Val. Loss: 0.242 |  Val. f1: 0.61 |  Val. precision: 0.67 | Val. recall: 0.60\n",
      "\n",
      "Performance of best found Model:\n",
      "Val. Loss: 0.174 |  Val. f1: 0.60 | Val. precision: 0.66 | Val. recall: 0.60\n"
     ]
    }
   ],
   "source": [
    "model = elman_model\n",
    "model_name = elman_model_name\n",
    "criterion = baseline_criterion\n",
    "\n",
    "model.apply(init_weights)\n",
    "print(f'El modelo actual tiene {count_parameters(model):,} parámetros entrenables.')\n",
    "\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "model = model.to(device)\n",
    "criterion = criterion.to(device)\n",
    "\n",
    "optimize_model(model, train_iterator, valid_iterator, optimizer, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 785
    },
    "colab_type": "code",
    "id": "pPvycmIdG3u_",
    "outputId": "1deaceb2-4a48-4998-f814-c527792bbddf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El modelo actual tiene 3,165,118 parámetros entrenables.\n",
      "Epoch: 01 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.316 | Train f1: 0.33 | Train precision: 0.43 | Train recall: 0.31\n",
      "\t Val. Loss: 0.219 |  Val. f1: 0.52 |  Val. precision: 0.60 | Val. recall: 0.51\n",
      "Epoch: 02 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.110 | Train f1: 0.66 | Train precision: 0.71 | Train recall: 0.65\n",
      "\t Val. Loss: 0.182 |  Val. f1: 0.58 |  Val. precision: 0.65 | Val. recall: 0.57\n",
      "Epoch: 03 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.059 | Train f1: 0.78 | Train precision: 0.81 | Train recall: 0.78\n",
      "\t Val. Loss: 0.185 |  Val. f1: 0.61 |  Val. precision: 0.68 | Val. recall: 0.60\n",
      "Epoch: 04 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.035 | Train f1: 0.85 | Train precision: 0.87 | Train recall: 0.85\n",
      "\t Val. Loss: 0.191 |  Val. f1: 0.61 |  Val. precision: 0.67 | Val. recall: 0.61\n",
      "Epoch: 05 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.023 | Train f1: 0.89 | Train precision: 0.90 | Train recall: 0.89\n",
      "\t Val. Loss: 0.201 |  Val. f1: 0.62 |  Val. precision: 0.68 | Val. recall: 0.61\n",
      "Epoch: 06 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.017 | Train f1: 0.91 | Train precision: 0.92 | Train recall: 0.91\n",
      "\t Val. Loss: 0.208 |  Val. f1: 0.63 |  Val. precision: 0.68 | Val. recall: 0.62\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/numpy/lib/function_base.py:393: RuntimeWarning: Mean of empty slice.\n",
      "  avg = a.mean(axis)\n",
      "/usr/local/lib/python3.6/dist-packages/numpy/core/_methods.py:161: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 07 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.015 | Train f1: nan | Train precision: nan | Train recall: nan\n",
      "\t Val. Loss: 0.226 |  Val. f1: 0.62 |  Val. precision: 0.69 | Val. recall: 0.61\n",
      "Epoch: 08 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.011 | Train f1: 0.95 | Train precision: 0.95 | Train recall: 0.95\n",
      "\t Val. Loss: 0.220 |  Val. f1: 0.63 |  Val. precision: 0.68 | Val. recall: 0.62\n",
      "Epoch: 09 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.010 | Train f1: 0.95 | Train precision: 0.95 | Train recall: 0.95\n",
      "\t Val. Loss: 0.234 |  Val. f1: 0.63 |  Val. precision: 0.69 | Val. recall: 0.62\n",
      "Epoch: 10 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.009 | Train f1: 0.96 | Train precision: 0.96 | Train recall: 0.96\n",
      "\t Val. Loss: 0.241 |  Val. f1: 0.62 |  Val. precision: 0.68 | Val. recall: 0.61\n",
      "Epoch: 11 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.009 | Train f1: 0.96 | Train precision: 0.96 | Train recall: 0.96\n",
      "\t Val. Loss: 0.267 |  Val. f1: 0.61 |  Val. precision: 0.68 | Val. recall: 0.60\n",
      "Epoch: 12 | Epoch Time: 0m 5s\n",
      "\tTrain Loss: 0.007 | Train f1: 0.96 | Train precision: 0.97 | Train recall: 0.96\n",
      "\t Val. Loss: 0.257 |  Val. f1: 0.62 |  Val. precision: 0.69 | Val. recall: 0.61\n",
      "\n",
      "Performance of best found Model:\n",
      "Val. Loss: 0.182 |  Val. f1: 0.58 | Val. precision: 0.65 | Val. recall: 0.57\n"
     ]
    }
   ],
   "source": [
    "model = gru_model\n",
    "model_name = gru_model_name\n",
    "criterion = baseline_criterion\n",
    "\n",
    "model.apply(init_weights)\n",
    "print(f'El modelo actual tiene {count_parameters(model):,} parámetros entrenables.')\n",
    "\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "model = model.to(device)\n",
    "criterion = criterion.to(device)\n",
    "\n",
    "optimize_model(model, train_iterator, valid_iterator, optimizer, criterion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "f_IIVhYOPA9-"
   },
   "source": [
    "Como cabría de esperar, al disminuir la profundidad de la red se pierde capacidad predictiva."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "A37XolTGPJiI"
   },
   "outputs": [],
   "source": [
    "## Parámetros del Experimento ##\n",
    "\n",
    "# tamaño del vocabulario. recuerden que la entrada son vectores bag of word(one-hot).\n",
    "INPUT_DIM = len(TEXT.vocab)\n",
    "EMBEDDING_DIM = 100  # dimensión de los embeddings.\n",
    "HIDDEN_DIM = 256  # dimensión de la capas RNN\n",
    "OUTPUT_DIM = len(NER_TAGS.vocab)  # número de clases\n",
    "\n",
    "N_LAYERS = 3  # número de capas.\n",
    "DROPOUT = 0.25\n",
    "BIDIRECTIONAL = True\n",
    "\n",
    "TAG_PAD_IDX = NER_TAGS.vocab.stoi[NER_TAGS.pad_token]\n",
    "baseline_criterion = nn.CrossEntropyLoss(ignore_index = TAG_PAD_IDX)\n",
    "\n",
    "# Creamos nuestros modelos.\n",
    "lstm_model = NER_RNN(INPUT_DIM, EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM,\n",
    "                         N_LAYERS, BIDIRECTIONAL, DROPOUT, PAD_IDX)\n",
    "\n",
    "lstm_model_name = 'LSTM'  # nombre que tendrá el modelo guardado...\n",
    "\n",
    "elman_model = NER_ELMAN(INPUT_DIM, EMBEDDING_DIM, int(HIDDEN_DIM/2), OUTPUT_DIM,\n",
    "                         N_LAYERS, BIDIRECTIONAL, DROPOUT, PAD_IDX, 'relu')\n",
    "\n",
    "elman_model_name = 'Elman'  # nombre que tendrá el modelo guardado...\n",
    "\n",
    "gru_model = NER_GRU(INPUT_DIM, EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM,\n",
    "                         N_LAYERS, BIDIRECTIONAL, DROPOUT, PAD_IDX)\n",
    "\n",
    "gru_model_name = 'GRU'  # nombre que tendrá el modelo guardado..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 697
    },
    "colab_type": "code",
    "id": "z4dKynr3PNYB",
    "outputId": "9906e251-245a-4a4a-d7f5-fe7eadca6107"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El modelo actual tiene 6,502,334 parámetros entrenables.\n",
      "Epoch: 01 | Epoch Time: 0m 18s\n",
      "\tTrain Loss: 0.364 | Train f1: 0.25 | Train precision: 0.32 | Train recall: 0.24\n",
      "\t Val. Loss: 0.243 |  Val. f1: 0.46 |  Val. precision: 0.56 | Val. recall: 0.44\n",
      "Epoch: 02 | Epoch Time: 0m 18s\n",
      "\tTrain Loss: 0.128 | Train f1: 0.60 | Train precision: 0.66 | Train recall: 0.60\n",
      "\t Val. Loss: 0.188 |  Val. f1: 0.57 |  Val. precision: 0.64 | Val. recall: 0.57\n",
      "Epoch: 03 | Epoch Time: 0m 18s\n",
      "\tTrain Loss: 0.071 | Train f1: 0.73 | Train precision: 0.77 | Train recall: 0.73\n",
      "\t Val. Loss: 0.206 |  Val. f1: 0.59 |  Val. precision: 0.66 | Val. recall: 0.58\n",
      "Epoch: 04 | Epoch Time: 0m 18s\n",
      "\tTrain Loss: 0.045 | Train f1: 0.82 | Train precision: 0.84 | Train recall: 0.82\n",
      "\t Val. Loss: 0.197 |  Val. f1: 0.61 |  Val. precision: 0.66 | Val. recall: 0.62\n",
      "Epoch: 05 | Epoch Time: 0m 18s\n",
      "\tTrain Loss: 0.032 | Train f1: 0.86 | Train precision: 0.88 | Train recall: 0.86\n",
      "\t Val. Loss: 0.210 |  Val. f1: 0.61 |  Val. precision: 0.66 | Val. recall: 0.62\n",
      "Epoch: 06 | Epoch Time: 0m 18s\n",
      "\tTrain Loss: 0.024 | Train f1: 0.89 | Train precision: 0.90 | Train recall: 0.89\n",
      "\t Val. Loss: 0.217 |  Val. f1: 0.63 |  Val. precision: 0.67 | Val. recall: 0.63\n",
      "Epoch: 07 | Epoch Time: 0m 18s\n",
      "\tTrain Loss: 0.019 | Train f1: 0.91 | Train precision: 0.92 | Train recall: 0.91\n",
      "\t Val. Loss: 0.232 |  Val. f1: 0.62 |  Val. precision: 0.67 | Val. recall: 0.62\n",
      "Epoch: 08 | Epoch Time: 0m 18s\n",
      "\tTrain Loss: 0.016 | Train f1: 0.93 | Train precision: 0.93 | Train recall: 0.93\n",
      "\t Val. Loss: 0.245 |  Val. f1: 0.61 |  Val. precision: 0.66 | Val. recall: 0.62\n",
      "Epoch: 09 | Epoch Time: 0m 18s\n",
      "\tTrain Loss: 0.013 | Train f1: 0.94 | Train precision: 0.95 | Train recall: 0.94\n",
      "\t Val. Loss: 0.259 |  Val. f1: 0.62 |  Val. precision: 0.67 | Val. recall: 0.62\n",
      "Epoch: 10 | Epoch Time: 0m 18s\n",
      "\tTrain Loss: 0.012 | Train f1: 0.94 | Train precision: 0.95 | Train recall: 0.94\n",
      "\t Val. Loss: 0.262 |  Val. f1: 0.62 |  Val. precision: 0.67 | Val. recall: 0.62\n",
      "Epoch: 11 | Epoch Time: 0m 18s\n",
      "\tTrain Loss: 0.011 | Train f1: 0.95 | Train precision: 0.95 | Train recall: 0.95\n",
      "\t Val. Loss: 0.281 |  Val. f1: 0.61 |  Val. precision: 0.67 | Val. recall: 0.61\n",
      "Epoch: 12 | Epoch Time: 0m 18s\n",
      "\tTrain Loss: 0.010 | Train f1: 0.96 | Train precision: 0.96 | Train recall: 0.96\n",
      "\t Val. Loss: 0.272 |  Val. f1: 0.61 |  Val. precision: 0.66 | Val. recall: 0.62\n",
      "\n",
      "Performance of best found Model:\n",
      "Val. Loss: 0.188 |  Val. f1: 0.57 | Val. precision: 0.64 | Val. recall: 0.57\n"
     ]
    }
   ],
   "source": [
    "model = lstm_model\n",
    "model_name = lstm_model_name\n",
    "criterion = baseline_criterion\n",
    "\n",
    "model.apply(init_weights)\n",
    "print(f'El modelo actual tiene {count_parameters(model):,} parámetros entrenables.')\n",
    "\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "model = model.to(device)\n",
    "criterion = criterion.to(device)\n",
    "\n",
    "optimize_model(model, train_iterator, valid_iterator, optimizer, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 799
    },
    "colab_type": "code",
    "id": "FYTNYBlRPPu3",
    "outputId": "9182c6e6-516d-4618-880c-238d27dc9b63"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El modelo actual tiene 2,869,182 parámetros entrenables.\n",
      "Epoch: 01 | Epoch Time: 0m 13s\n",
      "\tTrain Loss: 0.441 | Train f1: 0.17 | Train precision: 0.23 | Train recall: 0.15\n",
      "\t Val. Loss: 0.280 |  Val. f1: 0.33 |  Val. precision: 0.39 | Val. recall: 0.33\n",
      "Epoch: 02 | Epoch Time: 0m 13s\n",
      "\tTrain Loss: 0.176 | Train f1: 0.46 | Train precision: 0.52 | Train recall: 0.46\n",
      "\t Val. Loss: 0.216 |  Val. f1: 0.50 |  Val. precision: 0.58 | Val. recall: 0.50\n",
      "Epoch: 03 | Epoch Time: 0m 13s\n",
      "\tTrain Loss: 0.106 | Train f1: 0.63 | Train precision: 0.68 | Train recall: 0.63\n",
      "\t Val. Loss: 0.199 |  Val. f1: 0.57 |  Val. precision: 0.63 | Val. recall: 0.58\n",
      "Epoch: 04 | Epoch Time: 0m 13s\n",
      "\tTrain Loss: 0.073 | Train f1: 0.72 | Train precision: 0.75 | Train recall: 0.72\n",
      "\t Val. Loss: 0.208 |  Val. f1: 0.56 |  Val. precision: 0.64 | Val. recall: 0.56\n",
      "Epoch: 05 | Epoch Time: 0m 13s\n",
      "\tTrain Loss: 0.055 | Train f1: 0.79 | Train precision: 0.81 | Train recall: 0.79\n",
      "\t Val. Loss: 0.195 |  Val. f1: 0.60 |  Val. precision: 0.65 | Val. recall: 0.62\n",
      "Epoch: 06 | Epoch Time: 0m 13s\n",
      "\tTrain Loss: 0.042 | Train f1: 0.83 | Train precision: 0.85 | Train recall: 0.83\n",
      "\t Val. Loss: 0.220 |  Val. f1: 0.59 |  Val. precision: 0.65 | Val. recall: 0.60\n",
      "Epoch: 07 | Epoch Time: 0m 13s\n",
      "\tTrain Loss: 0.033 | Train f1: 0.86 | Train precision: 0.87 | Train recall: 0.86\n",
      "\t Val. Loss: 0.206 |  Val. f1: 0.62 |  Val. precision: 0.66 | Val. recall: 0.64\n",
      "Epoch: 08 | Epoch Time: 0m 13s\n",
      "\tTrain Loss: 0.026 | Train f1: 0.88 | Train precision: 0.89 | Train recall: 0.88\n",
      "\t Val. Loss: 0.253 |  Val. f1: 0.60 |  Val. precision: 0.65 | Val. recall: 0.61\n",
      "Epoch: 09 | Epoch Time: 0m 13s\n",
      "\tTrain Loss: 0.025 | Train f1: 0.89 | Train precision: 0.90 | Train recall: 0.89\n",
      "\t Val. Loss: 0.234 |  Val. f1: 0.62 |  Val. precision: 0.67 | Val. recall: 0.62\n",
      "Epoch: 10 | Epoch Time: 0m 13s\n",
      "\tTrain Loss: 0.020 | Train f1: 0.91 | Train precision: 0.92 | Train recall: 0.91\n",
      "\t Val. Loss: 0.242 |  Val. f1: 0.62 |  Val. precision: 0.67 | Val. recall: 0.62\n",
      "Epoch: 11 | Epoch Time: 0m 12s\n",
      "\tTrain Loss: 0.019 | Train f1: 0.92 | Train precision: 0.92 | Train recall: 0.92\n",
      "\t Val. Loss: 0.247 |  Val. f1: 0.61 |  Val. precision: 0.66 | Val. recall: 0.63\n",
      "Epoch: 12 | Epoch Time: 0m 13s\n",
      "\tTrain Loss: 0.016 | Train f1: 0.92 | Train precision: 0.93 | Train recall: 0.92\n",
      "\t Val. Loss: 0.276 |  Val. f1: 0.60 |  Val. precision: 0.66 | Val. recall: 0.60\n",
      "Epoch: 13 | Epoch Time: 0m 13s\n",
      "\tTrain Loss: 0.015 | Train f1: 0.93 | Train precision: 0.93 | Train recall: 0.93\n",
      "\t Val. Loss: 0.268 |  Val. f1: 0.60 |  Val. precision: 0.65 | Val. recall: 0.60\n",
      "Epoch: 14 | Epoch Time: 0m 13s\n",
      "\tTrain Loss: 0.013 | Train f1: 0.94 | Train precision: 0.94 | Train recall: 0.94\n",
      "\t Val. Loss: 0.265 |  Val. f1: 0.61 |  Val. precision: 0.66 | Val. recall: 0.63\n",
      "\n",
      "Performance of best found Model:\n",
      "Val. Loss: 0.195 |  Val. f1: 0.60 | Val. precision: 0.65 | Val. recall: 0.62\n"
     ]
    }
   ],
   "source": [
    "model = elman_model\n",
    "model_name = elman_model_name\n",
    "criterion = baseline_criterion\n",
    "\n",
    "model.apply(init_weights)\n",
    "print(f'El modelo actual tiene {count_parameters(model):,} parámetros entrenables.')\n",
    "\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "model = model.to(device)\n",
    "criterion = criterion.to(device)\n",
    "\n",
    "optimize_model(model, train_iterator, valid_iterator, optimizer, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 748
    },
    "colab_type": "code",
    "id": "kkAiWC4HPSWO",
    "outputId": "98240cf7-e14f-48de-9f1e-75e9e78ec0f8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El modelo actual tiene 5,530,558 parámetros entrenables.\n",
      "Epoch: 01 | Epoch Time: 0m 17s\n",
      "\tTrain Loss: 0.430 | Train f1: 0.25 | Train precision: 0.34 | Train recall: 0.23\n",
      "\t Val. Loss: 0.255 |  Val. f1: 0.49 |  Val. precision: 0.59 | Val. recall: 0.48\n",
      "Epoch: 02 | Epoch Time: 0m 16s\n",
      "\tTrain Loss: 0.139 | Train f1: 0.60 | Train precision: 0.66 | Train recall: 0.60\n",
      "\t Val. Loss: 0.200 |  Val. f1: 0.58 |  Val. precision: 0.65 | Val. recall: 0.57\n",
      "Epoch: 03 | Epoch Time: 0m 16s\n",
      "\tTrain Loss: 0.073 | Train f1: 0.74 | Train precision: 0.77 | Train recall: 0.74\n",
      "\t Val. Loss: 0.184 |  Val. f1: 0.63 |  Val. precision: 0.69 | Val. recall: 0.63\n",
      "Epoch: 04 | Epoch Time: 0m 17s\n",
      "\tTrain Loss: 0.044 | Train f1: 0.82 | Train precision: 0.84 | Train recall: 0.82\n",
      "\t Val. Loss: 0.223 |  Val. f1: 0.61 |  Val. precision: 0.66 | Val. recall: 0.62\n",
      "Epoch: 05 | Epoch Time: 0m 17s\n",
      "\tTrain Loss: 0.031 | Train f1: 0.87 | Train precision: 0.88 | Train recall: 0.87\n",
      "\t Val. Loss: 0.225 |  Val. f1: 0.62 |  Val. precision: 0.67 | Val. recall: 0.63\n",
      "Epoch: 06 | Epoch Time: 0m 17s\n",
      "\tTrain Loss: 0.023 | Train f1: 0.89 | Train precision: 0.90 | Train recall: 0.89\n",
      "\t Val. Loss: 0.247 |  Val. f1: 0.61 |  Val. precision: 0.67 | Val. recall: 0.61\n",
      "Epoch: 07 | Epoch Time: 0m 16s\n",
      "\tTrain Loss: 0.019 | Train f1: 0.92 | Train precision: 0.92 | Train recall: 0.92\n",
      "\t Val. Loss: 0.252 |  Val. f1: 0.63 |  Val. precision: 0.68 | Val. recall: 0.63\n",
      "Epoch: 08 | Epoch Time: 0m 16s\n",
      "\tTrain Loss: 0.017 | Train f1: 0.92 | Train precision: 0.93 | Train recall: 0.92\n",
      "\t Val. Loss: 0.280 |  Val. f1: 0.63 |  Val. precision: 0.69 | Val. recall: 0.62\n",
      "Epoch: 09 | Epoch Time: 0m 16s\n",
      "\tTrain Loss: 0.014 | Train f1: 0.93 | Train precision: 0.94 | Train recall: 0.93\n",
      "\t Val. Loss: 0.329 |  Val. f1: 0.60 |  Val. precision: 0.66 | Val. recall: 0.61\n",
      "Epoch: 10 | Epoch Time: 0m 16s\n",
      "\tTrain Loss: 0.013 | Train f1: 0.94 | Train precision: 0.95 | Train recall: 0.94\n",
      "\t Val. Loss: 0.279 |  Val. f1: 0.61 |  Val. precision: 0.68 | Val. recall: 0.61\n",
      "Epoch: 11 | Epoch Time: 0m 16s\n",
      "\tTrain Loss: 0.012 | Train f1: 0.94 | Train precision: 0.95 | Train recall: 0.94\n",
      "\t Val. Loss: 0.296 |  Val. f1: 0.62 |  Val. precision: 0.67 | Val. recall: 0.62\n",
      "Epoch: 12 | Epoch Time: 0m 16s\n",
      "\tTrain Loss: 0.011 | Train f1: 0.95 | Train precision: 0.95 | Train recall: 0.95\n",
      "\t Val. Loss: 0.329 |  Val. f1: 0.60 |  Val. precision: 0.67 | Val. recall: 0.60\n",
      "Epoch: 13 | Epoch Time: 0m 16s\n",
      "\tTrain Loss: 0.011 | Train f1: 0.95 | Train precision: 0.96 | Train recall: 0.95\n",
      "\t Val. Loss: 0.325 |  Val. f1: 0.61 |  Val. precision: 0.67 | Val. recall: 0.61\n",
      "\n",
      "Performance of best found Model:\n",
      "Val. Loss: 0.184 |  Val. f1: 0.63 | Val. precision: 0.69 | Val. recall: 0.63\n"
     ]
    }
   ],
   "source": [
    "model = gru_model\n",
    "model_name = gru_model_name\n",
    "criterion = baseline_criterion\n",
    "\n",
    "model.apply(init_weights)\n",
    "print(f'El modelo actual tiene {count_parameters(model):,} parámetros entrenables.')\n",
    "\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "model = model.to(device)\n",
    "criterion = criterion.to(device)\n",
    "\n",
    "optimize_model(model, train_iterator, valid_iterator, optimizer, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "l59jyf2KUBGG"
   },
   "outputs": [],
   "source": [
    "## Parámetros del Experimento ##\n",
    "\n",
    "# tamaño del vocabulario. recuerden que la entrada son vectores bag of word(one-hot).\n",
    "INPUT_DIM = len(TEXT.vocab)\n",
    "EMBEDDING_DIM = 100  # dimensión de los embeddings.\n",
    "HIDDEN_DIM = 256  # dimensión de la capas RNN\n",
    "OUTPUT_DIM = len(NER_TAGS.vocab)  # número de clases\n",
    "\n",
    "N_LAYERS = 4  # número de capas.\n",
    "DROPOUT = 0.25\n",
    "BIDIRECTIONAL = True\n",
    "\n",
    "TAG_PAD_IDX = NER_TAGS.vocab.stoi[NER_TAGS.pad_token]\n",
    "baseline_criterion = nn.CrossEntropyLoss(ignore_index = TAG_PAD_IDX)\n",
    "\n",
    "# Creamos nuestros modelos.\n",
    "lstm_model = NER_RNN(INPUT_DIM, EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM,\n",
    "                         N_LAYERS, BIDIRECTIONAL, DROPOUT, PAD_IDX)\n",
    "\n",
    "lstm_model_name = 'LSTM'  # nombre que tendrá el modelo guardado...\n",
    "\n",
    "elman_model = NER_ELMAN(INPUT_DIM, EMBEDDING_DIM, int(HIDDEN_DIM/2), OUTPUT_DIM,\n",
    "                         N_LAYERS, BIDIRECTIONAL, DROPOUT, PAD_IDX, 'relu')\n",
    "\n",
    "elman_model_name = 'Elman'  # nombre que tendrá el modelo guardado...\n",
    "\n",
    "gru_model = NER_GRU(INPUT_DIM, EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM,\n",
    "                         N_LAYERS, BIDIRECTIONAL, DROPOUT, PAD_IDX)\n",
    "\n",
    "gru_model_name = 'GRU'  # nombre que tendrá el modelo guardado..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 748
    },
    "colab_type": "code",
    "id": "GFOLV_xRUEmn",
    "outputId": "877e07e7-88b3-4473-d17f-7ccb2f98ebf0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El modelo actual tiene 8,079,294 parámetros entrenables.\n",
      "Epoch: 01 | Epoch Time: 0m 25s\n",
      "\tTrain Loss: 0.426 | Train f1: 0.18 | Train precision: 0.23 | Train recall: 0.17\n",
      "\t Val. Loss: 0.319 |  Val. f1: 0.35 |  Val. precision: 0.47 | Val. recall: 0.35\n",
      "Epoch: 02 | Epoch Time: 0m 25s\n",
      "\tTrain Loss: 0.150 | Train f1: 0.54 | Train precision: 0.60 | Train recall: 0.54\n",
      "\t Val. Loss: 0.208 |  Val. f1: 0.55 |  Val. precision: 0.61 | Val. recall: 0.55\n",
      "Epoch: 03 | Epoch Time: 0m 25s\n",
      "\tTrain Loss: 0.085 | Train f1: 0.70 | Train precision: 0.74 | Train recall: 0.71\n",
      "\t Val. Loss: 0.203 |  Val. f1: 0.56 |  Val. precision: 0.63 | Val. recall: 0.56\n",
      "Epoch: 04 | Epoch Time: 0m 25s\n",
      "\tTrain Loss: 0.057 | Train f1: 0.78 | Train precision: 0.80 | Train recall: 0.78\n",
      "\t Val. Loss: 0.212 |  Val. f1: 0.59 |  Val. precision: 0.64 | Val. recall: 0.60\n",
      "Epoch: 05 | Epoch Time: 0m 25s\n",
      "\tTrain Loss: 0.041 | Train f1: 0.83 | Train precision: 0.85 | Train recall: 0.83\n",
      "\t Val. Loss: 0.229 |  Val. f1: 0.59 |  Val. precision: 0.65 | Val. recall: 0.61\n",
      "Epoch: 06 | Epoch Time: 0m 25s\n",
      "\tTrain Loss: 0.032 | Train f1: 0.86 | Train precision: 0.87 | Train recall: 0.86\n",
      "\t Val. Loss: 0.227 |  Val. f1: 0.60 |  Val. precision: 0.66 | Val. recall: 0.61\n",
      "Epoch: 07 | Epoch Time: 0m 25s\n",
      "\tTrain Loss: 0.027 | Train f1: 0.89 | Train precision: 0.90 | Train recall: 0.89\n",
      "\t Val. Loss: 0.234 |  Val. f1: 0.60 |  Val. precision: 0.65 | Val. recall: 0.62\n",
      "Epoch: 08 | Epoch Time: 0m 25s\n",
      "\tTrain Loss: 0.021 | Train f1: 0.91 | Train precision: 0.92 | Train recall: 0.91\n",
      "\t Val. Loss: 0.223 |  Val. f1: 0.61 |  Val. precision: 0.65 | Val. recall: 0.62\n",
      "Epoch: 09 | Epoch Time: 0m 25s\n",
      "\tTrain Loss: 0.018 | Train f1: 0.92 | Train precision: 0.93 | Train recall: 0.92\n",
      "\t Val. Loss: 0.249 |  Val. f1: 0.62 |  Val. precision: 0.66 | Val. recall: 0.63\n",
      "Epoch: 10 | Epoch Time: 0m 25s\n",
      "\tTrain Loss: 0.016 | Train f1: 0.93 | Train precision: 0.94 | Train recall: 0.93\n",
      "\t Val. Loss: 0.249 |  Val. f1: 0.61 |  Val. precision: 0.66 | Val. recall: 0.61\n",
      "Epoch: 11 | Epoch Time: 0m 25s\n",
      "\tTrain Loss: 0.014 | Train f1: 0.94 | Train precision: 0.95 | Train recall: 0.94\n",
      "\t Val. Loss: 0.240 |  Val. f1: 0.62 |  Val. precision: 0.67 | Val. recall: 0.62\n",
      "Epoch: 12 | Epoch Time: 0m 25s\n",
      "\tTrain Loss: 0.013 | Train f1: 0.94 | Train precision: 0.95 | Train recall: 0.94\n",
      "\t Val. Loss: 0.242 |  Val. f1: 0.61 |  Val. precision: 0.65 | Val. recall: 0.62\n",
      "Epoch: 13 | Epoch Time: 0m 25s\n",
      "\tTrain Loss: 0.010 | Train f1: 0.96 | Train precision: 0.96 | Train recall: 0.96\n",
      "\t Val. Loss: 0.280 |  Val. f1: 0.62 |  Val. precision: 0.66 | Val. recall: 0.64\n",
      "\n",
      "Performance of best found Model:\n",
      "Val. Loss: 0.203 |  Val. f1: 0.56 | Val. precision: 0.63 | Val. recall: 0.56\n"
     ]
    }
   ],
   "source": [
    "model = lstm_model\n",
    "model_name = lstm_model_name\n",
    "criterion = baseline_criterion\n",
    "\n",
    "model.apply(init_w  eights)\n",
    "print(f'El modelo actual tiene {count_parameters(model):,} parámetros entrenables.')\n",
    "\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "model = model.to(device)\n",
    "criterion = criterion.to(device)\n",
    "\n",
    "optimize_model(model, train_iterator, valid_iterator, optimizer, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 799
    },
    "colab_type": "code",
    "id": "rcsPYU9LUGWr",
    "outputId": "57dfd712-ae90-4e5a-cead-dbf1c75625f1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El modelo actual tiene 2,967,998 parámetros entrenables.\n",
      "Epoch: 01 | Epoch Time: 0m 17s\n",
      "\tTrain Loss: 0.472 | Train f1: 0.13 | Train precision: 0.20 | Train recall: 0.12\n",
      "\t Val. Loss: 0.310 |  Val. f1: 0.33 |  Val. precision: 0.36 | Val. recall: 0.36\n",
      "Epoch: 02 | Epoch Time: 0m 17s\n",
      "\tTrain Loss: 0.200 | Train f1: 0.43 | Train precision: 0.49 | Train recall: 0.43\n",
      "\t Val. Loss: 0.227 |  Val. f1: 0.50 |  Val. precision: 0.56 | Val. recall: 0.52\n",
      "Epoch: 03 | Epoch Time: 0m 17s\n",
      "\tTrain Loss: 0.125 | Train f1: 0.59 | Train precision: 0.64 | Train recall: 0.59\n",
      "\t Val. Loss: 0.227 |  Val. f1: 0.55 |  Val. precision: 0.62 | Val. recall: 0.55\n",
      "Epoch: 04 | Epoch Time: 0m 17s\n",
      "\tTrain Loss: 0.089 | Train f1: 0.69 | Train precision: 0.72 | Train recall: 0.69\n",
      "\t Val. Loss: 0.198 |  Val. f1: 0.61 |  Val. precision: 0.65 | Val. recall: 0.62\n",
      "Epoch: 05 | Epoch Time: 0m 17s\n",
      "\tTrain Loss: 0.067 | Train f1: 0.75 | Train precision: 0.78 | Train recall: 0.75\n",
      "\t Val. Loss: 0.221 |  Val. f1: 0.60 |  Val. precision: 0.64 | Val. recall: 0.63\n",
      "Epoch: 06 | Epoch Time: 0m 17s\n",
      "\tTrain Loss: 0.054 | Train f1: 0.79 | Train precision: 0.82 | Train recall: 0.80\n",
      "\t Val. Loss: 0.217 |  Val. f1: 0.60 |  Val. precision: 0.67 | Val. recall: 0.60\n",
      "Epoch: 07 | Epoch Time: 0m 16s\n",
      "\tTrain Loss: 0.042 | Train f1: 0.83 | Train precision: 0.85 | Train recall: 0.83\n",
      "\t Val. Loss: 0.201 |  Val. f1: 0.63 |  Val. precision: 0.67 | Val. recall: 0.64\n",
      "Epoch: 08 | Epoch Time: 0m 16s\n",
      "\tTrain Loss: 0.035 | Train f1: 0.85 | Train precision: 0.87 | Train recall: 0.85\n",
      "\t Val. Loss: 0.220 |  Val. f1: 0.63 |  Val. precision: 0.67 | Val. recall: 0.64\n",
      "Epoch: 09 | Epoch Time: 0m 16s\n",
      "\tTrain Loss: 0.030 | Train f1: 0.87 | Train precision: 0.89 | Train recall: 0.87\n",
      "\t Val. Loss: 0.214 |  Val. f1: 0.63 |  Val. precision: 0.68 | Val. recall: 0.64\n",
      "Epoch: 10 | Epoch Time: 0m 17s\n",
      "\tTrain Loss: 0.026 | Train f1: 0.89 | Train precision: 0.90 | Train recall: 0.89\n",
      "\t Val. Loss: 0.239 |  Val. f1: 0.63 |  Val. precision: 0.67 | Val. recall: 0.64\n",
      "Epoch: 11 | Epoch Time: 0m 17s\n",
      "\tTrain Loss: 0.025 | Train f1: 0.89 | Train precision: 0.91 | Train recall: 0.90\n",
      "\t Val. Loss: 0.285 |  Val. f1: 0.61 |  Val. precision: 0.67 | Val. recall: 0.63\n",
      "Epoch: 12 | Epoch Time: 0m 16s\n",
      "\tTrain Loss: 0.020 | Train f1: 0.91 | Train precision: 0.92 | Train recall: 0.91\n",
      "\t Val. Loss: 0.233 |  Val. f1: 0.63 |  Val. precision: 0.69 | Val. recall: 0.64\n",
      "Epoch: 13 | Epoch Time: 0m 16s\n",
      "\tTrain Loss: 0.017 | Train f1: 0.92 | Train precision: 0.93 | Train recall: 0.92\n",
      "\t Val. Loss: 0.241 |  Val. f1: 0.60 |  Val. precision: 0.65 | Val. recall: 0.63\n",
      "Epoch: 14 | Epoch Time: 0m 17s\n",
      "\tTrain Loss: 0.016 | Train f1: 0.93 | Train precision: 0.94 | Train recall: 0.93\n",
      "\t Val. Loss: 0.282 |  Val. f1: 0.62 |  Val. precision: 0.67 | Val. recall: 0.62\n",
      "\n",
      "Performance of best found Model:\n",
      "Val. Loss: 0.198 |  Val. f1: 0.61 | Val. precision: 0.65 | Val. recall: 0.62\n"
     ]
    }
   ],
   "source": [
    "model = elman_model\n",
    "model_name = elman_model_name\n",
    "criterion = baseline_criterion\n",
    "\n",
    "model.apply(init_weights)\n",
    "print(f'El modelo actual tiene {count_parameters(model):,} parámetros entrenables.')\n",
    "\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "model = model.to(device)\n",
    "criterion = criterion.to(device)\n",
    "\n",
    "optimize_model(model, train_iterator, valid_iterator, optimizer, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 748
    },
    "colab_type": "code",
    "id": "pZcZIZO2UIPF",
    "outputId": "cd7e153f-380e-4281-c657-94c337325553"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El modelo actual tiene 6,713,278 parámetros entrenables.\n",
      "Epoch: 01 | Epoch Time: 0m 23s\n",
      "\tTrain Loss: 0.510 | Train f1: 0.15 | Train precision: 0.20 | Train recall: 0.14\n",
      "\t Val. Loss: 0.278 |  Val. f1: 0.39 |  Val. precision: 0.48 | Val. recall: 0.38\n",
      "Epoch: 02 | Epoch Time: 0m 23s\n",
      "\tTrain Loss: 0.167 | Train f1: 0.52 | Train precision: 0.59 | Train recall: 0.52\n",
      "\t Val. Loss: 0.214 |  Val. f1: 0.55 |  Val. precision: 0.60 | Val. recall: 0.58\n",
      "Epoch: 03 | Epoch Time: 0m 23s\n",
      "\tTrain Loss: 0.091 | Train f1: 0.69 | Train precision: 0.73 | Train recall: 0.69\n",
      "\t Val. Loss: 0.202 |  Val. f1: 0.59 |  Val. precision: 0.65 | Val. recall: 0.58\n",
      "Epoch: 04 | Epoch Time: 0m 23s\n",
      "\tTrain Loss: 0.061 | Train f1: 0.78 | Train precision: 0.80 | Train recall: 0.78\n",
      "\t Val. Loss: 0.210 |  Val. f1: 0.61 |  Val. precision: 0.66 | Val. recall: 0.61\n",
      "Epoch: 05 | Epoch Time: 0m 23s\n",
      "\tTrain Loss: 0.043 | Train f1: 0.83 | Train precision: 0.85 | Train recall: 0.82\n",
      "\t Val. Loss: 0.227 |  Val. f1: 0.60 |  Val. precision: 0.65 | Val. recall: 0.61\n",
      "Epoch: 06 | Epoch Time: 0m 23s\n",
      "\tTrain Loss: 0.033 | Train f1: 0.86 | Train precision: 0.88 | Train recall: 0.86\n",
      "\t Val. Loss: 0.220 |  Val. f1: 0.63 |  Val. precision: 0.67 | Val. recall: 0.64\n",
      "Epoch: 07 | Epoch Time: 0m 23s\n",
      "\tTrain Loss: 0.027 | Train f1: 0.89 | Train precision: 0.90 | Train recall: 0.89\n",
      "\t Val. Loss: 0.250 |  Val. f1: 0.62 |  Val. precision: 0.66 | Val. recall: 0.64\n",
      "Epoch: 08 | Epoch Time: 0m 23s\n",
      "\tTrain Loss: 0.022 | Train f1: 0.90 | Train precision: 0.91 | Train recall: 0.90\n",
      "\t Val. Loss: 0.262 |  Val. f1: 0.62 |  Val. precision: 0.67 | Val. recall: 0.62\n",
      "Epoch: 09 | Epoch Time: 0m 23s\n",
      "\tTrain Loss: 0.019 | Train f1: 0.92 | Train precision: 0.93 | Train recall: 0.92\n",
      "\t Val. Loss: 0.271 |  Val. f1: 0.61 |  Val. precision: 0.67 | Val. recall: 0.61\n",
      "Epoch: 10 | Epoch Time: 0m 23s\n",
      "\tTrain Loss: 0.017 | Train f1: 0.92 | Train precision: 0.93 | Train recall: 0.92\n",
      "\t Val. Loss: 0.298 |  Val. f1: 0.60 |  Val. precision: 0.66 | Val. recall: 0.62\n",
      "Epoch: 11 | Epoch Time: 0m 23s\n",
      "\tTrain Loss: 0.015 | Train f1: 0.93 | Train precision: 0.94 | Train recall: 0.94\n",
      "\t Val. Loss: 0.274 |  Val. f1: 0.62 |  Val. precision: 0.67 | Val. recall: 0.61\n",
      "Epoch: 12 | Epoch Time: 0m 23s\n",
      "\tTrain Loss: 0.014 | Train f1: 0.94 | Train precision: 0.95 | Train recall: 0.94\n",
      "\t Val. Loss: 0.293 |  Val. f1: 0.62 |  Val. precision: 0.67 | Val. recall: 0.62\n",
      "Epoch: 13 | Epoch Time: 0m 23s\n",
      "\tTrain Loss: 0.013 | Train f1: 0.94 | Train precision: 0.95 | Train recall: 0.94\n",
      "\t Val. Loss: 0.304 |  Val. f1: 0.61 |  Val. precision: 0.67 | Val. recall: 0.60\n",
      "\n",
      "Performance of best found Model:\n",
      "Val. Loss: 0.202 |  Val. f1: 0.59 | Val. precision: 0.65 | Val. recall: 0.58\n"
     ]
    }
   ],
   "source": [
    "model = gru_model\n",
    "model_name = gru_model_name\n",
    "criterion = baseline_criterion\n",
    "  \n",
    "model.apply(init_weights)\n",
    "print(f'El modelo actual tiene {count_parameters(model):,} parámetros entrenables.')\n",
    "\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "model = model.to(device)\n",
    "criterion = criterion.to(device)\n",
    "\n",
    "optimize_model(model, train_iterator, valid_iterator, optimizer, criterion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "n87VQzRernCf"
   },
   "source": [
    "En promedio, al aumentar o disminuir la profundidad de los modelos recurrentes se incurre en una pérdida general de *performance* y aún en los casos en que alguna métrica mejora las pérdidas en las otras son mayores en magnitud."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HvvcfJKCr770"
   },
   "source": [
    "#### Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "y4-qZSRUr-AQ"
   },
   "source": [
    "Para analizar el efecto del *dropout* cabe hacer antes la siguiente observación: en los modelos provistos se usa un mismo *dropout* en cada capa de la red, entendiendo por esto a que entre la capa de *Embedding* y el modelo recurrente se incorpora una capa de *dropout*, la cual posee el mismo parámetro que se utiliza dentro de la red recurrente y, a su vez, entre la salida de esta y la red de clasificación se incorpora otra capa de *dropout* con igual parámetro. Esto no es usual en la práctica por lo que se propone separar estas tres instancias de *dropout* y experimentar con ellas individualmente.\n",
    "\n",
    "En virtud de lo anterior, se prueban primero los modelos sin *dropout* y se varía cada componente en forma individual en busca de la configuración óptima.\n",
    "\n",
    "Pero antes, es necesario modificar las clases de los modelos estudiados hasta ahora:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xymfKg5Ot6FF"
   },
   "outputs": [],
   "source": [
    "class NER_RNN(nn.Module):\n",
    "    def __init__(self, \n",
    "                 input_dim, \n",
    "                 embedding_dim, \n",
    "                 hidden_dim, \n",
    "                 output_dim,\n",
    "                 n_layers, \n",
    "                 bidirectional, \n",
    "                 in_dropout,\n",
    "                 rnn_dropout,\n",
    "                 out_dropout, \n",
    "                 pad_idx):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        # Capa de embedding\n",
    "        self.embedding = nn.Embedding(input_dim,\n",
    "                                      embedding_dim,\n",
    "                                      padding_idx=pad_idx)\n",
    "\n",
    "        # Capa LSTM\n",
    "        self.lstm = nn.LSTM(embedding_dim,\n",
    "                           hidden_dim,\n",
    "                           num_layers=n_layers,\n",
    "                           bidirectional=bidirectional, \n",
    "                           dropout = rnn_dropout if n_layers > 1 else 0)\n",
    "\n",
    "        # Capa de salida\n",
    "        self.fc = nn.Linear(hidden_dim * 2 if bidirectional else hidden_dim,\n",
    "                            output_dim)\n",
    "\n",
    "        # Dropout\n",
    "        self.in_dropout = nn.Dropout(in_dropout)\n",
    "        self.out_dropout = nn.Dropout(out_dropout)\n",
    "\n",
    "    def forward(self, text):\n",
    "\n",
    "        #text = [sent len, batch size]\n",
    "\n",
    "        # Convertir lo enviado a embedding\n",
    "        embedded = self.in_dropout(self.embedding(text))\n",
    "        \n",
    "        outputs, (hidden, cell) = self.lstm(embedded)\n",
    "        #embedded = [sent len, batch size, emb dim]\n",
    "\n",
    "        # Pasar los embeddings por la rnn (LSTM)\n",
    "\n",
    "        #output = [sent len, batch size, hid dim * n directions]\n",
    "        #hidden/cell = [n layers * n directions, batch size, hid dim]\n",
    "\n",
    "        # Predecir usando la capa de salida.\n",
    "        predictions = self.fc(self.out_dropout(outputs))\n",
    "        #predictions = [sent len, batch size, output dim]\n",
    "\n",
    "        return predictions\n",
    "\n",
    "class NER_ELMAN(nn.Module):\n",
    "    def __init__(self, \n",
    "                 input_dim, \n",
    "                 embedding_dim, \n",
    "                 hidden_dim, \n",
    "                 output_dim,\n",
    "                 n_layers, \n",
    "                 bidirectional, \n",
    "                 in_dropout,\n",
    "                 rnn_dropout,\n",
    "                 out_dropout, \n",
    "                 pad_idx,\n",
    "                 nonlinearity = 'tanh'):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        # Capa de embedding\n",
    "        self.embedding = nn.Embedding(input_dim,\n",
    "                                      embedding_dim,\n",
    "                                      padding_idx=pad_idx)\n",
    "\n",
    "        # Capa Elman RNN\n",
    "        self.rnn = nn.RNN(embedding_dim,\n",
    "                           hidden_dim,\n",
    "                           num_layers=n_layers,\n",
    "                           bidirectional=bidirectional, \n",
    "                           dropout = rnn_dropout if n_layers > 1 else 0,\n",
    "                           nonlinearity = nonlinearity)\n",
    "\n",
    "        # Capa de salida\n",
    "        self.fc = nn.Linear(hidden_dim * 2 if bidirectional else hidden_dim,\n",
    "                            output_dim)\n",
    "\n",
    "        # Dropout\n",
    "        self.in_dropout = nn.Dropout(in_dropout)\n",
    "        self.out_dropout = nn.Dropout(out_dropout)\n",
    "\n",
    "    def forward(self, text):\n",
    "\n",
    "        #text = [sent len, batch size]\n",
    "\n",
    "        # Convertir lo enviado a embedding\n",
    "        embedded = self.in_dropout(self.embedding(text))\n",
    "        \n",
    "        outputs, hidden = self.rnn(embedded)\n",
    "        #embedded = [sent len, batch size, emb dim]\n",
    "\n",
    "        # Pasar los embeddings por la rnn (LSTM)\n",
    "\n",
    "        #output = [sent len, batch size, hid dim * n directions]\n",
    "        #hidden/cell = [n layers * n directions, batch size, hid dim]\n",
    "\n",
    "        # Predecir usando la capa de salida.\n",
    "        predictions = self.fc(self.out_dropout(outputs))\n",
    "        #predictions = [sent len, batch size, output dim]\n",
    "\n",
    "        return predictions\n",
    "\n",
    "class NER_GRU(nn.Module):\n",
    "    def __init__(self, \n",
    "                 input_dim, \n",
    "                 embedding_dim, \n",
    "                 hidden_dim, \n",
    "                 output_dim,\n",
    "                 n_layers, \n",
    "                 bidirectional, \n",
    "                 in_dropout,\n",
    "                 rnn_dropout,\n",
    "                 out_dropout, \n",
    "                 pad_idx):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        # Capa de embedding\n",
    "        self.embedding = nn.Embedding(input_dim,\n",
    "                                      embedding_dim,\n",
    "                                      padding_idx=pad_idx)\n",
    "\n",
    "        # Capa Elman RNN\n",
    "        self.gru = nn.GRU(embedding_dim,\n",
    "                           hidden_dim,\n",
    "                           num_layers=n_layers,\n",
    "                           bidirectional=bidirectional, \n",
    "                           dropout = rnn_dropout if n_layers > 1 else 0)\n",
    "\n",
    "        # Capa de salida\n",
    "        self.fc = nn.Linear(hidden_dim * 2 if bidirectional else hidden_dim,\n",
    "                            output_dim)\n",
    "\n",
    "        # Dropout\n",
    "        self.in_dropout = nn.Dropout(in_dropout)\n",
    "        self.out_dropout = nn.Dropout(out_dropout)\n",
    "\n",
    "    def forward(self, text):\n",
    "\n",
    "        #text = [sent len, batch size]\n",
    "\n",
    "        # Convertir lo enviado a embedding\n",
    "        embedded = self.in_dropout(self.embedding(text))\n",
    "        \n",
    "        outputs, hidden = self.gru(embedded)\n",
    "        #embedded = [sent len, batch size, emb dim]\n",
    "\n",
    "        # Pasar los embeddings por la rnn (LSTM)\n",
    "\n",
    "        #output = [sent len, batch size, hid dim * n directions]\n",
    "        #hidden/cell = [n layers * n directions, batch size, hid dim]\n",
    "\n",
    "        # Predecir usando la capa de salida.\n",
    "        predictions = self.fc(self.out_dropout(outputs))\n",
    "        #predictions = [sent len, batch size, output dim]\n",
    "\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7dWmph7fvG_n"
   },
   "source": [
    "##### Input *Dropout*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ADWrKtCivKAp"
   },
   "outputs": [],
   "source": [
    "## Parámetros del Experimento ##\n",
    "\n",
    "# tamaño del vocabulario. recuerden que la entrada son vectores bag of word(one-hot).\n",
    "INPUT_DIM = len(TEXT.vocab)\n",
    "EMBEDDING_DIM = 100  # dimensión de los embeddings.\n",
    "HIDDEN_DIM = 256  # dimensión de la capas RNN\n",
    "OUTPUT_DIM = len(NER_TAGS.vocab)  # número de clases\n",
    "\n",
    "N_LAYERS = 2  # número de capas.\n",
    "#IN_DROPOUT = 0.0\n",
    "#IN_DROPOUT = 0.1\n",
    "#IN_DROPOUT = 0.25\n",
    "#IN_DROPOUT = 0.5\n",
    "IN_DROPOUT = 0.75\n",
    "RNN_DROPOUT = 0.0\n",
    "OUT_DROPOUT = 0.0\n",
    "BIDIRECTIONAL = True\n",
    "\n",
    "TAG_PAD_IDX = NER_TAGS.vocab.stoi[NER_TAGS.pad_token]\n",
    "baseline_criterion = nn.CrossEntropyLoss(ignore_index = TAG_PAD_IDX)\n",
    "\n",
    "# Creamos nuestros modelos.\n",
    "lstm_model = NER_RNN(INPUT_DIM, EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM,\n",
    "                     N_LAYERS, BIDIRECTIONAL, IN_DROPOUT, RNN_DROPOUT, \n",
    "                     OUT_DROPOUT, PAD_IDX)\n",
    "\n",
    "lstm_model_name = 'LSTM'  # nombre que tendrá el modelo guardado...\n",
    "\n",
    "elman_model = NER_ELMAN(INPUT_DIM, EMBEDDING_DIM, int(HIDDEN_DIM/2), OUTPUT_DIM,\n",
    "                        N_LAYERS, BIDIRECTIONAL, IN_DROPOUT, RNN_DROPOUT, \n",
    "                        OUT_DROPOUT, PAD_IDX, 'relu')\n",
    "\n",
    "elman_model_name = 'Elman'  # nombre que tendrá el modelo guardado...\n",
    "\n",
    "gru_model = NER_GRU(INPUT_DIM, EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM,\n",
    "                    N_LAYERS, BIDIRECTIONAL, IN_DROPOUT, RNN_DROPOUT, \n",
    "                    OUT_DROPOUT, PAD_IDX)\n",
    "\n",
    "gru_model_name = 'GRU'  # nombre que tendrá el modelo guardado..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 901
    },
    "colab_type": "code",
    "id": "pzho2-2hvskt",
    "outputId": "3698d7de-5f0f-4eaa-fe96-4c463662230d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El modelo actual tiene 4,925,374 parámetros entrenables.\n",
      "Epoch: 01 | Epoch Time: 0m 9s\n",
      "\tTrain Loss: 0.429 | Train f1: 0.18 | Train precision: 0.29 | Train recall: 0.15\n",
      "\t Val. Loss: 0.305 |  Val. f1: 0.36 |  Val. precision: 0.48 | Val. recall: 0.32\n",
      "Epoch: 02 | Epoch Time: 0m 8s\n",
      "\tTrain Loss: 0.231 | Train f1: 0.43 | Train precision: 0.52 | Train recall: 0.40\n",
      "\t Val. Loss: 0.227 |  Val. f1: 0.48 |  Val. precision: 0.57 | Val. recall: 0.46\n",
      "Epoch: 03 | Epoch Time: 0m 8s\n",
      "\tTrain Loss: 0.166 | Train f1: 0.54 | Train precision: 0.62 | Train recall: 0.52\n",
      "\t Val. Loss: 0.207 |  Val. f1: 0.54 |  Val. precision: 0.63 | Val. recall: 0.52\n",
      "Epoch: 04 | Epoch Time: 0m 8s\n",
      "\tTrain Loss: 0.127 | Train f1: 0.61 | Train precision: 0.67 | Train recall: 0.60\n",
      "\t Val. Loss: 0.183 |  Val. f1: 0.60 |  Val. precision: 0.66 | Val. recall: 0.60\n",
      "Epoch: 05 | Epoch Time: 0m 8s\n",
      "\tTrain Loss: 0.102 | Train f1: 0.66 | Train precision: 0.71 | Train recall: 0.65\n",
      "\t Val. Loss: 0.177 |  Val. f1: 0.60 |  Val. precision: 0.66 | Val. recall: 0.60\n",
      "Epoch: 06 | Epoch Time: 0m 8s\n",
      "\tTrain Loss: 0.086 | Train f1: 0.70 | Train precision: 0.74 | Train recall: 0.69\n",
      "\t Val. Loss: 0.161 |  Val. f1: 0.64 |  Val. precision: 0.69 | Val. recall: 0.63\n",
      "Epoch: 07 | Epoch Time: 0m 8s\n",
      "\tTrain Loss: 0.071 | Train f1: 0.74 | Train precision: 0.78 | Train recall: 0.73\n",
      "\t Val. Loss: 0.170 |  Val. f1: 0.62 |  Val. precision: 0.68 | Val. recall: 0.62\n",
      "Epoch: 08 | Epoch Time: 0m 8s\n",
      "\tTrain Loss: 0.062 | Train f1: 0.76 | Train precision: 0.79 | Train recall: 0.76\n",
      "\t Val. Loss: 0.166 |  Val. f1: 0.62 |  Val. precision: 0.68 | Val. recall: 0.62\n",
      "Epoch: 09 | Epoch Time: 0m 9s\n",
      "\tTrain Loss: 0.053 | Train f1: 0.78 | Train precision: 0.81 | Train recall: 0.78\n",
      "\t Val. Loss: 0.171 |  Val. f1: 0.65 |  Val. precision: 0.70 | Val. recall: 0.65\n",
      "Epoch: 10 | Epoch Time: 0m 9s\n",
      "\tTrain Loss: 0.048 | Train f1: 0.80 | Train precision: 0.83 | Train recall: 0.80\n",
      "\t Val. Loss: 0.177 |  Val. f1: 0.65 |  Val. precision: 0.70 | Val. recall: 0.65\n",
      "Epoch: 11 | Epoch Time: 0m 8s\n",
      "\tTrain Loss: 0.043 | Train f1: 0.81 | Train precision: 0.84 | Train recall: 0.81\n",
      "\t Val. Loss: 0.182 |  Val. f1: 0.64 |  Val. precision: 0.69 | Val. recall: 0.64\n",
      "Epoch: 12 | Epoch Time: 0m 8s\n",
      "\tTrain Loss: 0.035 | Train f1: 0.84 | Train precision: 0.86 | Train recall: 0.84\n",
      "\t Val. Loss: 0.174 |  Val. f1: 0.65 |  Val. precision: 0.70 | Val. recall: 0.66\n",
      "Epoch: 13 | Epoch Time: 0m 9s\n",
      "\tTrain Loss: 0.034 | Train f1: 0.84 | Train precision: 0.86 | Train recall: 0.84\n",
      "\t Val. Loss: 0.188 |  Val. f1: 0.64 |  Val. precision: 0.70 | Val. recall: 0.64\n",
      "Epoch: 14 | Epoch Time: 0m 8s\n",
      "\tTrain Loss: 0.029 | Train f1: 0.86 | Train precision: 0.87 | Train recall: 0.86\n",
      "\t Val. Loss: 0.193 |  Val. f1: 0.64 |  Val. precision: 0.68 | Val. recall: 0.64\n",
      "Epoch: 15 | Epoch Time: 0m 9s\n",
      "\tTrain Loss: 0.027 | Train f1: 0.87 | Train precision: 0.88 | Train recall: 0.87\n",
      "\t Val. Loss: 0.201 |  Val. f1: 0.63 |  Val. precision: 0.69 | Val. recall: 0.64\n",
      "Epoch: 16 | Epoch Time: 0m 8s\n",
      "\tTrain Loss: 0.027 | Train f1: 0.87 | Train precision: 0.88 | Train recall: 0.87\n",
      "\t Val. Loss: 0.192 |  Val. f1: 0.65 |  Val. precision: 0.69 | Val. recall: 0.66\n",
      "\n",
      "Performance of best found Model:\n",
      "Val. Loss: 0.161 |  Val. f1: 0.64 | Val. precision: 0.69 | Val. recall: 0.63\n"
     ]
    }
   ],
   "source": [
    "model = lstm_model\n",
    "model_name = lstm_model_name\n",
    "criterion = baseline_criterion\n",
    "\n",
    "model.apply(init_weights)\n",
    "print(f'El modelo actual tiene {count_parameters(model):,} parámetros entrenables.')\n",
    "\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "model = model.to(device)\n",
    "criterion = criterion.to(device)\n",
    "\n",
    "optimize_model(model, train_iterator, valid_iterator, optimizer, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 952
    },
    "colab_type": "code",
    "id": "wM1E6NRJvwBH",
    "outputId": "68750d9e-abe7-40e4-8738-ae0462c46f8a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El modelo actual tiene 2,770,366 parámetros entrenables.\n",
      "Epoch: 01 | Epoch Time: 0m 7s\n",
      "\tTrain Loss: 0.473 | Train f1: 0.12 | Train precision: 0.23 | Train recall: 0.09\n",
      "\t Val. Loss: 0.340 |  Val. f1: 0.34 |  Val. precision: 0.45 | Val. recall: 0.31\n",
      "Epoch: 02 | Epoch Time: 0m 7s\n",
      "\tTrain Loss: 0.267 | Train f1: 0.37 | Train precision: 0.46 | Train recall: 0.34\n",
      "\t Val. Loss: 0.271 |  Val. f1: 0.46 |  Val. precision: 0.52 | Val. recall: 0.47\n",
      "Epoch: 03 | Epoch Time: 0m 7s\n",
      "\tTrain Loss: 0.195 | Train f1: 0.48 | Train precision: 0.57 | Train recall: 0.46\n",
      "\t Val. Loss: 0.214 |  Val. f1: 0.52 |  Val. precision: 0.58 | Val. recall: 0.53\n",
      "Epoch: 04 | Epoch Time: 0m 7s\n",
      "\tTrain Loss: 0.151 | Train f1: 0.56 | Train precision: 0.63 | Train recall: 0.54\n",
      "\t Val. Loss: 0.198 |  Val. f1: 0.57 |  Val. precision: 0.62 | Val. recall: 0.59\n",
      "Epoch: 05 | Epoch Time: 0m 7s\n",
      "\tTrain Loss: 0.124 | Train f1: 0.62 | Train precision: 0.68 | Train recall: 0.61\n",
      "\t Val. Loss: 0.186 |  Val. f1: 0.59 |  Val. precision: 0.64 | Val. recall: 0.60\n",
      "Epoch: 06 | Epoch Time: 0m 7s\n",
      "\tTrain Loss: 0.107 | Train f1: 0.65 | Train precision: 0.71 | Train recall: 0.65\n",
      "\t Val. Loss: 0.179 |  Val. f1: 0.60 |  Val. precision: 0.65 | Val. recall: 0.62\n",
      "Epoch: 07 | Epoch Time: 0m 7s\n",
      "\tTrain Loss: 0.091 | Train f1: 0.69 | Train precision: 0.73 | Train recall: 0.68\n",
      "\t Val. Loss: 0.180 |  Val. f1: 0.62 |  Val. precision: 0.66 | Val. recall: 0.64\n",
      "Epoch: 08 | Epoch Time: 0m 7s\n",
      "\tTrain Loss: 0.080 | Train f1: 0.71 | Train precision: 0.75 | Train recall: 0.71\n",
      "\t Val. Loss: 0.160 |  Val. f1: 0.63 |  Val. precision: 0.68 | Val. recall: 0.63\n",
      "Epoch: 09 | Epoch Time: 0m 7s\n",
      "\tTrain Loss: 0.072 | Train f1: 0.74 | Train precision: 0.77 | Train recall: 0.73\n",
      "\t Val. Loss: 0.166 |  Val. f1: 0.64 |  Val. precision: 0.68 | Val. recall: 0.65\n",
      "Epoch: 10 | Epoch Time: 0m 7s\n",
      "\tTrain Loss: 0.065 | Train f1: 0.76 | Train precision: 0.79 | Train recall: 0.75\n",
      "\t Val. Loss: 0.179 |  Val. f1: 0.63 |  Val. precision: 0.68 | Val. recall: 0.63\n",
      "Epoch: 11 | Epoch Time: 0m 7s\n",
      "\tTrain Loss: 0.058 | Train f1: 0.77 | Train precision: 0.80 | Train recall: 0.77\n",
      "\t Val. Loss: 0.165 |  Val. f1: 0.64 |  Val. precision: 0.68 | Val. recall: 0.65\n",
      "Epoch: 12 | Epoch Time: 0m 7s\n",
      "\tTrain Loss: 0.054 | Train f1: 0.79 | Train precision: 0.81 | Train recall: 0.78\n",
      "\t Val. Loss: 0.171 |  Val. f1: 0.63 |  Val. precision: 0.67 | Val. recall: 0.64\n",
      "Epoch: 13 | Epoch Time: 0m 7s\n",
      "\tTrain Loss: 0.049 | Train f1: 0.80 | Train precision: 0.82 | Train recall: 0.80\n",
      "\t Val. Loss: 0.170 |  Val. f1: 0.65 |  Val. precision: 0.69 | Val. recall: 0.66\n",
      "Epoch: 14 | Epoch Time: 0m 7s\n",
      "\tTrain Loss: 0.046 | Train f1: 0.80 | Train precision: 0.83 | Train recall: 0.80\n",
      "\t Val. Loss: 0.184 |  Val. f1: 0.64 |  Val. precision: 0.67 | Val. recall: 0.65\n",
      "Epoch: 15 | Epoch Time: 0m 7s\n",
      "\tTrain Loss: 0.043 | Train f1: 0.82 | Train precision: 0.84 | Train recall: 0.82\n",
      "\t Val. Loss: 0.178 |  Val. f1: 0.65 |  Val. precision: 0.68 | Val. recall: 0.67\n",
      "Epoch: 16 | Epoch Time: 0m 7s\n",
      "\tTrain Loss: 0.040 | Train f1: 0.82 | Train precision: 0.84 | Train recall: 0.82\n",
      "\t Val. Loss: 0.165 |  Val. f1: 0.65 |  Val. precision: 0.69 | Val. recall: 0.67\n",
      "Epoch: 17 | Epoch Time: 0m 7s\n",
      "\tTrain Loss: 0.037 | Train f1: 0.84 | Train precision: 0.85 | Train recall: 0.83\n",
      "\t Val. Loss: 0.181 |  Val. f1: 0.66 |  Val. precision: 0.70 | Val. recall: 0.67\n",
      "\n",
      "Performance of best found Model:\n",
      "Val. Loss: 0.160 |  Val. f1: 0.63 | Val. precision: 0.68 | Val. recall: 0.63\n"
     ]
    }
   ],
   "source": [
    "model = elman_model\n",
    "model_name = elman_model_name\n",
    "criterion = baseline_criterion\n",
    "\n",
    "model.apply(init_weights)\n",
    "print(f'El modelo actual tiene {count_parameters(model):,} parámetros entrenables.')\n",
    "\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "model = model.to(device)\n",
    "criterion = criterion.to(device)\n",
    "\n",
    "optimize_model(model, train_iterator, valid_iterator, optimizer, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 952
    },
    "colab_type": "code",
    "id": "WjHztAaev083",
    "outputId": "1ab66566-4631-4edc-fa83-e7c4542b9819"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El modelo actual tiene 4,347,838 parámetros entrenables.\n",
      "Epoch: 01 | Epoch Time: 0m 8s\n",
      "\tTrain Loss: 0.451 | Train f1: 0.17 | Train precision: 0.31 | Train recall: 0.14\n",
      "\t Val. Loss: 0.309 |  Val. f1: 0.38 |  Val. precision: 0.49 | Val. recall: 0.36\n",
      "Epoch: 02 | Epoch Time: 0m 8s\n",
      "\tTrain Loss: 0.247 | Train f1: 0.43 | Train precision: 0.55 | Train recall: 0.39\n",
      "\t Val. Loss: 0.226 |  Val. f1: 0.50 |  Val. precision: 0.58 | Val. recall: 0.49\n",
      "Epoch: 03 | Epoch Time: 0m 8s\n",
      "\tTrain Loss: 0.171 | Train f1: 0.55 | Train precision: 0.65 | Train recall: 0.53\n",
      "\t Val. Loss: 0.217 |  Val. f1: 0.53 |  Val. precision: 0.61 | Val. recall: 0.53\n",
      "Epoch: 04 | Epoch Time: 0m 8s\n",
      "\tTrain Loss: 0.127 | Train f1: 0.63 | Train precision: 0.70 | Train recall: 0.61\n",
      "\t Val. Loss: 0.179 |  Val. f1: 0.58 |  Val. precision: 0.66 | Val. recall: 0.57\n",
      "Epoch: 05 | Epoch Time: 0m 8s\n",
      "\tTrain Loss: 0.104 | Train f1: 0.67 | Train precision: 0.73 | Train recall: 0.66\n",
      "\t Val. Loss: 0.172 |  Val. f1: 0.61 |  Val. precision: 0.68 | Val. recall: 0.60\n",
      "Epoch: 06 | Epoch Time: 0m 8s\n",
      "\tTrain Loss: 0.084 | Train f1: 0.72 | Train precision: 0.76 | Train recall: 0.71\n",
      "\t Val. Loss: 0.167 |  Val. f1: 0.62 |  Val. precision: 0.69 | Val. recall: 0.62\n",
      "Epoch: 07 | Epoch Time: 0m 8s\n",
      "\tTrain Loss: 0.071 | Train f1: 0.74 | Train precision: 0.78 | Train recall: 0.73\n",
      "\t Val. Loss: 0.166 |  Val. f1: 0.63 |  Val. precision: 0.69 | Val. recall: 0.63\n",
      "Epoch: 08 | Epoch Time: 0m 8s\n",
      "\tTrain Loss: 0.060 | Train f1: 0.77 | Train precision: 0.80 | Train recall: 0.77\n",
      "\t Val. Loss: 0.175 |  Val. f1: 0.64 |  Val. precision: 0.69 | Val. recall: 0.64\n",
      "Epoch: 09 | Epoch Time: 0m 8s\n",
      "\tTrain Loss: 0.054 | Train f1: 0.78 | Train precision: 0.81 | Train recall: 0.78\n",
      "\t Val. Loss: 0.177 |  Val. f1: 0.66 |  Val. precision: 0.71 | Val. recall: 0.66\n",
      "Epoch: 10 | Epoch Time: 0m 8s\n",
      "\tTrain Loss: 0.047 | Train f1: 0.81 | Train precision: 0.84 | Train recall: 0.81\n",
      "\t Val. Loss: 0.172 |  Val. f1: 0.64 |  Val. precision: 0.70 | Val. recall: 0.64\n",
      "Epoch: 11 | Epoch Time: 0m 8s\n",
      "\tTrain Loss: 0.042 | Train f1: 0.82 | Train precision: 0.84 | Train recall: 0.81\n",
      "\t Val. Loss: 0.176 |  Val. f1: 0.65 |  Val. precision: 0.70 | Val. recall: 0.65\n",
      "Epoch: 12 | Epoch Time: 0m 8s\n",
      "\tTrain Loss: 0.037 | Train f1: 0.83 | Train precision: 0.85 | Train recall: 0.83\n",
      "\t Val. Loss: 0.185 |  Val. f1: 0.65 |  Val. precision: 0.70 | Val. recall: 0.64\n",
      "Epoch: 13 | Epoch Time: 0m 8s\n",
      "\tTrain Loss: 0.034 | Train f1: 0.84 | Train precision: 0.86 | Train recall: 0.84\n",
      "\t Val. Loss: 0.204 |  Val. f1: 0.64 |  Val. precision: 0.70 | Val. recall: 0.64\n",
      "Epoch: 14 | Epoch Time: 0m 8s\n",
      "\tTrain Loss: 0.032 | Train f1: 0.85 | Train precision: 0.87 | Train recall: 0.85\n",
      "\t Val. Loss: 0.195 |  Val. f1: 0.65 |  Val. precision: 0.71 | Val. recall: 0.65\n",
      "Epoch: 15 | Epoch Time: 0m 8s\n",
      "\tTrain Loss: 0.030 | Train f1: 0.86 | Train precision: 0.88 | Train recall: 0.86\n",
      "\t Val. Loss: 0.209 |  Val. f1: 0.65 |  Val. precision: 0.70 | Val. recall: 0.65\n",
      "Epoch: 16 | Epoch Time: 0m 8s\n",
      "\tTrain Loss: 0.028 | Train f1: 0.87 | Train precision: 0.88 | Train recall: 0.87\n",
      "\t Val. Loss: 0.200 |  Val. f1: 0.66 |  Val. precision: 0.72 | Val. recall: 0.65\n",
      "Epoch: 17 | Epoch Time: 0m 8s\n",
      "\tTrain Loss: 0.025 | Train f1: 0.88 | Train precision: 0.89 | Train recall: 0.87\n",
      "\t Val. Loss: 0.204 |  Val. f1: 0.66 |  Val. precision: 0.70 | Val. recall: 0.66\n",
      "\n",
      "Performance of best found Model:\n",
      "Val. Loss: 0.166 |  Val. f1: 0.63 | Val. precision: 0.69 | Val. recall: 0.63\n"
     ]
    }
   ],
   "source": [
    "model = gru_model\n",
    "model_name = gru_model_name\n",
    "criterion = baseline_criterion\n",
    "\n",
    "model.apply(init_weights)\n",
    "print(f'El modelo actual tiene {count_parameters(model):,} parámetros entrenables.')\n",
    "\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "model = model.to(device)\n",
    "criterion = criterion.to(device)\n",
    "\n",
    "optimize_model(model, train_iterator, valid_iterator, optimizer, criterion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uQ6vSBpUu2aY"
   },
   "source": [
    "Aparentemente, los mejores resultados se obtienen al incorporar un *dropout* de `0.75` en la entrada del modelo recurrente. Estudiemos acontinuación el efecto del *dropout* dentro del mismo modelo:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rPEuBounvOrK"
   },
   "source": [
    "##### RNN Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "C-2t90B-vVaX"
   },
   "outputs": [],
   "source": [
    "## Parámetros del Experimento ##\n",
    "\n",
    "# tamaño del vocabulario. recuerden que la entrada son vectores bag of word(one-hot).\n",
    "INPUT_DIM = len(TEXT.vocab)\n",
    "EMBEDDING_DIM = 100  # dimensión de los embeddings.\n",
    "HIDDEN_DIM = 256  # dimensión de la capas RNN\n",
    "OUTPUT_DIM = len(NER_TAGS.vocab)  # número de clases\n",
    "\n",
    "N_LAYERS = 2  # número de capas.\n",
    "IN_DROPOUT = 0.75\n",
    "#RNN_DROPOUT = 0.0\n",
    "#RNN_DROPOUT = 0.1\n",
    "#RNN_DROPOUT = 0.25\n",
    "#RNN_DROPOUT = 0.5\n",
    "RNN_DROPOUT = 0.75\n",
    "OUT_DROPOUT = 0.0\n",
    "BIDIRECTIONAL = True\n",
    "\n",
    "TAG_PAD_IDX = NER_TAGS.vocab.stoi[NER_TAGS.pad_token]\n",
    "baseline_criterion = nn.CrossEntropyLoss(ignore_index = TAG_PAD_IDX)\n",
    "\n",
    "# Creamos nuestros modelos.\n",
    "lstm_model = NER_RNN(INPUT_DIM, EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM,\n",
    "                     N_LAYERS, BIDIRECTIONAL, IN_DROPOUT, RNN_DROPOUT, \n",
    "                     OUT_DROPOUT, PAD_IDX)\n",
    "\n",
    "lstm_model_name = 'LSTM'  # nombre que tendrá el modelo guardado...\n",
    "\n",
    "elman_model = NER_ELMAN(INPUT_DIM, EMBEDDING_DIM, int(HIDDEN_DIM/2), OUTPUT_DIM,\n",
    "                        N_LAYERS, BIDIRECTIONAL, IN_DROPOUT, RNN_DROPOUT, \n",
    "                        OUT_DROPOUT, PAD_IDX, 'relu')\n",
    "\n",
    "elman_model_name = 'Elman'  # nombre que tendrá el modelo guardado...\n",
    "\n",
    "gru_model = NER_GRU(INPUT_DIM, EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM,\n",
    "                    N_LAYERS, BIDIRECTIONAL, IN_DROPOUT, RNN_DROPOUT, \n",
    "                    OUT_DROPOUT, PAD_IDX)\n",
    "\n",
    "gru_model_name = 'GRU'  # nombre que tendrá el modelo guardado..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "uxoauLdyvjgg",
    "outputId": "e0d48a31-62dc-4ba6-ddb1-9faff438715c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El modelo actual tiene 4,925,374 parámetros entrenables.\n",
      "Epoch: 01 | Epoch Time: 0m 11s\n",
      "\tTrain Loss: 0.473 | Train f1: 0.12 | Train precision: 0.22 | Train recall: 0.10\n",
      "\t Val. Loss: 0.338 |  Val. f1: 0.34 |  Val. precision: 0.48 | Val. recall: 0.30\n",
      "Epoch: 02 | Epoch Time: 0m 11s\n",
      "\tTrain Loss: 0.258 | Train f1: 0.38 | Train precision: 0.48 | Train recall: 0.35\n",
      "\t Val. Loss: 0.240 |  Val. f1: 0.46 |  Val. precision: 0.56 | Val. recall: 0.45\n",
      "Epoch: 03 | Epoch Time: 0m 11s\n",
      "\tTrain Loss: 0.185 | Train f1: 0.50 | Train precision: 0.58 | Train recall: 0.48\n",
      "\t Val. Loss: 0.209 |  Val. f1: 0.53 |  Val. precision: 0.60 | Val. recall: 0.54\n",
      "Epoch: 04 | Epoch Time: 0m 11s\n",
      "\tTrain Loss: 0.143 | Train f1: 0.58 | Train precision: 0.64 | Train recall: 0.56\n",
      "\t Val. Loss: 0.189 |  Val. f1: 0.57 |  Val. precision: 0.65 | Val. recall: 0.57\n",
      "Epoch: 05 | Epoch Time: 0m 11s\n",
      "\tTrain Loss: 0.116 | Train f1: 0.64 | Train precision: 0.69 | Train recall: 0.63\n",
      "\t Val. Loss: 0.206 |  Val. f1: 0.58 |  Val. precision: 0.64 | Val. recall: 0.59\n",
      "Epoch: 06 | Epoch Time: 0m 11s\n",
      "\tTrain Loss: 0.099 | Train f1: 0.67 | Train precision: 0.71 | Train recall: 0.66\n",
      "\t Val. Loss: 0.177 |  Val. f1: 0.61 |  Val. precision: 0.67 | Val. recall: 0.61\n",
      "Epoch: 07 | Epoch Time: 0m 11s\n",
      "\tTrain Loss: 0.084 | Train f1: 0.71 | Train precision: 0.75 | Train recall: 0.70\n",
      "\t Val. Loss: 0.180 |  Val. f1: 0.61 |  Val. precision: 0.66 | Val. recall: 0.61\n",
      "Epoch: 08 | Epoch Time: 0m 11s\n",
      "\tTrain Loss: 0.074 | Train f1: 0.73 | Train precision: 0.77 | Train recall: 0.73\n",
      "\t Val. Loss: 0.163 |  Val. f1: 0.63 |  Val. precision: 0.68 | Val. recall: 0.63\n",
      "Epoch: 09 | Epoch Time: 0m 11s\n",
      "\tTrain Loss: 0.064 | Train f1: 0.75 | Train precision: 0.78 | Train recall: 0.75\n",
      "\t Val. Loss: 0.181 |  Val. f1: 0.62 |  Val. precision: 0.67 | Val. recall: 0.62\n",
      "Epoch: 10 | Epoch Time: 0m 11s\n",
      "\tTrain Loss: 0.057 | Train f1: 0.78 | Train precision: 0.80 | Train recall: 0.77\n",
      "\t Val. Loss: 0.173 |  Val. f1: 0.64 |  Val. precision: 0.69 | Val. recall: 0.64\n",
      "Epoch: 11 | Epoch Time: 0m 11s\n",
      "\tTrain Loss: 0.051 | Train f1: 0.79 | Train precision: 0.82 | Train recall: 0.79\n",
      "\t Val. Loss: 0.161 |  Val. f1: 0.66 |  Val. precision: 0.70 | Val. recall: 0.67\n",
      "Epoch: 12 | Epoch Time: 0m 11s\n",
      "\tTrain Loss: 0.048 | Train f1: 0.80 | Train precision: 0.82 | Train recall: 0.80\n",
      "\t Val. Loss: 0.198 |  Val. f1: 0.62 |  Val. precision: 0.68 | Val. recall: 0.63\n",
      "Epoch: 13 | Epoch Time: 0m 11s\n",
      "\tTrain Loss: 0.043 | Train f1: 0.81 | Train precision: 0.84 | Train recall: 0.81\n",
      "\t Val. Loss: 0.172 |  Val. f1: 0.66 |  Val. precision: 0.70 | Val. recall: 0.66\n",
      "Epoch: 14 | Epoch Time: 0m 11s\n",
      "\tTrain Loss: 0.040 | Train f1: 0.82 | Train precision: 0.84 | Train recall: 0.82\n",
      "\t Val. Loss: 0.172 |  Val. f1: 0.65 |  Val. precision: 0.70 | Val. recall: 0.65\n",
      "Epoch: 15 | Epoch Time: 0m 11s\n",
      "\tTrain Loss: 0.036 | Train f1: 0.84 | Train precision: 0.85 | Train recall: 0.83\n",
      "\t Val. Loss: 0.184 |  Val. f1: 0.65 |  Val. precision: 0.70 | Val. recall: 0.64\n",
      "Epoch: 16 | Epoch Time: 0m 11s\n",
      "\tTrain Loss: 0.034 | Train f1: 0.85 | Train precision: 0.87 | Train recall: 0.85\n",
      "\t Val. Loss: 0.193 |  Val. f1: 0.65 |  Val. precision: 0.70 | Val. recall: 0.65\n",
      "Epoch: 17 | Epoch Time: 0m 11s\n",
      "\tTrain Loss: 0.032 | Train f1: 0.85 | Train precision: 0.87 | Train recall: 0.85\n",
      "\t Val. Loss: 0.202 |  Val. f1: 0.63 |  Val. precision: 0.68 | Val. recall: 0.64\n",
      "Epoch: 18 | Epoch Time: 0m 11s\n",
      "\tTrain Loss: 0.030 | Train f1: 0.86 | Train precision: 0.88 | Train recall: 0.86\n",
      "\t Val. Loss: 0.196 |  Val. f1: 0.65 |  Val. precision: 0.71 | Val. recall: 0.65\n",
      "Epoch: 19 | Epoch Time: 0m 11s\n",
      "\tTrain Loss: 0.028 | Train f1: 0.87 | Train precision: 0.89 | Train recall: 0.87\n",
      "\t Val. Loss: 0.212 |  Val. f1: 0.64 |  Val. precision: 0.70 | Val. recall: 0.64\n",
      "Epoch: 20 | Epoch Time: 0m 11s\n",
      "\tTrain Loss: 0.027 | Train f1: 0.87 | Train precision: 0.89 | Train recall: 0.87\n",
      "\t Val. Loss: 0.204 |  Val. f1: 0.65 |  Val. precision: 0.69 | Val. recall: 0.65\n",
      "Epoch: 21 | Epoch Time: 0m 11s\n",
      "\tTrain Loss: 0.025 | Train f1: 0.88 | Train precision: 0.89 | Train recall: 0.88\n",
      "\t Val. Loss: 0.196 |  Val. f1: 0.66 |  Val. precision: 0.70 | Val. recall: 0.66\n",
      "\n",
      "Performance of best found Model:\n",
      "Val. Loss: 0.161 |  Val. f1: 0.66 | Val. precision: 0.70 | Val. recall: 0.67\n"
     ]
    }
   ],
   "source": [
    "model = lstm_model\n",
    "model_name = lstm_model_name\n",
    "criterion = baseline_criterion\n",
    "\n",
    "model.apply(init_weights)\n",
    "print(f'El modelo actual tiene {count_parameters(model):,} parámetros entrenables.')\n",
    "\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "model = model.to(device)\n",
    "criterion = criterion.to(device)\n",
    "\n",
    "optimize_model(model, train_iterator, valid_iterator, optimizer, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nvNnGwWaj_SE"
   },
   "outputs": [],
   "source": [
    "%%script false\n",
    "import numpy as np\n",
    "test_sentence1 = \"La pandemia de enfermedad por coronavirus de 2019-2020 es una pandemia derivada de la enfermedad por coronavirus iniciada en 2019 (COVID-19), ocasionada por el virus coronavirus 2 del síndrome respiratorio agudo grave (SARS-CoV-2). Se identificó por primera vez en diciembre de 2019 en la ciudad de Wuhan,​ capital de la provincia de Hubei, en la República Popular China\"\n",
    "test_sentence2 = 'Vardoc es el seudónimo de Nicolás Ignacio Liñán de Ariza Baquerizo, su canal es Vardoc1. Nicolás es uno de los Youtubers más prestigiosos de Chile, oriundo de la ciudad de Temuco, realiza gameplays y vlogs diarios en su canal. '\n",
    "test_sentences = [test_sentence1, test_sentence2]\n",
    "tokenized_sentence = TEXT.process(test_sentences)\n",
    "print(tokenized_sentence[0])\n",
    "input_ids = torch.tensor([tokenized_sentence]).cuda()\n",
    "print(input_ids)\n",
    "\n",
    "# tokenized_sentence = TEXT.tokenize(test_sentence)\n",
    "# print(tokenized_sentence.text)\n",
    "# input_ids = TEXT.numericalize(tokenized_sentence[0]).cuda()\n",
    "# print(input_ids)\n",
    "# with torch.no_grad():\n",
    "#     output = elman_model(tokenized_sentence)\n",
    "#     print(output.shape)\n",
    "# label_indices = np.argmax(output[0].to('cpu').numpy(), axis = -1)\n",
    "# #print(label_indices)\n",
    "# for i in tokenized_sentence[0]:\n",
    "#     print(TEXT.vocab.itos[i])\n",
    "\n",
    "\n",
    "# test_sentence = \"Vardoc es el seudónimo de Nicolás Ignacio Liñán de Ariza Baquerizo, su canal es Vardoc1. Nicolás es uno de los Youtubers más prestigiosos de Chile, oriundo de la ciudad de Temuco, realiza gameplays y vlogs diarios en su canal.\"\n",
    "# tokenized_sentence = bert_tokenizer.encode(test_sentence)\n",
    "# input_ids = torch.tensor([tokenized_sentence]).cuda()\n",
    "# with torch.no_grad():\n",
    "#     output = model(input_ids)\n",
    "# label_indices = np.argmax(output[0].to('cpu').numpy(), axis=2)\n",
    "# tokens = bert_tokenizer.convert_ids_to_tokens(input_ids.to('cpu').numpy()[0])\n",
    "# new_tokens, new_labels = [], []\n",
    "# for token, label_idx in zip(tokens, label_indices[0]):\n",
    "#     if token.startswith(\"##\"):\n",
    "#         new_tokens[-1] = new_tokens[-1] + token[2:]\n",
    "#     else:\n",
    "#         new_labels.append(tag_values[label_idx])\n",
    "#         new_tokens.append(token)\n",
    "# for token, label in zip(new_tokens, new_labels):\n",
    "#     print(\"{}\\t{}\".format(label, token))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "4ZEe35BkvooL",
    "outputId": "79f02c30-6514-41b6-a817-828241e47cf3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El modelo actual tiene 2,770,366 parámetros entrenables.\n",
      "Epoch: 01 | Epoch Time: 0m 8s\n",
      "\tTrain Loss: 0.563 | Train f1: 0.02 | Train precision: 0.07 | Train recall: 0.01\n",
      "\t Val. Loss: 0.588 |  Val. f1: 0.22 |  Val. precision: 0.31 | Val. recall: 0.21\n",
      "Epoch: 02 | Epoch Time: 0m 9s\n",
      "\tTrain Loss: 0.365 | Train f1: 0.21 | Train precision: 0.33 | Train recall: 0.18\n",
      "\t Val. Loss: 0.380 |  Val. f1: 0.34 |  Val. precision: 0.37 | Val. recall: 0.37\n",
      "Epoch: 03 | Epoch Time: 0m 8s\n",
      "\tTrain Loss: 0.269 | Train f1: 0.33 | Train precision: 0.40 | Train recall: 0.32\n",
      "\t Val. Loss: 0.316 |  Val. f1: 0.40 |  Val. precision: 0.47 | Val. recall: 0.43\n",
      "Epoch: 04 | Epoch Time: 0m 8s\n",
      "\tTrain Loss: 0.218 | Train f1: 0.41 | Train precision: 0.47 | Train recall: 0.39\n",
      "\t Val. Loss: 0.265 |  Val. f1: 0.47 |  Val. precision: 0.52 | Val. recall: 0.50\n",
      "Epoch: 05 | Epoch Time: 0m 8s\n",
      "\tTrain Loss: 0.183 | Train f1: 0.47 | Train precision: 0.55 | Train recall: 0.46\n",
      "\t Val. Loss: 0.272 |  Val. f1: 0.50 |  Val. precision: 0.56 | Val. recall: 0.54\n",
      "Epoch: 06 | Epoch Time: 0m 9s\n",
      "\tTrain Loss: 0.159 | Train f1: 0.52 | Train precision: 0.58 | Train recall: 0.51\n",
      "\t Val. Loss: 0.217 |  Val. f1: 0.53 |  Val. precision: 0.58 | Val. recall: 0.55\n",
      "Epoch: 07 | Epoch Time: 0m 8s\n",
      "\tTrain Loss: 0.138 | Train f1: 0.56 | Train precision: 0.62 | Train recall: 0.56\n",
      "\t Val. Loss: 0.200 |  Val. f1: 0.56 |  Val. precision: 0.60 | Val. recall: 0.58\n",
      "Epoch: 08 | Epoch Time: 0m 8s\n",
      "\tTrain Loss: 0.126 | Train f1: 0.60 | Train precision: 0.66 | Train recall: 0.60\n",
      "\t Val. Loss: 0.207 |  Val. f1: 0.58 |  Val. precision: 0.63 | Val. recall: 0.61\n",
      "Epoch: 09 | Epoch Time: 0m 8s\n",
      "\tTrain Loss: 0.111 | Train f1: 0.64 | Train precision: 0.68 | Train recall: 0.63\n",
      "\t Val. Loss: 0.196 |  Val. f1: 0.59 |  Val. precision: 0.63 | Val. recall: 0.61\n",
      "Epoch: 10 | Epoch Time: 0m 8s\n",
      "\tTrain Loss: 0.101 | Train f1: 0.66 | Train precision: 0.70 | Train recall: 0.66\n",
      "\t Val. Loss: 0.192 |  Val. f1: 0.59 |  Val. precision: 0.64 | Val. recall: 0.61\n",
      "Epoch: 11 | Epoch Time: 0m 9s\n",
      "\tTrain Loss: 0.092 | Train f1: 0.68 | Train precision: 0.72 | Train recall: 0.68\n",
      "\t Val. Loss: 0.189 |  Val. f1: 0.61 |  Val. precision: 0.66 | Val. recall: 0.63\n",
      "Epoch: 12 | Epoch Time: 0m 9s\n",
      "\tTrain Loss: 0.087 | Train f1: 0.69 | Train precision: 0.73 | Train recall: 0.69\n",
      "\t Val. Loss: 0.192 |  Val. f1: 0.62 |  Val. precision: 0.66 | Val. recall: 0.64\n",
      "Epoch: 13 | Epoch Time: 0m 9s\n",
      "\tTrain Loss: 0.080 | Train f1: 0.72 | Train precision: 0.75 | Train recall: 0.71\n",
      "\t Val. Loss: 0.188 |  Val. f1: 0.63 |  Val. precision: 0.67 | Val. recall: 0.65\n",
      "Epoch: 14 | Epoch Time: 0m 9s\n",
      "\tTrain Loss: 0.074 | Train f1: 0.73 | Train precision: 0.77 | Train recall: 0.73\n",
      "\t Val. Loss: 0.194 |  Val. f1: 0.63 |  Val. precision: 0.66 | Val. recall: 0.66\n",
      "Epoch: 15 | Epoch Time: 0m 9s\n",
      "\tTrain Loss: 0.069 | Train f1: 0.75 | Train precision: 0.78 | Train recall: 0.74\n",
      "\t Val. Loss: 0.189 |  Val. f1: 0.62 |  Val. precision: 0.66 | Val. recall: 0.63\n",
      "Epoch: 16 | Epoch Time: 0m 9s\n",
      "\tTrain Loss: 0.068 | Train f1: 0.75 | Train precision: 0.78 | Train recall: 0.75\n",
      "\t Val. Loss: 0.218 |  Val. f1: 0.61 |  Val. precision: 0.65 | Val. recall: 0.65\n",
      "Epoch: 17 | Epoch Time: 0m 8s\n",
      "\tTrain Loss: 0.063 | Train f1: 0.76 | Train precision: 0.79 | Train recall: 0.76\n",
      "\t Val. Loss: 0.195 |  Val. f1: 0.62 |  Val. precision: 0.66 | Val. recall: 0.65\n",
      "Epoch: 18 | Epoch Time: 0m 8s\n",
      "\tTrain Loss: 0.061 | Train f1: 0.76 | Train precision: 0.79 | Train recall: 0.76\n",
      "\t Val. Loss: 0.186 |  Val. f1: 0.63 |  Val. precision: 0.67 | Val. recall: 0.64\n",
      "Epoch: 19 | Epoch Time: 0m 9s\n",
      "\tTrain Loss: 0.056 | Train f1: 0.78 | Train precision: 0.81 | Train recall: 0.78\n",
      "\t Val. Loss: 0.196 |  Val. f1: 0.61 |  Val. precision: 0.66 | Val. recall: 0.63\n",
      "Epoch: 20 | Epoch Time: 0m 9s\n",
      "\tTrain Loss: 0.054 | Train f1: 0.78 | Train precision: 0.81 | Train recall: 0.78\n",
      "\t Val. Loss: 0.205 |  Val. f1: 0.60 |  Val. precision: 0.65 | Val. recall: 0.63\n",
      "Epoch: 21 | Epoch Time: 0m 9s\n",
      "\tTrain Loss: 0.052 | Train f1: 0.79 | Train precision: 0.82 | Train recall: 0.79\n",
      "\t Val. Loss: 0.208 |  Val. f1: 0.61 |  Val. precision: 0.66 | Val. recall: 0.63\n",
      "Epoch: 22 | Epoch Time: 0m 9s\n",
      "\tTrain Loss: 0.050 | Train f1: 0.80 | Train precision: 0.82 | Train recall: 0.80\n",
      "\t Val. Loss: 0.201 |  Val. f1: 0.62 |  Val. precision: 0.68 | Val. recall: 0.64\n",
      "Epoch: 23 | Epoch Time: 0m 8s\n",
      "\tTrain Loss: 0.049 | Train f1: 0.80 | Train precision: 0.82 | Train recall: 0.79\n",
      "\t Val. Loss: 0.208 |  Val. f1: 0.62 |  Val. precision: 0.67 | Val. recall: 0.63\n",
      "Epoch: 24 | Epoch Time: 0m 8s\n",
      "\tTrain Loss: 0.046 | Train f1: 0.81 | Train precision: 0.83 | Train recall: 0.81\n",
      "\t Val. Loss: 0.188 |  Val. f1: 0.63 |  Val. precision: 0.67 | Val. recall: 0.65\n",
      "Epoch: 25 | Epoch Time: 0m 9s\n",
      "\tTrain Loss: 0.045 | Train f1: 0.81 | Train precision: 0.83 | Train recall: 0.81\n",
      "\t Val. Loss: 0.196 |  Val. f1: 0.64 |  Val. precision: 0.68 | Val. recall: 0.65\n",
      "Epoch: 26 | Epoch Time: 0m 8s\n",
      "\tTrain Loss: 0.042 | Train f1: 0.82 | Train precision: 0.84 | Train recall: 0.82\n",
      "\t Val. Loss: 0.199 |  Val. f1: 0.64 |  Val. precision: 0.68 | Val. recall: 0.65\n",
      "Epoch: 27 | Epoch Time: 0m 9s\n",
      "\tTrain Loss: 0.042 | Train f1: 0.82 | Train precision: 0.84 | Train recall: 0.82\n",
      "\t Val. Loss: 0.206 |  Val. f1: 0.63 |  Val. precision: 0.67 | Val. recall: 0.64\n",
      "Epoch: 28 | Epoch Time: 0m 9s\n",
      "\tTrain Loss: 0.041 | Train f1: 0.83 | Train precision: 0.85 | Train recall: 0.83\n",
      "\t Val. Loss: 0.205 |  Val. f1: 0.63 |  Val. precision: 0.67 | Val. recall: 0.65\n",
      "\n",
      "Performance of best found Model:\n",
      "Val. Loss: 0.186 |  Val. f1: 0.63 | Val. precision: 0.67 | Val. recall: 0.64\n"
     ]
    }
   ],
   "source": [
    "model = elman_model\n",
    "model_name = elman_model_name\n",
    "criterion = baseline_criterion\n",
    "\n",
    "model.apply(init_weights)\n",
    "print(f'El modelo actual tiene {count_parameters(model):,} parámetros entrenables.')\n",
    "\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "model = model.to(device)\n",
    "criterion = criterion.to(device)\n",
    "\n",
    "optimize_model(model, train_iterator, valid_iterator, optimizer, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "u3rFH-K4vsqK",
    "outputId": "6f8f9366-8185-46d0-cf5b-63ae552dfb47"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El modelo actual tiene 4,347,838 parámetros entrenables.\n",
      "Epoch: 01 | Epoch Time: 0m 10s\n",
      "\tTrain Loss: 0.530 | Train f1: 0.09 | Train precision: 0.19 | Train recall: 0.06\n",
      "\t Val. Loss: 0.369 |  Val. f1: 0.30 |  Val. precision: 0.45 | Val. recall: 0.26\n",
      "Epoch: 02 | Epoch Time: 0m 10s\n",
      "\tTrain Loss: 0.303 | Train f1: 0.35 | Train precision: 0.47 | Train recall: 0.31\n",
      "\t Val. Loss: 0.257 |  Val. f1: 0.47 |  Val. precision: 0.58 | Val. recall: 0.44\n",
      "Epoch: 03 | Epoch Time: 0m 10s\n",
      "\tTrain Loss: 0.210 | Train f1: 0.48 | Train precision: 0.58 | Train recall: 0.45\n",
      "\t Val. Loss: 0.232 |  Val. f1: 0.50 |  Val. precision: 0.60 | Val. recall: 0.50\n",
      "Epoch: 04 | Epoch Time: 0m 10s\n",
      "\tTrain Loss: 0.164 | Train f1: 0.56 | Train precision: 0.64 | Train recall: 0.54\n",
      "\t Val. Loss: 0.196 |  Val. f1: 0.58 |  Val. precision: 0.63 | Val. recall: 0.59\n",
      "Epoch: 05 | Epoch Time: 0m 10s\n",
      "\tTrain Loss: 0.133 | Train f1: 0.61 | Train precision: 0.68 | Train recall: 0.60\n",
      "\t Val. Loss: 0.197 |  Val. f1: 0.58 |  Val. precision: 0.66 | Val. recall: 0.56\n",
      "Epoch: 06 | Epoch Time: 0m 10s\n",
      "\tTrain Loss: 0.110 | Train f1: 0.66 | Train precision: 0.72 | Train recall: 0.65\n",
      "\t Val. Loss: 0.174 |  Val. f1: 0.62 |  Val. precision: 0.67 | Val. recall: 0.63\n",
      "Epoch: 07 | Epoch Time: 0m 10s\n",
      "\tTrain Loss: 0.094 | Train f1: 0.69 | Train precision: 0.74 | Train recall: 0.68\n",
      "\t Val. Loss: 0.185 |  Val. f1: 0.61 |  Val. precision: 0.67 | Val. recall: 0.61\n",
      "Epoch: 08 | Epoch Time: 0m 10s\n",
      "\tTrain Loss: 0.083 | Train f1: 0.72 | Train precision: 0.76 | Train recall: 0.71\n",
      "\t Val. Loss: 0.173 |  Val. f1: 0.63 |  Val. precision: 0.69 | Val. recall: 0.62\n",
      "Epoch: 09 | Epoch Time: 0m 10s\n",
      "\tTrain Loss: 0.073 | Train f1: 0.74 | Train precision: 0.77 | Train recall: 0.73\n",
      "\t Val. Loss: 0.185 |  Val. f1: 0.63 |  Val. precision: 0.69 | Val. recall: 0.63\n",
      "Epoch: 10 | Epoch Time: 0m 10s\n",
      "\tTrain Loss: 0.067 | Train f1: 0.75 | Train precision: 0.79 | Train recall: 0.75\n",
      "\t Val. Loss: 0.176 |  Val. f1: 0.65 |  Val. precision: 0.71 | Val. recall: 0.64\n",
      "Epoch: 11 | Epoch Time: 0m 10s\n",
      "\tTrain Loss: 0.060 | Train f1: 0.77 | Train precision: 0.80 | Train recall: 0.77\n",
      "\t Val. Loss: 0.199 |  Val. f1: 0.64 |  Val. precision: 0.70 | Val. recall: 0.64\n",
      "Epoch: 12 | Epoch Time: 0m 10s\n",
      "\tTrain Loss: 0.055 | Train f1: 0.78 | Train precision: 0.81 | Train recall: 0.78\n",
      "\t Val. Loss: 0.181 |  Val. f1: 0.66 |  Val. precision: 0.71 | Val. recall: 0.67\n",
      "Epoch: 13 | Epoch Time: 0m 10s\n",
      "\tTrain Loss: 0.052 | Train f1: 0.79 | Train precision: 0.82 | Train recall: 0.79\n",
      "\t Val. Loss: 0.192 |  Val. f1: 0.65 |  Val. precision: 0.70 | Val. recall: 0.64\n",
      "Epoch: 14 | Epoch Time: 0m 10s\n",
      "\tTrain Loss: 0.046 | Train f1: 0.81 | Train precision: 0.84 | Train recall: 0.81\n",
      "\t Val. Loss: 0.204 |  Val. f1: 0.65 |  Val. precision: 0.70 | Val. recall: 0.65\n",
      "Epoch: 15 | Epoch Time: 0m 10s\n",
      "\tTrain Loss: 0.045 | Train f1: 0.81 | Train precision: 0.84 | Train recall: 0.81\n",
      "\t Val. Loss: 0.190 |  Val. f1: 0.66 |  Val. precision: 0.71 | Val. recall: 0.66\n",
      "Epoch: 16 | Epoch Time: 0m 10s\n",
      "\tTrain Loss: 0.041 | Train f1: 0.82 | Train precision: 0.84 | Train recall: 0.82\n",
      "\t Val. Loss: 0.199 |  Val. f1: 0.66 |  Val. precision: 0.70 | Val. recall: 0.67\n",
      "Epoch: 17 | Epoch Time: 0m 10s\n",
      "\tTrain Loss: 0.040 | Train f1: 0.83 | Train precision: 0.85 | Train recall: 0.82\n",
      "\t Val. Loss: 0.204 |  Val. f1: 0.67 |  Val. precision: 0.71 | Val. recall: 0.66\n",
      "Epoch: 18 | Epoch Time: 0m 10s\n",
      "\tTrain Loss: 0.038 | Train f1: 0.84 | Train precision: 0.86 | Train recall: 0.83\n",
      "\t Val. Loss: 0.210 |  Val. f1: 0.66 |  Val. precision: 0.71 | Val. recall: 0.65\n",
      "\n",
      "Performance of best found Model:\n",
      "Val. Loss: 0.173 |  Val. f1: 0.63 | Val. precision: 0.69 | Val. recall: 0.62\n"
     ]
    }
   ],
   "source": [
    "model = gru_model\n",
    "model_name = gru_model_name\n",
    "criterion = baseline_criterion\n",
    "\n",
    "model.apply(init_weights)\n",
    "print(f'El modelo actual tiene {count_parameters(model):,} parámetros entrenables.')\n",
    "\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "model = model.to(device)\n",
    "criterion = criterion.to(device)\n",
    "\n",
    "optimize_model(model, train_iterator, valid_iterator, optimizer, criterion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AmO7diDc5RIh"
   },
   "source": [
    "##### Output *Dropout*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hKGeolHSB2JF"
   },
   "source": [
    "Para estos experimentos se considerarán dos modelos basados en GRU, puesto que se obtuvieron resultados bastante competitivos entre sí al considerar una red recurrente tipo GRU sin *dropout* y con un *dropout* de `0.5`. Para los otros dos tipos de arquitectura se tiene que, en promedio y en términos globales, se obtienen mejores resultados sin *dropout* en la red recurrente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Gdot3TZeCcSK"
   },
   "outputs": [],
   "source": [
    "## Parámetros del Experimento ##\n",
    "\n",
    "# tamaño del vocabulario. recuerden que la entrada son vectores bag of word(one-hot).\n",
    "INPUT_DIM = len(TEXT.vocab)\n",
    "EMBEDDING_DIM = 100  # dimensión de los embeddings.\n",
    "HIDDEN_DIM = 256  # dimensión de la capas RNN\n",
    "OUTPUT_DIM = len(NER_TAGS.vocab)  # número de clases\n",
    "\n",
    "N_LAYERS = 2  # número de capas.\n",
    "IN_DROPOUT = 0.75\n",
    "RNN_DROPOUT = 0.0\n",
    "#OUT_DROPOUT = 0.0\n",
    "#OUT_DROPOUT = 0.1\n",
    "#OUT_DROPOUT = 0.25\n",
    "#OUT_DROPOUT = 0.5\n",
    "OUT_DROPOUT = 0.75\n",
    "BIDIRECTIONAL = True\n",
    "\n",
    "TAG_PAD_IDX = NER_TAGS.vocab.stoi[NER_TAGS.pad_token]\n",
    "baseline_criterion = nn.CrossEntropyLoss(ignore_index = TAG_PAD_IDX)\n",
    "\n",
    "# Creamos nuestros modelos.\n",
    "lstm_model = NER_RNN(INPUT_DIM, EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM,\n",
    "                     N_LAYERS, BIDIRECTIONAL, IN_DROPOUT, RNN_DROPOUT, \n",
    "                     OUT_DROPOUT, PAD_IDX)\n",
    "\n",
    "lstm_model_name = 'LSTM'  # nombre que tendrá el modelo guardado...\n",
    "\n",
    "elman_model = NER_ELMAN(INPUT_DIM, EMBEDDING_DIM, int(HIDDEN_DIM/2), OUTPUT_DIM,\n",
    "                        N_LAYERS, BIDIRECTIONAL, IN_DROPOUT, RNN_DROPOUT, \n",
    "                        OUT_DROPOUT, PAD_IDX, 'relu')\n",
    "\n",
    "elman_model_name = 'Elman'  # nombre que tendrá el modelo guardado...\n",
    "\n",
    "gru_model1 = NER_GRU(INPUT_DIM, EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM,\n",
    "                    N_LAYERS, BIDIRECTIONAL, IN_DROPOUT, RNN_DROPOUT, \n",
    "                    OUT_DROPOUT, PAD_IDX)\n",
    "\n",
    "gru_model_name1 = 'GRU'  # nombre que tendrá el modelo guardado...\n",
    "\n",
    "gru_model2 = NER_GRU(INPUT_DIM, EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM,\n",
    "                    N_LAYERS, BIDIRECTIONAL, IN_DROPOUT, 0.5, \n",
    "                    OUT_DROPOUT, PAD_IDX)\n",
    "\n",
    "gru_model_name2 = 'GRU'  # nombre que tendrá el modelo guardado..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "TxH1t8flCxWC",
    "outputId": "cfd1ffe3-9b2f-4def-ba05-09417d416be0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El modelo actual tiene 4,925,374 parámetros entrenables.\n",
      "Epoch: 01 | Epoch Time: 0m 26s\n",
      "\tTrain Loss: 0.487 | Train f1: 0.12 | Train precision: 0.21 | Train recall: 0.10\n",
      "\t Val. Loss: 0.328 |  Val. f1: 0.32 |  Val. precision: 0.43 | Val. recall: 0.30\n",
      "Epoch: 02 | Epoch Time: 0m 26s\n",
      "\tTrain Loss: 0.265 | Train f1: 0.38 | Train precision: 0.48 | Train recall: 0.36\n",
      "\t Val. Loss: 0.243 |  Val. f1: 0.46 |  Val. precision: 0.56 | Val. recall: 0.43\n",
      "Epoch: 03 | Epoch Time: 0m 26s\n",
      "\tTrain Loss: 0.190 | Train f1: 0.49 | Train precision: 0.57 | Train recall: 0.47\n",
      "\t Val. Loss: 0.210 |  Val. f1: 0.49 |  Val. precision: 0.59 | Val. recall: 0.48\n",
      "Epoch: 04 | Epoch Time: 0m 26s\n",
      "\tTrain Loss: 0.148 | Train f1: 0.57 | Train precision: 0.64 | Train recall: 0.56\n",
      "\t Val. Loss: 0.199 |  Val. f1: 0.54 |  Val. precision: 0.62 | Val. recall: 0.55\n",
      "Epoch: 05 | Epoch Time: 0m 26s\n",
      "\tTrain Loss: 0.120 | Train f1: 0.63 | Train precision: 0.68 | Train recall: 0.62\n",
      "\t Val. Loss: 0.168 |  Val. f1: 0.61 |  Val. precision: 0.66 | Val. recall: 0.61\n",
      "Epoch: 06 | Epoch Time: 0m 26s\n",
      "\tTrain Loss: 0.100 | Train f1: 0.67 | Train precision: 0.71 | Train recall: 0.66\n",
      "\t Val. Loss: 0.173 |  Val. f1: 0.61 |  Val. precision: 0.67 | Val. recall: 0.61\n",
      "Epoch: 07 | Epoch Time: 0m 26s\n",
      "\tTrain Loss: 0.087 | Train f1: 0.70 | Train precision: 0.74 | Train recall: 0.69\n",
      "\t Val. Loss: 0.171 |  Val. f1: 0.60 |  Val. precision: 0.66 | Val. recall: 0.61\n",
      "Epoch: 08 | Epoch Time: 0m 26s\n",
      "\tTrain Loss: 0.075 | Train f1: 0.73 | Train precision: 0.77 | Train recall: 0.73\n",
      "\t Val. Loss: 0.162 |  Val. f1: 0.63 |  Val. precision: 0.68 | Val. recall: 0.63\n",
      "Epoch: 09 | Epoch Time: 0m 26s\n",
      "\tTrain Loss: 0.067 | Train f1: 0.75 | Train precision: 0.79 | Train recall: 0.75\n",
      "\t Val. Loss: 0.177 |  Val. f1: 0.62 |  Val. precision: 0.68 | Val. recall: 0.62\n",
      "Epoch: 10 | Epoch Time: 0m 26s\n",
      "\tTrain Loss: 0.060 | Train f1: 0.76 | Train precision: 0.79 | Train recall: 0.76\n",
      "\t Val. Loss: 0.169 |  Val. f1: 0.66 |  Val. precision: 0.69 | Val. recall: 0.67\n",
      "Epoch: 11 | Epoch Time: 0m 26s\n",
      "\tTrain Loss: 0.053 | Train f1: 0.79 | Train precision: 0.81 | Train recall: 0.79\n",
      "\t Val. Loss: 0.164 |  Val. f1: 0.65 |  Val. precision: 0.69 | Val. recall: 0.65\n",
      "Epoch: 12 | Epoch Time: 0m 26s\n",
      "\tTrain Loss: 0.048 | Train f1: 0.80 | Train precision: 0.82 | Train recall: 0.80\n",
      "\t Val. Loss: 0.177 |  Val. f1: 0.64 |  Val. precision: 0.69 | Val. recall: 0.65\n",
      "Epoch: 13 | Epoch Time: 0m 26s\n",
      "\tTrain Loss: 0.046 | Train f1: 0.81 | Train precision: 0.83 | Train recall: 0.81\n",
      "\t Val. Loss: 0.177 |  Val. f1: 0.64 |  Val. precision: 0.68 | Val. recall: 0.65\n",
      "Epoch: 14 | Epoch Time: 0m 26s\n",
      "\tTrain Loss: 0.040 | Train f1: 0.83 | Train precision: 0.84 | Train recall: 0.82\n",
      "\t Val. Loss: 0.180 |  Val. f1: 0.65 |  Val. precision: 0.69 | Val. recall: 0.66\n",
      "Epoch: 15 | Epoch Time: 0m 26s\n",
      "\tTrain Loss: 0.037 | Train f1: 0.84 | Train precision: 0.86 | Train recall: 0.84\n",
      "\t Val. Loss: 0.185 |  Val. f1: 0.64 |  Val. precision: 0.68 | Val. recall: 0.65\n",
      "Epoch: 16 | Epoch Time: 0m 26s\n",
      "\tTrain Loss: 0.033 | Train f1: 0.85 | Train precision: 0.87 | Train recall: 0.85\n",
      "\t Val. Loss: 0.186 |  Val. f1: 0.64 |  Val. precision: 0.69 | Val. recall: 0.65\n",
      "Epoch: 17 | Epoch Time: 0m 26s\n",
      "\tTrain Loss: 0.031 | Train f1: 0.86 | Train precision: 0.87 | Train recall: 0.86\n",
      "\t Val. Loss: 0.195 |  Val. f1: 0.64 |  Val. precision: 0.69 | Val. recall: 0.65\n",
      "Epoch: 18 | Epoch Time: 0m 26s\n",
      "\tTrain Loss: 0.029 | Train f1: 0.87 | Train precision: 0.88 | Train recall: 0.87\n",
      "\t Val. Loss: 0.201 |  Val. f1: 0.65 |  Val. precision: 0.70 | Val. recall: 0.65\n",
      "\n",
      "Performance of best found Model:\n",
      "Val. Loss: 0.162 |  Val. f1: 0.63 | Val. precision: 0.68 | Val. recall: 0.63\n"
     ]
    }
   ],
   "source": [
    "model = lstm_model\n",
    "model_name = lstm_model_name\n",
    "criterion = baseline_criterion\n",
    "\n",
    "model.apply(init_weights)\n",
    "print(f'El modelo actual tiene {count_parameters(model):,} parámetros entrenables.')\n",
    "\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "model = model.to(device)\n",
    "criterion = criterion.to(device)\n",
    "\n",
    "optimize_model(model, train_iterator, valid_iterator, optimizer, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "RKjpcsSlC0RR",
    "outputId": "9a2b0fcb-5de3-49ec-b438-d1f6a7d8f785"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El modelo actual tiene 2,770,366 parámetros entrenables.\n",
      "Epoch: 01 | Epoch Time: 0m 8s\n",
      "\tTrain Loss: 0.587 | Train f1: 0.03 | Train precision: 0.08 | Train recall: 0.02\n",
      "\t Val. Loss: 0.487 |  Val. f1: 0.24 |  Val. precision: 0.35 | Val. recall: 0.22\n",
      "Epoch: 02 | Epoch Time: 0m 8s\n",
      "\tTrain Loss: 0.356 | Train f1: 0.23 | Train precision: 0.34 | Train recall: 0.20\n",
      "\t Val. Loss: 0.305 |  Val. f1: 0.34 |  Val. precision: 0.41 | Val. recall: 0.35\n",
      "Epoch: 03 | Epoch Time: 0m 8s\n",
      "\tTrain Loss: 0.263 | Train f1: 0.34 | Train precision: 0.40 | Train recall: 0.32\n",
      "\t Val. Loss: 0.250 |  Val. f1: 0.42 |  Val. precision: 0.48 | Val. recall: 0.42\n",
      "Epoch: 04 | Epoch Time: 0m 8s\n",
      "\tTrain Loss: 0.212 | Train f1: 0.41 | Train precision: 0.48 | Train recall: 0.40\n",
      "\t Val. Loss: 0.222 |  Val. f1: 0.47 |  Val. precision: 0.54 | Val. recall: 0.48\n",
      "Epoch: 05 | Epoch Time: 0m 8s\n",
      "\tTrain Loss: 0.178 | Train f1: 0.47 | Train precision: 0.54 | Train recall: 0.47\n",
      "\t Val. Loss: 0.208 |  Val. f1: 0.51 |  Val. precision: 0.57 | Val. recall: 0.53\n",
      "Epoch: 06 | Epoch Time: 0m 8s\n",
      "\tTrain Loss: 0.155 | Train f1: 0.53 | Train precision: 0.59 | Train recall: 0.52\n",
      "\t Val. Loss: 0.197 |  Val. f1: 0.55 |  Val. precision: 0.61 | Val. recall: 0.55\n",
      "Epoch: 07 | Epoch Time: 0m 8s\n",
      "\tTrain Loss: 0.137 | Train f1: 0.57 | Train precision: 0.63 | Train recall: 0.57\n",
      "\t Val. Loss: 0.194 |  Val. f1: 0.55 |  Val. precision: 0.61 | Val. recall: 0.57\n",
      "Epoch: 08 | Epoch Time: 0m 8s\n",
      "\tTrain Loss: 0.123 | Train f1: 0.62 | Train precision: 0.67 | Train recall: 0.61\n",
      "\t Val. Loss: 0.187 |  Val. f1: 0.59 |  Val. precision: 0.64 | Val. recall: 0.59\n",
      "Epoch: 09 | Epoch Time: 0m 8s\n",
      "\tTrain Loss: 0.110 | Train f1: 0.64 | Train precision: 0.69 | Train recall: 0.64\n",
      "\t Val. Loss: 0.186 |  Val. f1: 0.60 |  Val. precision: 0.65 | Val. recall: 0.61\n",
      "Epoch: 10 | Epoch Time: 0m 8s\n",
      "\tTrain Loss: 0.099 | Train f1: 0.67 | Train precision: 0.72 | Train recall: 0.67\n",
      "\t Val. Loss: 0.184 |  Val. f1: 0.60 |  Val. precision: 0.65 | Val. recall: 0.62\n",
      "Epoch: 11 | Epoch Time: 0m 8s\n",
      "\tTrain Loss: 0.092 | Train f1: 0.69 | Train precision: 0.73 | Train recall: 0.69\n",
      "\t Val. Loss: 0.181 |  Val. f1: 0.60 |  Val. precision: 0.64 | Val. recall: 0.62\n",
      "Epoch: 12 | Epoch Time: 0m 8s\n",
      "\tTrain Loss: 0.084 | Train f1: 0.71 | Train precision: 0.75 | Train recall: 0.71\n",
      "\t Val. Loss: 0.184 |  Val. f1: 0.61 |  Val. precision: 0.66 | Val. recall: 0.62\n",
      "Epoch: 13 | Epoch Time: 0m 8s\n",
      "\tTrain Loss: 0.077 | Train f1: 0.73 | Train precision: 0.76 | Train recall: 0.72\n",
      "\t Val. Loss: 0.180 |  Val. f1: 0.63 |  Val. precision: 0.67 | Val. recall: 0.64\n",
      "Epoch: 14 | Epoch Time: 0m 8s\n",
      "\tTrain Loss: 0.071 | Train f1: 0.74 | Train precision: 0.77 | Train recall: 0.74\n",
      "\t Val. Loss: 0.177 |  Val. f1: 0.63 |  Val. precision: 0.68 | Val. recall: 0.64\n",
      "Epoch: 15 | Epoch Time: 0m 8s\n",
      "\tTrain Loss: 0.069 | Train f1: 0.76 | Train precision: 0.78 | Train recall: 0.75\n",
      "\t Val. Loss: 0.180 |  Val. f1: 0.64 |  Val. precision: 0.68 | Val. recall: 0.65\n",
      "Epoch: 16 | Epoch Time: 0m 8s\n",
      "\tTrain Loss: 0.063 | Train f1: 0.77 | Train precision: 0.79 | Train recall: 0.77\n",
      "\t Val. Loss: 0.191 |  Val. f1: 0.62 |  Val. precision: 0.66 | Val. recall: 0.64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/numpy/lib/function_base.py:393: RuntimeWarning: Mean of empty slice.\n",
      "  avg = a.mean(axis)\n",
      "/usr/local/lib/python3.6/dist-packages/numpy/core/_methods.py:161: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 17 | Epoch Time: 0m 8s\n",
      "\tTrain Loss: 0.060 | Train f1: nan | Train precision: nan | Train recall: nan\n",
      "\t Val. Loss: 0.187 |  Val. f1: 0.63 |  Val. precision: 0.68 | Val. recall: 0.64\n",
      "Epoch: 18 | Epoch Time: 0m 8s\n",
      "\tTrain Loss: 0.060 | Train f1: 0.78 | Train precision: 0.81 | Train recall: 0.78\n",
      "\t Val. Loss: 0.195 |  Val. f1: 0.63 |  Val. precision: 0.67 | Val. recall: 0.64\n",
      "Epoch: 19 | Epoch Time: 0m 8s\n",
      "\tTrain Loss: 0.055 | Train f1: 0.79 | Train precision: 0.81 | Train recall: 0.79\n",
      "\t Val. Loss: 0.191 |  Val. f1: 0.64 |  Val. precision: 0.68 | Val. recall: 0.65\n",
      "Epoch: 20 | Epoch Time: 0m 8s\n",
      "\tTrain Loss: 0.053 | Train f1: 0.80 | Train precision: 0.82 | Train recall: 0.80\n",
      "\t Val. Loss: 0.194 |  Val. f1: 0.64 |  Val. precision: 0.67 | Val. recall: 0.65\n",
      "Epoch: 21 | Epoch Time: 0m 8s\n",
      "\tTrain Loss: 0.049 | Train f1: 0.80 | Train precision: 0.82 | Train recall: 0.80\n",
      "\t Val. Loss: 0.207 |  Val. f1: 0.65 |  Val. precision: 0.69 | Val. recall: 0.66\n",
      "Epoch: 22 | Epoch Time: 0m 8s\n",
      "\tTrain Loss: 0.049 | Train f1: 0.81 | Train precision: 0.83 | Train recall: 0.81\n",
      "\t Val. Loss: 0.199 |  Val. f1: 0.65 |  Val. precision: 0.68 | Val. recall: 0.66\n",
      "Epoch: 23 | Epoch Time: 0m 8s\n",
      "\tTrain Loss: 0.046 | Train f1: 0.81 | Train precision: 0.83 | Train recall: 0.81\n",
      "\t Val. Loss: 0.199 |  Val. f1: 0.65 |  Val. precision: 0.69 | Val. recall: 0.65\n",
      "Epoch: 24 | Epoch Time: 0m 8s\n",
      "\tTrain Loss: 0.043 | Train f1: 0.82 | Train precision: 0.84 | Train recall: 0.82\n",
      "\t Val. Loss: 0.206 |  Val. f1: 0.65 |  Val. precision: 0.68 | Val. recall: 0.67\n",
      "\n",
      "Performance of best found Model:\n",
      "Val. Loss: 0.177 |  Val. f1: 0.63 | Val. precision: 0.68 | Val. recall: 0.64\n"
     ]
    }
   ],
   "source": [
    "model = elman_model\n",
    "model_name = elman_model_name\n",
    "criterion = baseline_criterion\n",
    "\n",
    "model.apply(init_weights)\n",
    "print(f'El modelo actual tiene {count_parameters(model):,} parámetros entrenables.')\n",
    "\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "model = model.to(device)\n",
    "criterion = criterion.to(device)\n",
    "\n",
    "optimize_model(model, train_iterator, valid_iterator, optimizer, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "9yYBcQSvC4Rf",
    "outputId": "3ff42ee5-f4a8-400e-dae4-e68777938bb8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El modelo actual tiene 4,347,838 parámetros entrenables.\n",
      "Epoch: 01 | Epoch Time: 0m 8s\n",
      "\tTrain Loss: 0.563 | Train f1: 0.10 | Train precision: 0.22 | Train recall: 0.08\n",
      "\t Val. Loss: 0.378 |  Val. f1: 0.27 |  Val. precision: 0.42 | Val. recall: 0.22\n",
      "Epoch: 02 | Epoch Time: 0m 8s\n",
      "\tTrain Loss: 0.308 | Train f1: 0.34 | Train precision: 0.45 | Train recall: 0.30\n",
      "\t Val. Loss: 0.283 |  Val. f1: 0.41 |  Val. precision: 0.51 | Val. recall: 0.40\n",
      "Epoch: 03 | Epoch Time: 0m 8s\n",
      "\tTrain Loss: 0.215 | Train f1: 0.48 | Train precision: 0.57 | Train recall: 0.45\n",
      "\t Val. Loss: 0.223 |  Val. f1: 0.50 |  Val. precision: 0.60 | Val. recall: 0.48\n",
      "Epoch: 04 | Epoch Time: 0m 8s\n",
      "\tTrain Loss: 0.164 | Train f1: 0.56 | Train precision: 0.63 | Train recall: 0.54\n",
      "\t Val. Loss: 0.192 |  Val. f1: 0.56 |  Val. precision: 0.65 | Val. recall: 0.54\n",
      "Epoch: 05 | Epoch Time: 0m 8s\n",
      "\tTrain Loss: 0.133 | Train f1: 0.62 | Train precision: 0.68 | Train recall: 0.60\n",
      "\t Val. Loss: 0.191 |  Val. f1: 0.58 |  Val. precision: 0.67 | Val. recall: 0.56\n",
      "Epoch: 06 | Epoch Time: 0m 8s\n",
      "\tTrain Loss: 0.110 | Train f1: 0.67 | Train precision: 0.72 | Train recall: 0.65\n",
      "\t Val. Loss: 0.174 |  Val. f1: 0.60 |  Val. precision: 0.67 | Val. recall: 0.59\n",
      "Epoch: 07 | Epoch Time: 0m 8s\n",
      "\tTrain Loss: 0.093 | Train f1: 0.70 | Train precision: 0.74 | Train recall: 0.69\n",
      "\t Val. Loss: 0.182 |  Val. f1: 0.61 |  Val. precision: 0.68 | Val. recall: 0.60\n",
      "Epoch: 08 | Epoch Time: 0m 8s\n",
      "\tTrain Loss: 0.082 | Train f1: 0.72 | Train precision: 0.76 | Train recall: 0.71\n",
      "\t Val. Loss: 0.184 |  Val. f1: 0.61 |  Val. precision: 0.68 | Val. recall: 0.61\n",
      "Epoch: 09 | Epoch Time: 0m 8s\n",
      "\tTrain Loss: 0.072 | Train f1: 0.75 | Train precision: 0.78 | Train recall: 0.74\n",
      "\t Val. Loss: 0.166 |  Val. f1: 0.63 |  Val. precision: 0.69 | Val. recall: 0.64\n",
      "Epoch: 10 | Epoch Time: 0m 8s\n",
      "\tTrain Loss: 0.066 | Train f1: 0.76 | Train precision: 0.80 | Train recall: 0.76\n",
      "\t Val. Loss: 0.171 |  Val. f1: 0.64 |  Val. precision: 0.70 | Val. recall: 0.64\n",
      "Epoch: 11 | Epoch Time: 0m 8s\n",
      "\tTrain Loss: 0.059 | Train f1: 0.78 | Train precision: 0.81 | Train recall: 0.77\n",
      "\t Val. Loss: 0.171 |  Val. f1: 0.65 |  Val. precision: 0.70 | Val. recall: 0.65\n",
      "Epoch: 12 | Epoch Time: 0m 8s\n",
      "\tTrain Loss: 0.055 | Train f1: 0.79 | Train precision: 0.81 | Train recall: 0.78\n",
      "\t Val. Loss: 0.190 |  Val. f1: 0.63 |  Val. precision: 0.69 | Val. recall: 0.62\n",
      "Epoch: 13 | Epoch Time: 0m 8s\n",
      "\tTrain Loss: 0.050 | Train f1: 0.80 | Train precision: 0.83 | Train recall: 0.80\n",
      "\t Val. Loss: 0.180 |  Val. f1: 0.64 |  Val. precision: 0.70 | Val. recall: 0.64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/numpy/lib/function_base.py:393: RuntimeWarning: Mean of empty slice.\n",
      "  avg = a.mean(axis)\n",
      "/usr/local/lib/python3.6/dist-packages/numpy/core/_methods.py:161: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 14 | Epoch Time: 0m 8s\n",
      "\tTrain Loss: 0.046 | Train f1: nan | Train precision: nan | Train recall: nan\n",
      "\t Val. Loss: 0.191 |  Val. f1: 0.64 |  Val. precision: 0.70 | Val. recall: 0.63\n",
      "Epoch: 15 | Epoch Time: 0m 8s\n",
      "\tTrain Loss: 0.042 | Train f1: 0.83 | Train precision: 0.85 | Train recall: 0.83\n",
      "\t Val. Loss: 0.201 |  Val. f1: 0.64 |  Val. precision: 0.70 | Val. recall: 0.63\n",
      "Epoch: 16 | Epoch Time: 0m 8s\n",
      "\tTrain Loss: 0.038 | Train f1: 0.84 | Train precision: 0.86 | Train recall: 0.84\n",
      "\t Val. Loss: 0.196 |  Val. f1: 0.64 |  Val. precision: 0.70 | Val. recall: 0.63\n",
      "Epoch: 17 | Epoch Time: 0m 8s\n",
      "\tTrain Loss: 0.037 | Train f1: 0.85 | Train precision: 0.86 | Train recall: 0.84\n",
      "\t Val. Loss: 0.222 |  Val. f1: 0.63 |  Val. precision: 0.70 | Val. recall: 0.63\n",
      "Epoch: 18 | Epoch Time: 0m 8s\n",
      "\tTrain Loss: 0.036 | Train f1: 0.85 | Train precision: 0.86 | Train recall: 0.84\n",
      "\t Val. Loss: 0.203 |  Val. f1: 0.66 |  Val. precision: 0.71 | Val. recall: 0.65\n",
      "Epoch: 19 | Epoch Time: 0m 8s\n",
      "\tTrain Loss: 0.033 | Train f1: 0.86 | Train precision: 0.88 | Train recall: 0.86\n",
      "\t Val. Loss: 0.209 |  Val. f1: 0.66 |  Val. precision: 0.70 | Val. recall: 0.66\n",
      "\n",
      "Performance of best found Model:\n",
      "Val. Loss: 0.166 |  Val. f1: 0.63 | Val. precision: 0.69 | Val. recall: 0.64\n"
     ]
    }
   ],
   "source": [
    "model = gru_model1\n",
    "model_name = gru_model_name1\n",
    "criterion = baseline_criterion\n",
    "\n",
    "model.apply(init_weights)\n",
    "print(f'El modelo actual tiene {count_parameters(model):,} parámetros entrenables.')\n",
    "\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "model = model.to(device)\n",
    "criterion = criterion.to(device)\n",
    "\n",
    "optimize_model(model, train_iterator, valid_iterator, optimizer, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "hi9HcklKC6GK",
    "outputId": "2ed78109-3aec-4841-dcf7-8cc130499976"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El modelo actual tiene 4,347,838 parámetros entrenables.\n",
      "Epoch: 01 | Epoch Time: 0m 10s\n",
      "\tTrain Loss: 0.615 | Train f1: 0.05 | Train precision: 0.14 | Train recall: 0.04\n",
      "\t Val. Loss: 0.439 |  Val. f1: 0.19 |  Val. precision: 0.36 | Val. recall: 0.14\n",
      "Epoch: 02 | Epoch Time: 0m 10s\n",
      "\tTrain Loss: 0.342 | Train f1: 0.29 | Train precision: 0.40 | Train recall: 0.25\n",
      "\t Val. Loss: 0.282 |  Val. f1: 0.42 |  Val. precision: 0.54 | Val. recall: 0.39\n",
      "Epoch: 03 | Epoch Time: 0m 10s\n",
      "\tTrain Loss: 0.233 | Train f1: 0.44 | Train precision: 0.53 | Train recall: 0.41\n",
      "\t Val. Loss: 0.223 |  Val. f1: 0.51 |  Val. precision: 0.60 | Val. recall: 0.49\n",
      "Epoch: 04 | Epoch Time: 0m 10s\n",
      "\tTrain Loss: 0.179 | Train f1: 0.54 | Train precision: 0.62 | Train recall: 0.52\n",
      "\t Val. Loss: 0.216 |  Val. f1: 0.55 |  Val. precision: 0.64 | Val. recall: 0.52\n",
      "Epoch: 05 | Epoch Time: 0m 10s\n",
      "\tTrain Loss: 0.144 | Train f1: 0.59 | Train precision: 0.66 | Train recall: 0.58\n",
      "\t Val. Loss: 0.200 |  Val. f1: 0.57 |  Val. precision: 0.65 | Val. recall: 0.55\n",
      "Epoch: 06 | Epoch Time: 0m 10s\n",
      "\tTrain Loss: 0.122 | Train f1: 0.64 | Train precision: 0.69 | Train recall: 0.62\n",
      "\t Val. Loss: 0.210 |  Val. f1: 0.58 |  Val. precision: 0.66 | Val. recall: 0.56\n",
      "Epoch: 07 | Epoch Time: 0m 10s\n",
      "\tTrain Loss: 0.106 | Train f1: 0.67 | Train precision: 0.72 | Train recall: 0.66\n",
      "\t Val. Loss: 0.192 |  Val. f1: 0.60 |  Val. precision: 0.67 | Val. recall: 0.59\n",
      "Epoch: 08 | Epoch Time: 0m 10s\n",
      "\tTrain Loss: 0.091 | Train f1: 0.70 | Train precision: 0.74 | Train recall: 0.69\n",
      "\t Val. Loss: 0.196 |  Val. f1: 0.61 |  Val. precision: 0.68 | Val. recall: 0.60\n",
      "Epoch: 09 | Epoch Time: 0m 10s\n",
      "\tTrain Loss: 0.084 | Train f1: 0.72 | Train precision: 0.76 | Train recall: 0.71\n",
      "\t Val. Loss: 0.189 |  Val. f1: 0.63 |  Val. precision: 0.69 | Val. recall: 0.62\n",
      "Epoch: 10 | Epoch Time: 0m 10s\n",
      "\tTrain Loss: 0.076 | Train f1: 0.75 | Train precision: 0.78 | Train recall: 0.74\n",
      "\t Val. Loss: 0.174 |  Val. f1: 0.64 |  Val. precision: 0.70 | Val. recall: 0.64\n",
      "Epoch: 11 | Epoch Time: 0m 10s\n",
      "\tTrain Loss: 0.068 | Train f1: 0.76 | Train precision: 0.79 | Train recall: 0.75\n",
      "\t Val. Loss: 0.200 |  Val. f1: 0.62 |  Val. precision: 0.69 | Val. recall: 0.62\n",
      "Epoch: 12 | Epoch Time: 0m 10s\n",
      "\tTrain Loss: 0.062 | Train f1: 0.78 | Train precision: 0.81 | Train recall: 0.77\n",
      "\t Val. Loss: 0.210 |  Val. f1: 0.63 |  Val. precision: 0.70 | Val. recall: 0.62\n",
      "Epoch: 13 | Epoch Time: 0m 10s\n",
      "\tTrain Loss: 0.056 | Train f1: 0.79 | Train precision: 0.82 | Train recall: 0.78\n",
      "\t Val. Loss: 0.215 |  Val. f1: 0.63 |  Val. precision: 0.70 | Val. recall: 0.61\n",
      "Epoch: 14 | Epoch Time: 0m 10s\n",
      "\tTrain Loss: 0.055 | Train f1: 0.79 | Train precision: 0.82 | Train recall: 0.79\n",
      "\t Val. Loss: 0.200 |  Val. f1: 0.64 |  Val. precision: 0.70 | Val. recall: 0.63\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/numpy/lib/function_base.py:393: RuntimeWarning: Mean of empty slice.\n",
      "  avg = a.mean(axis)\n",
      "/usr/local/lib/python3.6/dist-packages/numpy/core/_methods.py:161: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 15 | Epoch Time: 0m 10s\n",
      "\tTrain Loss: 0.050 | Train f1: nan | Train precision: nan | Train recall: nan\n",
      "\t Val. Loss: 0.206 |  Val. f1: 0.64 |  Val. precision: 0.70 | Val. recall: 0.64\n",
      "Epoch: 16 | Epoch Time: 0m 10s\n",
      "\tTrain Loss: 0.047 | Train f1: 0.82 | Train precision: 0.84 | Train recall: 0.82\n",
      "\t Val. Loss: 0.205 |  Val. f1: 0.64 |  Val. precision: 0.70 | Val. recall: 0.63\n",
      "Epoch: 17 | Epoch Time: 0m 10s\n",
      "\tTrain Loss: 0.044 | Train f1: 0.82 | Train precision: 0.84 | Train recall: 0.82\n",
      "\t Val. Loss: 0.227 |  Val. f1: 0.64 |  Val. precision: 0.70 | Val. recall: 0.62\n",
      "Epoch: 18 | Epoch Time: 0m 10s\n",
      "\tTrain Loss: 0.042 | Train f1: 0.83 | Train precision: 0.85 | Train recall: 0.83\n",
      "\t Val. Loss: 0.228 |  Val. f1: 0.63 |  Val. precision: 0.70 | Val. recall: 0.62\n",
      "Epoch: 19 | Epoch Time: 0m 10s\n",
      "\tTrain Loss: 0.039 | Train f1: 0.84 | Train precision: 0.86 | Train recall: 0.83\n",
      "\t Val. Loss: 0.218 |  Val. f1: 0.64 |  Val. precision: 0.70 | Val. recall: 0.63\n",
      "Epoch: 20 | Epoch Time: 0m 10s\n",
      "\tTrain Loss: 0.036 | Train f1: 0.85 | Train precision: 0.87 | Train recall: 0.84\n",
      "\t Val. Loss: 0.231 |  Val. f1: 0.64 |  Val. precision: 0.70 | Val. recall: 0.63\n",
      "\n",
      "Performance of best found Model:\n",
      "Val. Loss: 0.174 |  Val. f1: 0.64 | Val. precision: 0.70 | Val. recall: 0.64\n"
     ]
    }
   ],
   "source": [
    "model = gru_model2\n",
    "model_name = gru_model_name2\n",
    "criterion = baseline_criterion\n",
    "\n",
    "model.apply(init_weights)\n",
    "print(f'El modelo actual tiene {count_parameters(model):,} parámetros entrenables.')\n",
    "\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "model = model.to(device)\n",
    "criterion = criterion.to(device)\n",
    "\n",
    "optimize_model(model, train_iterator, valid_iterator, optimizer, criterion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FGambe2zNs7G"
   },
   "source": [
    "Si bien en algunos casos existen algunas mejoras en desempeño al incluir esta tercera componente de *dropout*, estas suelen ir acompañadas de un deterioro en otros aspectos, generalmente *loss*. Por lo tanto, se concluye que es preferible no considerar esta componente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QvKMQHrBOGys"
   },
   "source": [
    "#### Capa de Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vvAPBTqbOMhM"
   },
   "source": [
    "En esta sección se explora cómo mejorar el desempeño de los modelos por medio de los Embeddings. Para esto se conciben dos enfoques: entrenar la capa de Embedding de los modelos ya empleados, variando la dimensionalidad de las representaciones, o bien emplear Embeddings pre-entrenados (obtenidos desde https://github.com/BotCenter/spanishWordEmbeddings). Para comparar en mejor manera el efecto de los Embeddings pre-entrenados se consideran arquitecturas cuya capa de Embedding tiene la misma dimensionalidad que los obtenidos desde el enlace anterior.\n",
    "\n",
    "Además, dado que el modelo pre-entrenado se puede cargar directamente en la capa de Embedding de nuestros modelos, se pueden seguir entrenando, por lo que también se tendrá esto en cuenta.\n",
    "\n",
    "Para poder cargar los Embeddings pre-entrenados en nuestros modelos se realiza una leve modificación:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KgqVtjM_80hp"
   },
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9HOzEWavKDn7"
   },
   "outputs": [],
   "source": [
    "class NER_RNN(nn.Module):\n",
    "    def __init__(self, \n",
    "                 input_dim, \n",
    "                 embedding_dim, \n",
    "                 hidden_dim, \n",
    "                 output_dim,\n",
    "                 n_layers, \n",
    "                 bidirectional, \n",
    "                 in_dropout,\n",
    "                 rnn_dropout,\n",
    "                 out_dropout, \n",
    "                 pad_idx):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        # Capa de embedding\n",
    "        self.embedding = nn.Embedding(input_dim,\n",
    "                                      embedding_dim,\n",
    "                                      padding_idx=pad_idx)\n",
    "\n",
    "        # Capa LSTM\n",
    "        self.lstm = nn.LSTM(embedding_dim,\n",
    "                           hidden_dim,\n",
    "                           num_layers=n_layers,\n",
    "                           bidirectional=bidirectional, \n",
    "                           dropout = rnn_dropout if n_layers > 1 else 0)\n",
    "\n",
    "        # Capa de salida\n",
    "        self.fc = nn.Linear(hidden_dim * 2 if bidirectional else hidden_dim,\n",
    "                            output_dim)\n",
    "\n",
    "        # Dropout\n",
    "        self.in_dropout = nn.Dropout(in_dropout)\n",
    "        self.out_dropout = nn.Dropout(out_dropout)\n",
    "\n",
    "    def forward(self, text):\n",
    "\n",
    "        #text = [sent len, batch size]\n",
    "\n",
    "        # Convertir lo enviado a embedding\n",
    "        embedded = self.in_dropout(self.embedding(text))\n",
    "        \n",
    "        outputs, (hidden, cell) = self.lstm(embedded)\n",
    "        #embedded = [sent len, batch size, emb dim]\n",
    "\n",
    "        # Pasar los embeddings por la rnn (LSTM)\n",
    "\n",
    "        #output = [sent len, batch size, hid dim * n directions]\n",
    "        #hidden/cell = [n layers * n directions, batch size, hid dim]\n",
    "\n",
    "        # Predecir usando la capa de salida.\n",
    "        predictions = self.fc(self.out_dropout(outputs))\n",
    "        #predictions = [sent len, batch size, output dim]\n",
    "\n",
    "        return predictions\n",
    "    \n",
    "    def load_pretrained_embeddings(self, pre_trained_emb, requieres_grad):\n",
    "        self.embedding = nn.Embedding.from_pretrained(pre_trained_emb).to(device)\n",
    "        self.embedding.weight.requires_grad = requieres_grad\n",
    "\n",
    "class NER_ELMAN(nn.Module):\n",
    "    def __init__(self, \n",
    "                 input_dim, \n",
    "                 embedding_dim, \n",
    "                 hidden_dim, \n",
    "                 output_dim,\n",
    "                 n_layers, \n",
    "                 bidirectional, \n",
    "                 in_dropout,\n",
    "                 rnn_dropout,\n",
    "                 out_dropout, \n",
    "                 pad_idx,\n",
    "                 nonlinearity = 'tanh'):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        # Capa de embedding\n",
    "        self.embedding = nn.Embedding(input_dim,\n",
    "                                      embedding_dim,\n",
    "                                      padding_idx=pad_idx)\n",
    "\n",
    "        # Capa Elman RNN\n",
    "        self.rnn = nn.RNN(embedding_dim,\n",
    "                           hidden_dim,\n",
    "                           num_layers=n_layers,\n",
    "                           bidirectional=bidirectional, \n",
    "                           dropout = rnn_dropout if n_layers > 1 else 0,\n",
    "                           nonlinearity = nonlinearity)\n",
    "\n",
    "        # Capa de salida\n",
    "        self.fc = nn.Linear(hidden_dim * 2 if bidirectional else hidden_dim,\n",
    "                            output_dim)\n",
    "\n",
    "        # Dropout\n",
    "        self.in_dropout = nn.Dropout(in_dropout)\n",
    "        self.out_dropout = nn.Dropout(out_dropout)\n",
    "\n",
    "    def forward(self, text):\n",
    "\n",
    "        #text = [sent len, batch size]\n",
    "\n",
    "        # Convertir lo enviado a embedding\n",
    "        embedded = self.in_dropout(self.embedding(text))\n",
    "        \n",
    "        outputs, hidden = self.rnn(embedded)\n",
    "        #embedded = [sent len, batch size, emb dim]\n",
    "\n",
    "        # Pasar los embeddings por la rnn (LSTM)\n",
    "\n",
    "        #output = [sent len, batch size, hid dim * n directions]\n",
    "        #hidden/cell = [n layers * n directions, batch size, hid dim]\n",
    "\n",
    "        # Predecir usando la capa de salida.\n",
    "        predictions = self.fc(self.out_dropout(outputs))\n",
    "        #predictions = [sent len, batch size, output dim]\n",
    "\n",
    "        return predictions\n",
    "    \n",
    "    def load_pretrained_embeddings(self, pre_trained_emb, requieres_grad):\n",
    "        self.embedding = nn.Embedding.from_pretrained(pre_trained_emb).to(device)\n",
    "        self.embedding.weight.requires_grad = requieres_grad\n",
    "\n",
    "class NER_GRU(nn.Module):\n",
    "    def __init__(self, \n",
    "                 input_dim, \n",
    "                 embedding_dim, \n",
    "                 hidden_dim, \n",
    "                 output_dim,\n",
    "                 n_layers, \n",
    "                 bidirectional, \n",
    "                 in_dropout,\n",
    "                 rnn_dropout,\n",
    "                 out_dropout, \n",
    "                 pad_idx):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        # Capa de embedding\n",
    "        self.embedding = nn.Embedding(input_dim,\n",
    "                                      embedding_dim,\n",
    "                                      padding_idx=pad_idx)\n",
    "\n",
    "        # Capa Elman RNN\n",
    "        self.gru = nn.GRU(embedding_dim,\n",
    "                           hidden_dim,\n",
    "                           num_layers=n_layers,\n",
    "                           bidirectional=bidirectional, \n",
    "                           dropout = rnn_dropout if n_layers > 1 else 0)\n",
    "\n",
    "        # Capa de salida\n",
    "        self.fc = nn.Linear(hidden_dim * 2 if bidirectional else hidden_dim,\n",
    "                            output_dim)\n",
    "\n",
    "        # Dropout\n",
    "        self.in_dropout = nn.Dropout(in_dropout)\n",
    "        self.out_dropout = nn.Dropout(out_dropout)\n",
    "\n",
    "    def forward(self, text):\n",
    "\n",
    "        #text = [sent len, batch size]\n",
    "\n",
    "        # Convertir lo enviado a embedding\n",
    "        embedded = self.in_dropout(self.embedding(text))\n",
    "        \n",
    "        outputs, hidden = self.gru(embedded)\n",
    "        #embedded = [sent len, batch size, emb dim]\n",
    "\n",
    "        # Pasar los embeddings por la rnn (LSTM)\n",
    "\n",
    "        #output = [sent len, batch size, hid dim * n directions]\n",
    "        #hidden/cell = [n layers * n directions, batch size, hid dim]\n",
    "\n",
    "        # Predecir usando la capa de salida.\n",
    "        predictions = self.fc(self.out_dropout(outputs))\n",
    "        #predictions = [sent len, batch size, output dim]\n",
    "\n",
    "        return predictions\n",
    "\n",
    "    def load_pretrained_embeddings(self, pre_trained_emb, requieres_grad):\n",
    "        self.embedding = nn.Embedding.from_pretrained(pre_trained_emb).to(device)\n",
    "        self.embedding.weight.requires_grad = requieres_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 119
    },
    "colab_type": "code",
    "id": "AVy5Hd_XvNyj",
    "outputId": "2f211847-5587-4c19-a8b6-ba7427ba9346"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numero de ejemplos de entrenamiento: 8323\n",
      "Número de ejemplos de validación: 1915\n",
      "Número de ejemplos de test (competencia): 1517\n",
      "Tokens únicos en TEXT: 26101\n",
      "Tokens únicos en NER_TAGS: 10\n",
      "Using cuda\n"
     ]
    }
   ],
   "source": [
    "## Recuperamos estas líneas por simplicidad\n",
    "\n",
    "# Primer Field: TEXT. Representan los tokens de la secuencia\n",
    "TEXT = data.Field(lower=False) \n",
    "\n",
    "# Segundo Field: NER_TAGS. Representan los Tags asociados a cada palabra.\n",
    "NER_TAGS = data.Field(unk_token=None)\n",
    "\n",
    "fields = ((\"text\", TEXT), (\"nertags\", NER_TAGS))\n",
    "\n",
    "train_data, valid_data, test_data = datasets.SequenceTaggingDataset.splits(\n",
    "    path=\"./\",\n",
    "    train=\"train_NER_esp.txt\",\n",
    "    validation=\"val_NER_esp.txt\",\n",
    "    test=\"test_NER_esp.txt\",\n",
    "    fields=fields,\n",
    "    encoding=\"iso-8859-1\",\n",
    "    separator=\" \"\n",
    ")\n",
    "\n",
    "print(f\"Numero de ejemplos de entrenamiento: {len(train_data)}\")\n",
    "print(f\"Número de ejemplos de validación: {len(valid_data)}\")\n",
    "print(f\"Número de ejemplos de test (competencia): {len(test_data)}\")\n",
    "\n",
    "TEXT.build_vocab(train_data)\n",
    "NER_TAGS.build_vocab(train_data)\n",
    "\n",
    "print(f\"Tokens únicos en TEXT: {len(TEXT.vocab)}\")\n",
    "print(f\"Tokens únicos en NER_TAGS: {len(NER_TAGS.vocab)}\")\n",
    "\n",
    "BATCH_SIZE = 16  # disminuir si hay problemas de ram.\n",
    "\n",
    "# Usar cuda si es que está disponible.\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using', device)\n",
    "\n",
    "# Dividir datos entre entrenamiento y test\n",
    "train_iterator, valid_iterator, test_iterator = data.BucketIterator.splits(\n",
    "    (train_data, valid_data, test_data),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    device=device,\n",
    "    sort=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GayTma1CwA_n"
   },
   "outputs": [],
   "source": [
    "%%script false\n",
    "for batch in train_iterator:\n",
    "    print(batch.text)\n",
    "    print(batch.text.shape)\n",
    "    print(model.embedding(batch.text).shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 275
    },
    "colab_type": "code",
    "id": "pgTZBuO0s0hL",
    "outputId": "f4463696-e30c-4a58-ab26-793b0e2d9876"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: cannot remove 'pretrained_model.bin': No such file or directory\n",
      "--2020-07-31 03:12:02--  https://zenodo.org/record/3255001/files/embeddings-new_large-general_3B_fasttext.vec\n",
      "Resolving zenodo.org (zenodo.org)... 188.184.117.155\n",
      "Connecting to zenodo.org (zenodo.org)|188.184.117.155|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 3801508151 (3.5G) [application/octet-stream]\n",
      "Saving to: ‘pretrained_model.vec’\n",
      "\n",
      "pretrained_model.ve 100%[===================>]   3.54G  40.5MB/s    in 65s     \n",
      "\n",
      "2020-07-31 03:13:07 (56.1 MB/s) - ‘pretrained_model.vec’ saved [3801508151/3801508151]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:254: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
      "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
     ]
    }
   ],
   "source": [
    "import torchtext.vocab as vocab\n",
    "from tqdm import tqdm_notebook\n",
    "from gensim.models.wrappers import FastText\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "\n",
    "filename = 'pretrained_model.bin'\n",
    "!rm $filename\n",
    "filename = 'pretrained_model.vec'\n",
    "!rm $filename\n",
    "\n",
    "pretrained_dict = {0: 'XS',\n",
    "                   1: 'S',\n",
    "                   2: 'M',\n",
    "                   3: 'L',\n",
    "                   4: 'new L'}\n",
    "\n",
    "loadKey = 4\n",
    "\n",
    "if pretrained_dict[loadKey] == 'XS':\n",
    "    filename = 'pretrained_model.bin'\n",
    "    !wget -O $filename https://zenodo.org/record/3234051/files/embeddings-xs-model.bin -nc\n",
    "    W2V_SIZE = 10\n",
    "\n",
    "elif pretrained_dict[loadKey] == 'S':\n",
    "    filename = 'pretrained_model.bin'\n",
    "    !wget -O $filename https://zenodo.org/record/3234051/files/embeddings-s-model.bin  -nc\n",
    "    W2V_SIZE = 30\n",
    "\n",
    "elif pretrained_dict[loadKey] == 'M':\n",
    "    filename = 'pretrained_model.vec'\n",
    "    #!wget -O $filename https://zenodo.org/record/3234051/files/embeddings-m-model.bin  -nc\n",
    "    !wget -O $filename https://zenodo.org/record/3234051/files/embeddings-m-model.vec  -nc\n",
    "    W2V_SIZE = 100\n",
    "\n",
    "elif pretrained_dict[loadKey] == 'L':\n",
    "    filename = 'pretrained_model.vec'\n",
    "    #!wget -O $filename https://zenodo.org/record/3234051/files/embeddings-l-model.bin  -nc\n",
    "    !wget -O $filename https://zenodo.org/record/3234051/files/embeddings-l-model.vec  -nc\n",
    "    W2V_SIZE = 300\n",
    "\n",
    "elif pretrained_dict[loadKey] == 'new L':\n",
    "    filename = 'pretrained_model.vec'\n",
    "    #!wget -O $filename https://zenodo.org/record/3255001/files/embeddings-new_large-general_3B_fasttext.bin  -nc\n",
    "    !wget -O $filename https://zenodo.org/record/3255001/files/embeddings-new_large-general_3B_fasttext.vec  -nc\n",
    "    W2V_SIZE = 300\n",
    "\n",
    "if W2V_SIZE <= 30:\n",
    "    wordvectors_file = 'pretrained_model'\n",
    "    wordvectors = FastText.load_fasttext_format(wordvectors_file)\n",
    "\n",
    "else:\n",
    "    wordvectors_file = 'pretrained_model.vec'\n",
    "    wordvectors = KeyedVectors.load_word2vec_format(wordvectors_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "v9RAdiHDMrAC"
   },
   "source": [
    "##### Dimensión del Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "jGPJrN6EuNHf",
    "outputId": "ee3c2a58-f4cf-4ab8-cbd7-bd701a615fa8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300\n"
     ]
    }
   ],
   "source": [
    "## Parámetros del Experimento ##\n",
    "\n",
    "# tamaño del vocabulario. recuerden que la entrada son vectores bag of word(one-hot).\n",
    "INPUT_DIM = len(TEXT.vocab)\n",
    "EMBEDDING_DIM = W2V_SIZE  # dimensión de los embeddings.\n",
    "print(EMBEDDING_DIM)\n",
    "HIDDEN_DIM = 256  # dimensión de la capas RNN\n",
    "OUTPUT_DIM = len(NER_TAGS.vocab)  # número de clases\n",
    "\n",
    "N_LAYERS = 2  # número de capas.\n",
    "IN_DROPOUT = 0.75\n",
    "RNN_DROPOUT = 0.0\n",
    "OUT_DROPOUT = 0.0\n",
    "BIDIRECTIONAL = True\n",
    "\n",
    "TAG_PAD_IDX = NER_TAGS.vocab.stoi[NER_TAGS.pad_token]\n",
    "baseline_criterion = nn.CrossEntropyLoss(ignore_index = TAG_PAD_IDX)\n",
    "\n",
    "# Creamos nuestros modelos.\n",
    "lstm_model = NER_RNN(INPUT_DIM, EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM,\n",
    "                     N_LAYERS, BIDIRECTIONAL, IN_DROPOUT, RNN_DROPOUT, \n",
    "                     OUT_DROPOUT, PAD_IDX)\n",
    "\n",
    "lstm_model_name = 'LSTM'  # nombre que tendrá el modelo guardado...\n",
    "\n",
    "elman_model = NER_ELMAN(INPUT_DIM, EMBEDDING_DIM, int(HIDDEN_DIM/2), OUTPUT_DIM,\n",
    "                        N_LAYERS, BIDIRECTIONAL, IN_DROPOUT, RNN_DROPOUT, \n",
    "                        OUT_DROPOUT, PAD_IDX, 'relu')\n",
    "\n",
    "elman_model_name = 'Elman'  # nombre que tendrá el modelo guardado...\n",
    "\n",
    "gru_model1 = NER_GRU(INPUT_DIM, EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM,\n",
    "                    N_LAYERS, BIDIRECTIONAL, IN_DROPOUT, RNN_DROPOUT, \n",
    "                    OUT_DROPOUT, PAD_IDX)\n",
    "\n",
    "gru_model_name1 = 'GRU'  # nombre que tendrá el modelo guardado...\n",
    "\n",
    "gru_model2 = NER_GRU(INPUT_DIM, EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM,\n",
    "                    N_LAYERS, BIDIRECTIONAL, IN_DROPOUT, 0.5, \n",
    "                    OUT_DROPOUT, PAD_IDX)\n",
    "\n",
    "gru_model_name2 = 'GRU'  # nombre que tendrá el modelo guardado..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 799
    },
    "colab_type": "code",
    "id": "ZEsDe9fzu8QU",
    "outputId": "58de3929-5875-4ebc-f873-51b3fd7b4e35"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El modelo actual tiene 10,555,174 parámetros entrenables.\n",
      "Epoch: 01 | Epoch Time: 0m 10s\n",
      "\tTrain Loss: 0.377 | Train f1: 0.24 | Train precision: 0.35 | Train recall: 0.21\n",
      "\t Val. Loss: 0.287 |  Val. f1: 0.44 |  Val. precision: 0.52 | Val. recall: 0.44\n",
      "Epoch: 02 | Epoch Time: 0m 10s\n",
      "\tTrain Loss: 0.176 | Train f1: 0.54 | Train precision: 0.63 | Train recall: 0.51\n",
      "\t Val. Loss: 0.198 |  Val. f1: 0.55 |  Val. precision: 0.63 | Val. recall: 0.55\n",
      "Epoch: 03 | Epoch Time: 0m 10s\n",
      "\tTrain Loss: 0.113 | Train f1: 0.65 | Train precision: 0.71 | Train recall: 0.64\n",
      "\t Val. Loss: 0.171 |  Val. f1: 0.61 |  Val. precision: 0.67 | Val. recall: 0.60\n",
      "Epoch: 04 | Epoch Time: 0m 10s\n",
      "\tTrain Loss: 0.081 | Train f1: 0.72 | Train precision: 0.76 | Train recall: 0.71\n",
      "\t Val. Loss: 0.161 |  Val. f1: 0.65 |  Val. precision: 0.69 | Val. recall: 0.66\n",
      "Epoch: 05 | Epoch Time: 0m 10s\n",
      "\tTrain Loss: 0.061 | Train f1: 0.76 | Train precision: 0.79 | Train recall: 0.76\n",
      "\t Val. Loss: 0.176 |  Val. f1: 0.63 |  Val. precision: 0.69 | Val. recall: 0.62\n",
      "Epoch: 06 | Epoch Time: 0m 10s\n",
      "\tTrain Loss: 0.049 | Train f1: 0.80 | Train precision: 0.82 | Train recall: 0.79\n",
      "\t Val. Loss: 0.184 |  Val. f1: 0.64 |  Val. precision: 0.70 | Val. recall: 0.64\n",
      "Epoch: 07 | Epoch Time: 0m 10s\n",
      "\tTrain Loss: 0.039 | Train f1: 0.83 | Train precision: 0.85 | Train recall: 0.83\n",
      "\t Val. Loss: 0.190 |  Val. f1: 0.65 |  Val. precision: 0.70 | Val. recall: 0.66\n",
      "Epoch: 08 | Epoch Time: 0m 10s\n",
      "\tTrain Loss: 0.031 | Train f1: 0.85 | Train precision: 0.87 | Train recall: 0.85\n",
      "\t Val. Loss: 0.189 |  Val. f1: 0.66 |  Val. precision: 0.71 | Val. recall: 0.66\n",
      "Epoch: 09 | Epoch Time: 0m 10s\n",
      "\tTrain Loss: 0.027 | Train f1: 0.87 | Train precision: 0.89 | Train recall: 0.87\n",
      "\t Val. Loss: 0.200 |  Val. f1: 0.65 |  Val. precision: 0.69 | Val. recall: 0.65\n",
      "Epoch: 10 | Epoch Time: 0m 10s\n",
      "\tTrain Loss: 0.023 | Train f1: 0.89 | Train precision: 0.90 | Train recall: 0.88\n",
      "\t Val. Loss: 0.199 |  Val. f1: 0.66 |  Val. precision: 0.70 | Val. recall: 0.67\n",
      "Epoch: 11 | Epoch Time: 0m 10s\n",
      "\tTrain Loss: 0.019 | Train f1: 0.91 | Train precision: 0.92 | Train recall: 0.90\n",
      "\t Val. Loss: 0.193 |  Val. f1: 0.67 |  Val. precision: 0.70 | Val. recall: 0.68\n",
      "Epoch: 12 | Epoch Time: 0m 10s\n",
      "\tTrain Loss: 0.017 | Train f1: 0.91 | Train precision: 0.92 | Train recall: 0.91\n",
      "\t Val. Loss: 0.205 |  Val. f1: 0.66 |  Val. precision: 0.70 | Val. recall: 0.66\n",
      "Epoch: 13 | Epoch Time: 0m 10s\n",
      "\tTrain Loss: 0.014 | Train f1: 0.92 | Train precision: 0.93 | Train recall: 0.92\n",
      "\t Val. Loss: 0.211 |  Val. f1: 0.66 |  Val. precision: 0.70 | Val. recall: 0.67\n",
      "Epoch: 14 | Epoch Time: 0m 10s\n",
      "\tTrain Loss: 0.015 | Train f1: 0.92 | Train precision: 0.93 | Train recall: 0.92\n",
      "\t Val. Loss: 0.224 |  Val. f1: 0.65 |  Val. precision: 0.69 | Val. recall: 0.67\n",
      "\n",
      "Performance of best found Model:\n",
      "Val. Loss: 0.161 |  Val. f1: 0.65 | Val. precision: 0.69 | Val. recall: 0.66\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "model = lstm_model\n",
    "model_name = lstm_model_name\n",
    "criterion = baseline_criterion\n",
    "\n",
    "model.apply(init_weights)\n",
    "print(f'El modelo actual tiene {count_parameters(model):,} parámetros entrenables.')\n",
    "\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "model = model.to(device)\n",
    "criterion = criterion.to(device)\n",
    "\n",
    "optimize_model(model, train_iterator, valid_iterator, optimizer, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "4P42J9xZvAhI",
    "outputId": "118605fe-00d8-47eb-9ebc-df7ad1888bdd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El modelo actual tiene 8,041,766 parámetros entrenables.\n",
      "Epoch: 01 | Epoch Time: 0m 9s\n",
      "\tTrain Loss: 0.452 | Train f1: 0.15 | Train precision: 0.24 | Train recall: 0.13\n",
      "\t Val. Loss: 0.318 |  Val. f1: 0.38 |  Val. precision: 0.45 | Val. recall: 0.38\n",
      "Epoch: 02 | Epoch Time: 0m 9s\n",
      "\tTrain Loss: 0.220 | Train f1: 0.43 | Train precision: 0.53 | Train recall: 0.41\n",
      "\t Val. Loss: 0.266 |  Val. f1: 0.48 |  Val. precision: 0.55 | Val. recall: 0.51\n",
      "Epoch: 03 | Epoch Time: 0m 9s\n",
      "\tTrain Loss: 0.148 | Train f1: 0.57 | Train precision: 0.64 | Train recall: 0.56\n",
      "\t Val. Loss: 0.195 |  Val. f1: 0.57 |  Val. precision: 0.62 | Val. recall: 0.59\n",
      "Epoch: 04 | Epoch Time: 0m 9s\n",
      "\tTrain Loss: 0.109 | Train f1: 0.65 | Train precision: 0.70 | Train recall: 0.65\n",
      "\t Val. Loss: 0.182 |  Val. f1: 0.60 |  Val. precision: 0.65 | Val. recall: 0.62\n",
      "Epoch: 05 | Epoch Time: 0m 9s\n",
      "\tTrain Loss: 0.085 | Train f1: 0.71 | Train precision: 0.75 | Train recall: 0.70\n",
      "\t Val. Loss: 0.192 |  Val. f1: 0.61 |  Val. precision: 0.65 | Val. recall: 0.63\n",
      "Epoch: 06 | Epoch Time: 0m 9s\n",
      "\tTrain Loss: 0.070 | Train f1: 0.74 | Train precision: 0.78 | Train recall: 0.74\n",
      "\t Val. Loss: 0.189 |  Val. f1: 0.60 |  Val. precision: 0.65 | Val. recall: 0.62\n",
      "Epoch: 07 | Epoch Time: 0m 9s\n",
      "\tTrain Loss: 0.058 | Train f1: 0.77 | Train precision: 0.80 | Train recall: 0.77\n",
      "\t Val. Loss: 0.200 |  Val. f1: 0.61 |  Val. precision: 0.66 | Val. recall: 0.64\n",
      "Epoch: 08 | Epoch Time: 0m 9s\n",
      "\tTrain Loss: 0.049 | Train f1: 0.81 | Train precision: 0.83 | Train recall: 0.80\n",
      "\t Val. Loss: 0.171 |  Val. f1: 0.65 |  Val. precision: 0.69 | Val. recall: 0.66\n",
      "Epoch: 09 | Epoch Time: 0m 9s\n",
      "\tTrain Loss: 0.042 | Train f1: 0.82 | Train precision: 0.84 | Train recall: 0.82\n",
      "\t Val. Loss: 0.197 |  Val. f1: 0.62 |  Val. precision: 0.67 | Val. recall: 0.64\n",
      "Epoch: 10 | Epoch Time: 0m 9s\n",
      "\tTrain Loss: 0.036 | Train f1: 0.84 | Train precision: 0.86 | Train recall: 0.84\n",
      "\t Val. Loss: 0.181 |  Val. f1: 0.65 |  Val. precision: 0.69 | Val. recall: 0.66\n",
      "Epoch: 11 | Epoch Time: 0m 9s\n",
      "\tTrain Loss: 0.033 | Train f1: 0.85 | Train precision: 0.87 | Train recall: 0.85\n",
      "\t Val. Loss: 0.180 |  Val. f1: 0.65 |  Val. precision: 0.68 | Val. recall: 0.66\n",
      "Epoch: 12 | Epoch Time: 0m 9s\n",
      "\tTrain Loss: 0.029 | Train f1: 0.86 | Train precision: 0.88 | Train recall: 0.86\n",
      "\t Val. Loss: 0.180 |  Val. f1: 0.66 |  Val. precision: 0.70 | Val. recall: 0.67\n",
      "Epoch: 13 | Epoch Time: 0m 9s\n",
      "\tTrain Loss: 0.026 | Train f1: 0.87 | Train precision: 0.89 | Train recall: 0.87\n",
      "\t Val. Loss: 0.184 |  Val. f1: 0.66 |  Val. precision: 0.69 | Val. recall: 0.67\n",
      "Epoch: 14 | Epoch Time: 0m 9s\n",
      "\tTrain Loss: 0.024 | Train f1: 0.88 | Train precision: 0.90 | Train recall: 0.88\n",
      "\t Val. Loss: 0.207 |  Val. f1: 0.66 |  Val. precision: 0.70 | Val. recall: 0.67\n",
      "Epoch: 15 | Epoch Time: 0m 9s\n",
      "\tTrain Loss: 0.023 | Train f1: 0.89 | Train precision: 0.90 | Train recall: 0.89\n",
      "\t Val. Loss: 0.197 |  Val. f1: 0.66 |  Val. precision: 0.70 | Val. recall: 0.67\n",
      "Epoch: 16 | Epoch Time: 0m 9s\n",
      "\tTrain Loss: 0.022 | Train f1: 0.90 | Train precision: 0.91 | Train recall: 0.90\n",
      "\t Val. Loss: 0.207 |  Val. f1: 0.63 |  Val. precision: 0.68 | Val. recall: 0.64\n",
      "Epoch: 17 | Epoch Time: 0m 9s\n",
      "\tTrain Loss: 0.019 | Train f1: 0.90 | Train precision: 0.91 | Train recall: 0.90\n",
      "\t Val. Loss: 0.201 |  Val. f1: 0.65 |  Val. precision: 0.70 | Val. recall: 0.66\n",
      "Epoch: 18 | Epoch Time: 0m 9s\n",
      "\tTrain Loss: 0.017 | Train f1: 0.91 | Train precision: 0.92 | Train recall: 0.91\n",
      "\t Val. Loss: 0.210 |  Val. f1: 0.66 |  Val. precision: 0.70 | Val. recall: 0.66\n",
      "\n",
      "Performance of best found Model:\n",
      "Val. Loss: 0.171 |  Val. f1: 0.65 | Val. precision: 0.69 | Val. recall: 0.66\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "model = elman_model\n",
    "model_name = elman_model_name\n",
    "criterion = baseline_criterion\n",
    "  \n",
    "model.apply(init_weights)\n",
    "print(f'El modelo actual tiene {count_parameters(model):,} parámetros entrenables.')\n",
    "\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "model = model.to(device)\n",
    "criterion = criterion.to(device)\n",
    "\n",
    "optimize_model(model, train_iterator, valid_iterator, optimizer, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 850
    },
    "colab_type": "code",
    "id": "fbhBZQOHvEjh",
    "outputId": "7701da61-a435-44c4-d9b5-7b8b70de61a3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El modelo actual tiene 9,875,238 parámetros entrenables.\n",
      "Epoch: 01 | Epoch Time: 0m 9s\n",
      "\tTrain Loss: 0.404 | Train f1: 0.23 | Train precision: 0.37 | Train recall: 0.19\n",
      "\t Val. Loss: 0.283 |  Val. f1: 0.44 |  Val. precision: 0.58 | Val. recall: 0.42\n",
      "Epoch: 02 | Epoch Time: 0m 9s\n",
      "\tTrain Loss: 0.196 | Train f1: 0.52 | Train precision: 0.62 | Train recall: 0.50\n",
      "\t Val. Loss: 0.205 |  Val. f1: 0.57 |  Val. precision: 0.65 | Val. recall: 0.56\n",
      "Epoch: 03 | Epoch Time: 0m 10s\n",
      "\tTrain Loss: 0.122 | Train f1: 0.64 | Train precision: 0.70 | Train recall: 0.62\n",
      "\t Val. Loss: 0.213 |  Val. f1: 0.57 |  Val. precision: 0.64 | Val. recall: 0.58\n",
      "Epoch: 04 | Epoch Time: 0m 9s\n",
      "\tTrain Loss: 0.088 | Train f1: 0.72 | Train precision: 0.76 | Train recall: 0.71\n",
      "\t Val. Loss: 0.192 |  Val. f1: 0.60 |  Val. precision: 0.68 | Val. recall: 0.59\n",
      "Epoch: 05 | Epoch Time: 0m 9s\n",
      "\tTrain Loss: 0.066 | Train f1: 0.75 | Train precision: 0.79 | Train recall: 0.74\n",
      "\t Val. Loss: 0.179 |  Val. f1: 0.63 |  Val. precision: 0.68 | Val. recall: 0.63\n",
      "Epoch: 06 | Epoch Time: 0m 9s\n",
      "\tTrain Loss: 0.051 | Train f1: 0.80 | Train precision: 0.82 | Train recall: 0.79\n",
      "\t Val. Loss: 0.195 |  Val. f1: 0.63 |  Val. precision: 0.69 | Val. recall: 0.63\n",
      "Epoch: 07 | Epoch Time: 0m 9s\n",
      "\tTrain Loss: 0.041 | Train f1: 0.82 | Train precision: 0.84 | Train recall: 0.82\n",
      "\t Val. Loss: 0.201 |  Val. f1: 0.63 |  Val. precision: 0.69 | Val. recall: 0.63\n",
      "Epoch: 08 | Epoch Time: 0m 9s\n",
      "\tTrain Loss: 0.035 | Train f1: 0.84 | Train precision: 0.86 | Train recall: 0.84\n",
      "\t Val. Loss: 0.193 |  Val. f1: 0.64 |  Val. precision: 0.69 | Val. recall: 0.64\n",
      "Epoch: 09 | Epoch Time: 0m 9s\n",
      "\tTrain Loss: 0.030 | Train f1: 0.86 | Train precision: 0.88 | Train recall: 0.86\n",
      "\t Val. Loss: 0.220 |  Val. f1: 0.63 |  Val. precision: 0.69 | Val. recall: 0.64\n",
      "Epoch: 10 | Epoch Time: 0m 9s\n",
      "\tTrain Loss: 0.025 | Train f1: 0.88 | Train precision: 0.90 | Train recall: 0.88\n",
      "\t Val. Loss: 0.217 |  Val. f1: 0.64 |  Val. precision: 0.70 | Val. recall: 0.64\n",
      "Epoch: 11 | Epoch Time: 0m 9s\n",
      "\tTrain Loss: 0.021 | Train f1: 0.89 | Train precision: 0.90 | Train recall: 0.89\n",
      "\t Val. Loss: 0.252 |  Val. f1: 0.63 |  Val. precision: 0.70 | Val. recall: 0.63\n",
      "Epoch: 12 | Epoch Time: 0m 9s\n",
      "\tTrain Loss: 0.020 | Train f1: 0.90 | Train precision: 0.91 | Train recall: 0.90\n",
      "\t Val. Loss: 0.227 |  Val. f1: 0.65 |  Val. precision: 0.70 | Val. recall: 0.64\n",
      "Epoch: 13 | Epoch Time: 0m 9s\n",
      "\tTrain Loss: 0.019 | Train f1: 0.91 | Train precision: 0.92 | Train recall: 0.91\n",
      "\t Val. Loss: 0.227 |  Val. f1: 0.66 |  Val. precision: 0.71 | Val. recall: 0.65\n",
      "Epoch: 14 | Epoch Time: 0m 9s\n",
      "\tTrain Loss: 0.017 | Train f1: 0.91 | Train precision: 0.92 | Train recall: 0.91\n",
      "\t Val. Loss: 0.230 |  Val. f1: 0.65 |  Val. precision: 0.70 | Val. recall: 0.65\n",
      "Epoch: 15 | Epoch Time: 0m 9s\n",
      "\tTrain Loss: 0.015 | Train f1: 0.92 | Train precision: 0.93 | Train recall: 0.92\n",
      "\t Val. Loss: 0.226 |  Val. f1: 0.66 |  Val. precision: 0.71 | Val. recall: 0.66\n",
      "\n",
      "Performance of best found Model:\n",
      "Val. Loss: 0.179 |  Val. f1: 0.63 | Val. precision: 0.68 | Val. recall: 0.63\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "model = gru_model1\n",
    "model_name = gru_model_name1\n",
    "criterion = baseline_criterion\n",
    "\n",
    "model.apply(init_weights)\n",
    "print(f'El modelo actual tiene {count_parameters(model):,} parámetros entrenables.')\n",
    "\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "model = model.to(device)\n",
    "criterion = criterion.to(device)\n",
    "\n",
    "optimize_model(model, train_iterator, valid_iterator, optimizer, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 938
    },
    "colab_type": "code",
    "id": "CPUqSQoQvKRl",
    "outputId": "9bac33e5-15ca-4926-8e50-9666fb81a26a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El modelo actual tiene 9,875,238 parámetros entrenables.\n",
      "Epoch: 01 | Epoch Time: 0m 11s\n",
      "\tTrain Loss: 0.438 | Train f1: 0.19 | Train precision: 0.32 | Train recall: 0.16\n",
      "\t Val. Loss: 0.298 |  Val. f1: 0.41 |  Val. precision: 0.54 | Val. recall: 0.37\n",
      "Epoch: 02 | Epoch Time: 0m 11s\n",
      "\tTrain Loss: 0.219 | Train f1: 0.47 | Train precision: 0.58 | Train recall: 0.45\n",
      "\t Val. Loss: 0.214 |  Val. f1: 0.54 |  Val. precision: 0.62 | Val. recall: 0.53\n",
      "Epoch: 03 | Epoch Time: 0m 11s\n",
      "\tTrain Loss: 0.142 | Train f1: 0.60 | Train precision: 0.67 | Train recall: 0.58\n",
      "\t Val. Loss: 0.198 |  Val. f1: 0.57 |  Val. precision: 0.65 | Val. recall: 0.57\n",
      "Epoch: 04 | Epoch Time: 0m 11s\n",
      "\tTrain Loss: 0.101 | Train f1: 0.68 | Train precision: 0.73 | Train recall: 0.67\n",
      "\t Val. Loss: 0.184 |  Val. f1: 0.62 |  Val. precision: 0.68 | Val. recall: 0.62\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/numpy/lib/function_base.py:393: RuntimeWarning: Mean of empty slice.\n",
      "  avg = a.mean(axis)\n",
      "/usr/local/lib/python3.6/dist-packages/numpy/core/_methods.py:161: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 05 | Epoch Time: 0m 11s\n",
      "\tTrain Loss: 0.076 | Train f1: nan | Train precision: nan | Train recall: nan\n",
      "\t Val. Loss: 0.180 |  Val. f1: 0.63 |  Val. precision: 0.69 | Val. recall: 0.63\n",
      "Epoch: 06 | Epoch Time: 0m 11s\n",
      "\tTrain Loss: 0.062 | Train f1: 0.77 | Train precision: 0.80 | Train recall: 0.76\n",
      "\t Val. Loss: 0.185 |  Val. f1: 0.64 |  Val. precision: 0.69 | Val. recall: 0.65\n",
      "Epoch: 07 | Epoch Time: 0m 11s\n",
      "\tTrain Loss: 0.051 | Train f1: 0.79 | Train precision: 0.82 | Train recall: 0.78\n",
      "\t Val. Loss: 0.195 |  Val. f1: 0.64 |  Val. precision: 0.69 | Val. recall: 0.64\n",
      "Epoch: 08 | Epoch Time: 0m 11s\n",
      "\tTrain Loss: 0.043 | Train f1: 0.82 | Train precision: 0.84 | Train recall: 0.81\n",
      "\t Val. Loss: 0.199 |  Val. f1: 0.65 |  Val. precision: 0.69 | Val. recall: 0.66\n",
      "Epoch: 09 | Epoch Time: 0m 11s\n",
      "\tTrain Loss: 0.036 | Train f1: 0.84 | Train precision: 0.86 | Train recall: 0.84\n",
      "\t Val. Loss: 0.206 |  Val. f1: 0.66 |  Val. precision: 0.70 | Val. recall: 0.66\n",
      "Epoch: 10 | Epoch Time: 0m 11s\n",
      "\tTrain Loss: 0.031 | Train f1: 0.86 | Train precision: 0.88 | Train recall: 0.86\n",
      "\t Val. Loss: 0.208 |  Val. f1: 0.65 |  Val. precision: 0.70 | Val. recall: 0.65\n",
      "Epoch: 11 | Epoch Time: 0m 11s\n",
      "\tTrain Loss: 0.028 | Train f1: 0.87 | Train precision: 0.88 | Train recall: 0.87\n",
      "\t Val. Loss: 0.215 |  Val. f1: 0.65 |  Val. precision: 0.70 | Val. recall: 0.65\n",
      "Epoch: 12 | Epoch Time: 0m 11s\n",
      "\tTrain Loss: 0.025 | Train f1: 0.88 | Train precision: 0.89 | Train recall: 0.88\n",
      "\t Val. Loss: 0.233 |  Val. f1: 0.63 |  Val. precision: 0.69 | Val. recall: 0.63\n",
      "Epoch: 13 | Epoch Time: 0m 11s\n",
      "\tTrain Loss: 0.022 | Train f1: 0.89 | Train precision: 0.90 | Train recall: 0.89\n",
      "\t Val. Loss: 0.240 |  Val. f1: 0.65 |  Val. precision: 0.70 | Val. recall: 0.65\n",
      "Epoch: 14 | Epoch Time: 0m 11s\n",
      "\tTrain Loss: 0.021 | Train f1: 0.90 | Train precision: 0.91 | Train recall: 0.90\n",
      "\t Val. Loss: 0.250 |  Val. f1: 0.64 |  Val. precision: 0.70 | Val. recall: 0.64\n",
      "Epoch: 15 | Epoch Time: 0m 11s\n",
      "\tTrain Loss: 0.019 | Train f1: 0.91 | Train precision: 0.92 | Train recall: 0.91\n",
      "\t Val. Loss: 0.234 |  Val. f1: 0.66 |  Val. precision: 0.70 | Val. recall: 0.66\n",
      "\n",
      "Performance of best found Model:\n",
      "Val. Loss: 0.180 |  Val. f1: 0.63 | Val. precision: 0.69 | Val. recall: 0.63\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "model = gru_model2\n",
    "model_name = gru_model_name2\n",
    "criterion = baseline_criterion\n",
    "\n",
    "model.apply(init_weights)\n",
    "print(f'El modelo actual tiene {count_parameters(model):,} parámetros entrenables.')\n",
    "\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "model = model.to(device)\n",
    "criterion = criterion.to(device)\n",
    "\n",
    "optimize_model(model, train_iterator, valid_iterator, optimizer, criterion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AB49TFLXM3CP"
   },
   "source": [
    "##### Embeddings Pre-entrenados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 171,
     "referenced_widgets": [
      "873b40d4d05d496e99d6fe9d040fc3c0",
      "66ddd53c2cb647e7ac0fdbea7ae7f68a",
      "ac9dae42dc594402b132cfc53569157e",
      "dc755770a50a4b209c131b1d8a7ce865",
      "cc9affc16ca14e9a957c1abd3b52a411",
      "12265109990b4fd0baf30cbd9b1d8dd2",
      "e63a97dc96fa434fbba5dd99650c2b2a",
      "f311d9d2b57d4619ab9b3c748d2ee15a"
     ]
    },
    "colab_type": "code",
    "id": "nbpApqCERsES",
    "outputId": "5cd3ee6d-92ea-4550-c123-02c105a1a202"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:4: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  after removing the cwd from sys.path.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "873b40d4d05d496e99d6fe9d040fc3c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=29171.0), HTML(value='')))"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:5: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  \"\"\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "'El uso del modelo pre-entrenado se concibe con ayuda de:\\\n",
    "https://medium.com/@rohit_agrawal/using-fine-tuned-gensim-word2vec-embeddings-with-torchtext-and-pytorch-17eea2883cd'\n",
    "\n",
    "word2vec_vectors = []\n",
    "for token, idx in tqdm_notebook(TEXT.vocab.stoi.items()):\n",
    "    if token in wordvectors.wv.vocab.keys():\n",
    "        word2vec_vectors.append(torch.FloatTensor(wordvectors[token]))\n",
    "    else:\n",
    "        word2vec_vectors.append(torch.zeros(W2V_SIZE))\n",
    "TEXT.vocab.set_vectors(TEXT.vocab.stoi, word2vec_vectors, W2V_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Zdq5B9SdGR6j"
   },
   "outputs": [],
   "source": [
    "%%script false\n",
    "print(TEXT.vocab.vectors.shape)\n",
    "pre_trained_emb = torch.FloatTensor(TEXT.vocab.vectors)\n",
    "model.embedding = nn.Embedding.from_pretrained(pre_trained_emb).cuda()\n",
    "\n",
    "## Al cargar Embeddings pre-entrenados quedan con requires_grad = False\n",
    "print(model.embedding.weight.requires_grad)\n",
    "model.embedding.weight.requires_grad = True\n",
    "print(model.embedding.weight.requires_grad)\n",
    "\n",
    "for batch in train_iterator:\n",
    "    print(batch.text)\n",
    "    print(batch.text.shape)\n",
    "    print(model.embedding(batch.text.cuda()).shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 989
    },
    "colab_type": "code",
    "id": "s5V-GuwKNMNu",
    "outputId": "e46abc9b-d058-414e-fd92-cc4c8584d046"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El modelo actual tiene 2,724,874 parámetros entrenables.\n",
      "Epoch: 01 | Epoch Time: 0m 9s\n",
      "\tTrain Loss: 0.252 | Train f1: 0.32 | Train precision: 0.37 | Train recall: 0.33\n",
      "\t Val. Loss: 0.382 |  Val. f1: 0.44 |  Val. precision: 0.50 | Val. recall: 0.48\n",
      "Epoch: 02 | Epoch Time: 0m 9s\n",
      "\tTrain Loss: 0.169 | Train f1: 0.47 | Train precision: 0.52 | Train recall: 0.48\n",
      "\t Val. Loss: 0.324 |  Val. f1: 0.46 |  Val. precision: 0.52 | Val. recall: 0.48\n",
      "Epoch: 03 | Epoch Time: 0m 9s\n",
      "\tTrain Loss: 0.152 | Train f1: 0.51 | Train precision: 0.57 | Train recall: 0.52\n",
      "\t Val. Loss: 0.278 |  Val. f1: 0.49 |  Val. precision: 0.54 | Val. recall: 0.50\n",
      "Epoch: 04 | Epoch Time: 0m 9s\n",
      "\tTrain Loss: 0.138 | Train f1: 0.54 | Train precision: 0.60 | Train recall: 0.55\n",
      "\t Val. Loss: 0.305 |  Val. f1: 0.52 |  Val. precision: 0.57 | Val. recall: 0.53\n",
      "Epoch: 05 | Epoch Time: 0m 9s\n",
      "\tTrain Loss: 0.132 | Train f1: 0.56 | Train precision: 0.61 | Train recall: 0.56\n",
      "\t Val. Loss: 0.279 |  Val. f1: 0.52 |  Val. precision: 0.57 | Val. recall: 0.54\n",
      "Epoch: 06 | Epoch Time: 0m 9s\n",
      "\tTrain Loss: 0.123 | Train f1: 0.58 | Train precision: 0.63 | Train recall: 0.58\n",
      "\t Val. Loss: 0.253 |  Val. f1: 0.52 |  Val. precision: 0.57 | Val. recall: 0.53\n",
      "Epoch: 07 | Epoch Time: 0m 9s\n",
      "\tTrain Loss: 0.117 | Train f1: 0.59 | Train precision: 0.65 | Train recall: 0.60\n",
      "\t Val. Loss: 0.287 |  Val. f1: 0.54 |  Val. precision: 0.58 | Val. recall: 0.57\n",
      "Epoch: 08 | Epoch Time: 0m 9s\n",
      "\tTrain Loss: 0.112 | Train f1: 0.60 | Train precision: 0.65 | Train recall: 0.60\n",
      "\t Val. Loss: 0.311 |  Val. f1: 0.51 |  Val. precision: 0.55 | Val. recall: 0.54\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/numpy/lib/function_base.py:393: RuntimeWarning: Mean of empty slice.\n",
      "  avg = a.mean(axis)\n",
      "/usr/local/lib/python3.6/dist-packages/numpy/core/_methods.py:161: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 09 | Epoch Time: 0m 9s\n",
      "\tTrain Loss: 0.104 | Train f1: nan | Train precision: nan | Train recall: nan\n",
      "\t Val. Loss: 0.271 |  Val. f1: 0.56 |  Val. precision: 0.61 | Val. recall: 0.57\n",
      "Epoch: 10 | Epoch Time: 0m 9s\n",
      "\tTrain Loss: 0.099 | Train f1: 0.63 | Train precision: 0.68 | Train recall: 0.64\n",
      "\t Val. Loss: 0.286 |  Val. f1: 0.57 |  Val. precision: 0.61 | Val. recall: 0.58\n",
      "Epoch: 11 | Epoch Time: 0m 9s\n",
      "\tTrain Loss: 0.095 | Train f1: 0.65 | Train precision: 0.69 | Train recall: 0.65\n",
      "\t Val. Loss: 0.285 |  Val. f1: 0.56 |  Val. precision: 0.61 | Val. recall: 0.58\n",
      "Epoch: 12 | Epoch Time: 0m 9s\n",
      "\tTrain Loss: 0.087 | Train f1: 0.66 | Train precision: 0.70 | Train recall: 0.66\n",
      "\t Val. Loss: 0.304 |  Val. f1: 0.56 |  Val. precision: 0.61 | Val. recall: 0.58\n",
      "Epoch: 13 | Epoch Time: 0m 9s\n",
      "\tTrain Loss: 0.084 | Train f1: 0.68 | Train precision: 0.72 | Train recall: 0.68\n",
      "\t Val. Loss: 0.301 |  Val. f1: 0.58 |  Val. precision: 0.62 | Val. recall: 0.59\n",
      "Epoch: 14 | Epoch Time: 0m 9s\n",
      "\tTrain Loss: 0.076 | Train f1: 0.70 | Train precision: 0.74 | Train recall: 0.70\n",
      "\t Val. Loss: 0.322 |  Val. f1: 0.58 |  Val. precision: 0.61 | Val. recall: 0.59\n",
      "Epoch: 15 | Epoch Time: 0m 9s\n",
      "\tTrain Loss: 0.073 | Train f1: 0.71 | Train precision: 0.74 | Train recall: 0.71\n",
      "\t Val. Loss: 0.321 |  Val. f1: 0.56 |  Val. precision: 0.61 | Val. recall: 0.58\n",
      "Epoch: 16 | Epoch Time: 0m 9s\n",
      "\tTrain Loss: 0.069 | Train f1: 0.71 | Train precision: 0.74 | Train recall: 0.71\n",
      "\t Val. Loss: 0.301 |  Val. f1: 0.58 |  Val. precision: 0.63 | Val. recall: 0.59\n",
      "\n",
      "Performance of best found Model:\n",
      "Val. Loss: 0.253 |  Val. f1: 0.52 | Val. precision: 0.57 | Val. recall: 0.53\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "model = lstm_model\n",
    "model_name = lstm_model_name\n",
    "criterion = baseline_criterion\n",
    "\n",
    "model.apply(init_weights)\n",
    "\n",
    "pre_trained_emb = torch.FloatTensor(TEXT.vocab.vectors)\n",
    "model.load_pretrained_embeddings(pre_trained_emb, False)\n",
    "print(f'El modelo actual tiene {count_parameters(model):,} parámetros entrenables.')\n",
    "\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "model = model.to(device)\n",
    "criterion = criterion.to(device)\n",
    "\n",
    "optimize_model(model, train_iterator, valid_iterator, optimizer, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "jkE6g3jQNu-g",
    "outputId": "325feb76-0a56-49ba-d09e-8865b4eed391"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El modelo actual tiene 211,466 parámetros entrenables.\n",
      "Epoch: 01 | Epoch Time: 0m 8s\n",
      "\tTrain Loss: 0.309 | Train f1: 0.24 | Train precision: 0.28 | Train recall: 0.26\n",
      "\t Val. Loss: 0.380 |  Val. f1: 0.36 |  Val. precision: 0.38 | Val. recall: 0.40\n",
      "Epoch: 02 | Epoch Time: 0m 8s\n",
      "\tTrain Loss: 0.199 | Train f1: 0.39 | Train precision: 0.43 | Train recall: 0.40\n",
      "\t Val. Loss: 0.378 |  Val. f1: 0.42 |  Val. precision: 0.46 | Val. recall: 0.46\n",
      "Epoch: 03 | Epoch Time: 0m 8s\n",
      "\tTrain Loss: 0.178 | Train f1: 0.44 | Train precision: 0.50 | Train recall: 0.45\n",
      "\t Val. Loss: 0.342 |  Val. f1: 0.44 |  Val. precision: 0.50 | Val. recall: 0.46\n",
      "Epoch: 04 | Epoch Time: 0m 8s\n",
      "\tTrain Loss: 0.164 | Train f1: 0.48 | Train precision: 0.54 | Train recall: 0.48\n",
      "\t Val. Loss: 0.340 |  Val. f1: 0.44 |  Val. precision: 0.49 | Val. recall: 0.47\n",
      "Epoch: 05 | Epoch Time: 0m 8s\n",
      "\tTrain Loss: 0.156 | Train f1: 0.50 | Train precision: 0.55 | Train recall: 0.50\n",
      "\t Val. Loss: 0.313 |  Val. f1: 0.43 |  Val. precision: 0.50 | Val. recall: 0.45\n",
      "Epoch: 06 | Epoch Time: 0m 8s\n",
      "\tTrain Loss: 0.150 | Train f1: 0.51 | Train precision: 0.57 | Train recall: 0.52\n",
      "\t Val. Loss: 0.292 |  Val. f1: 0.48 |  Val. precision: 0.52 | Val. recall: 0.51\n",
      "Epoch: 07 | Epoch Time: 0m 8s\n",
      "\tTrain Loss: 0.142 | Train f1: 0.53 | Train precision: 0.58 | Train recall: 0.54\n",
      "\t Val. Loss: 0.281 |  Val. f1: 0.47 |  Val. precision: 0.51 | Val. recall: 0.49\n",
      "Epoch: 08 | Epoch Time: 0m 8s\n",
      "\tTrain Loss: 0.141 | Train f1: 0.53 | Train precision: 0.59 | Train recall: 0.54\n",
      "\t Val. Loss: 0.311 |  Val. f1: 0.48 |  Val. precision: 0.54 | Val. recall: 0.51\n",
      "Epoch: 09 | Epoch Time: 0m 8s\n",
      "\tTrain Loss: 0.137 | Train f1: 0.54 | Train precision: 0.59 | Train recall: 0.54\n",
      "\t Val. Loss: 0.300 |  Val. f1: 0.48 |  Val. precision: 0.53 | Val. recall: 0.51\n",
      "Epoch: 10 | Epoch Time: 0m 8s\n",
      "\tTrain Loss: 0.131 | Train f1: 0.56 | Train precision: 0.61 | Train recall: 0.56\n",
      "\t Val. Loss: 0.292 |  Val. f1: 0.50 |  Val. precision: 0.55 | Val. recall: 0.53\n",
      "Epoch: 11 | Epoch Time: 0m 8s\n",
      "\tTrain Loss: 0.127 | Train f1: 0.57 | Train precision: 0.62 | Train recall: 0.57\n",
      "\t Val. Loss: 0.281 |  Val. f1: 0.52 |  Val. precision: 0.57 | Val. recall: 0.54\n",
      "Epoch: 12 | Epoch Time: 0m 8s\n",
      "\tTrain Loss: 0.125 | Train f1: 0.57 | Train precision: 0.63 | Train recall: 0.58\n",
      "\t Val. Loss: 0.273 |  Val. f1: 0.50 |  Val. precision: 0.54 | Val. recall: 0.51\n",
      "Epoch: 13 | Epoch Time: 0m 8s\n",
      "\tTrain Loss: 0.123 | Train f1: 0.58 | Train precision: 0.63 | Train recall: 0.58\n",
      "\t Val. Loss: 0.289 |  Val. f1: 0.53 |  Val. precision: 0.57 | Val. recall: 0.55\n",
      "Epoch: 14 | Epoch Time: 0m 8s\n",
      "\tTrain Loss: 0.121 | Train f1: 0.59 | Train precision: 0.64 | Train recall: 0.59\n",
      "\t Val. Loss: 0.304 |  Val. f1: 0.50 |  Val. precision: 0.55 | Val. recall: 0.52\n",
      "Epoch: 15 | Epoch Time: 0m 8s\n",
      "\tTrain Loss: 0.118 | Train f1: 0.58 | Train precision: 0.63 | Train recall: 0.59\n",
      "\t Val. Loss: 0.282 |  Val. f1: 0.51 |  Val. precision: 0.55 | Val. recall: 0.53\n",
      "Epoch: 16 | Epoch Time: 0m 8s\n",
      "\tTrain Loss: 0.114 | Train f1: 0.60 | Train precision: 0.65 | Train recall: 0.60\n",
      "\t Val. Loss: 0.279 |  Val. f1: 0.52 |  Val. precision: 0.57 | Val. recall: 0.54\n",
      "Epoch: 17 | Epoch Time: 0m 8s\n",
      "\tTrain Loss: 0.112 | Train f1: 0.61 | Train precision: 0.66 | Train recall: 0.61\n",
      "\t Val. Loss: 0.275 |  Val. f1: 0.53 |  Val. precision: 0.58 | Val. recall: 0.54\n",
      "Epoch: 18 | Epoch Time: 0m 8s\n",
      "\tTrain Loss: 0.110 | Train f1: 0.61 | Train precision: 0.65 | Train recall: 0.61\n",
      "\t Val. Loss: 0.284 |  Val. f1: 0.51 |  Val. precision: 0.57 | Val. recall: 0.53\n",
      "Epoch: 19 | Epoch Time: 0m 8s\n",
      "\tTrain Loss: 0.107 | Train f1: 0.62 | Train precision: 0.66 | Train recall: 0.62\n",
      "\t Val. Loss: 0.277 |  Val. f1: 0.52 |  Val. precision: 0.57 | Val. recall: 0.54\n",
      "Epoch: 20 | Epoch Time: 0m 8s\n",
      "\tTrain Loss: 0.105 | Train f1: 0.62 | Train precision: 0.67 | Train recall: 0.62\n",
      "\t Val. Loss: 0.288 |  Val. f1: 0.52 |  Val. precision: 0.58 | Val. recall: 0.53\n",
      "Epoch: 21 | Epoch Time: 0m 8s\n",
      "\tTrain Loss: 0.104 | Train f1: 0.62 | Train precision: 0.66 | Train recall: 0.62\n",
      "\t Val. Loss: 0.303 |  Val. f1: 0.51 |  Val. precision: 0.56 | Val. recall: 0.53\n",
      "Epoch: 22 | Epoch Time: 0m 8s\n",
      "\tTrain Loss: 0.101 | Train f1: 0.62 | Train precision: 0.67 | Train recall: 0.63\n",
      "\t Val. Loss: 0.287 |  Val. f1: 0.53 |  Val. precision: 0.58 | Val. recall: 0.56\n",
      "\n",
      "Performance of best found Model:\n",
      "Val. Loss: 0.273 |  Val. f1: 0.50 | Val. precision: 0.54 | Val. recall: 0.51\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "model = elman_model\n",
    "model_name = elman_model_name\n",
    "criterion = baseline_criterion\n",
    "\n",
    "model.apply(init_weights)\n",
    "\n",
    "pre_trained_emb = torch.FloatTensor(TEXT.vocab.vectors)\n",
    "model.load_pretrained_embeddings(pre_trained_emb, False)\n",
    "print(f'El modelo actual tiene {count_parameters(model):,} parámetros entrenables.')\n",
    "\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "model = model.to(device)\n",
    "criterion = criterion.to(device)\n",
    "\n",
    "optimize_model(model, train_iterator, valid_iterator, optimizer, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "OtoaLD-cNz8a",
    "outputId": "03dcd6cb-306d-4e41-90e0-3cf5f6904354"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El modelo actual tiene 2,044,938 parámetros entrenables.\n",
      "Epoch: 01 | Epoch Time: 0m 8s\n",
      "\tTrain Loss: 0.251 | Train f1: 0.34 | Train precision: 0.40 | Train recall: 0.35\n",
      "\t Val. Loss: 0.376 |  Val. f1: 0.47 |  Val. precision: 0.52 | Val. recall: 0.49\n",
      "Epoch: 02 | Epoch Time: 0m 8s\n",
      "\tTrain Loss: 0.173 | Train f1: 0.47 | Train precision: 0.52 | Train recall: 0.47\n",
      "\t Val. Loss: 0.363 |  Val. f1: 0.47 |  Val. precision: 0.55 | Val. recall: 0.48\n",
      "Epoch: 03 | Epoch Time: 0m 8s\n",
      "\tTrain Loss: 0.154 | Train f1: 0.50 | Train precision: 0.56 | Train recall: 0.51\n",
      "\t Val. Loss: 0.356 |  Val. f1: 0.46 |  Val. precision: 0.53 | Val. recall: 0.48\n",
      "Epoch: 04 | Epoch Time: 0m 8s\n",
      "\tTrain Loss: 0.143 | Train f1: 0.53 | Train precision: 0.58 | Train recall: 0.54\n",
      "\t Val. Loss: 0.293 |  Val. f1: 0.53 |  Val. precision: 0.58 | Val. recall: 0.55\n",
      "Epoch: 05 | Epoch Time: 0m 8s\n",
      "\tTrain Loss: 0.133 | Train f1: 0.56 | Train precision: 0.61 | Train recall: 0.56\n",
      "\t Val. Loss: 0.293 |  Val. f1: 0.54 |  Val. precision: 0.58 | Val. recall: 0.56\n",
      "Epoch: 06 | Epoch Time: 0m 8s\n",
      "\tTrain Loss: 0.123 | Train f1: 0.58 | Train precision: 0.63 | Train recall: 0.58\n",
      "\t Val. Loss: 0.301 |  Val. f1: 0.55 |  Val. precision: 0.60 | Val. recall: 0.57\n",
      "Epoch: 07 | Epoch Time: 0m 8s\n",
      "\tTrain Loss: 0.119 | Train f1: 0.60 | Train precision: 0.65 | Train recall: 0.60\n",
      "\t Val. Loss: 0.301 |  Val. f1: 0.54 |  Val. precision: 0.58 | Val. recall: 0.56\n",
      "Epoch: 08 | Epoch Time: 0m 8s\n",
      "\tTrain Loss: 0.112 | Train f1: 0.61 | Train precision: 0.65 | Train recall: 0.61\n",
      "\t Val. Loss: 0.316 |  Val. f1: 0.51 |  Val. precision: 0.56 | Val. recall: 0.53\n",
      "Epoch: 09 | Epoch Time: 0m 8s\n",
      "\tTrain Loss: 0.106 | Train f1: 0.62 | Train precision: 0.67 | Train recall: 0.62\n",
      "\t Val. Loss: 0.271 |  Val. f1: 0.53 |  Val. precision: 0.58 | Val. recall: 0.54\n",
      "Epoch: 10 | Epoch Time: 0m 8s\n",
      "\tTrain Loss: 0.102 | Train f1: 0.63 | Train precision: 0.68 | Train recall: 0.63\n",
      "\t Val. Loss: 0.295 |  Val. f1: 0.52 |  Val. precision: 0.57 | Val. recall: 0.54\n",
      "Epoch: 11 | Epoch Time: 0m 8s\n",
      "\tTrain Loss: 0.098 | Train f1: 0.64 | Train precision: 0.68 | Train recall: 0.64\n",
      "\t Val. Loss: 0.314 |  Val. f1: 0.54 |  Val. precision: 0.59 | Val. recall: 0.56\n",
      "Epoch: 12 | Epoch Time: 0m 8s\n",
      "\tTrain Loss: 0.093 | Train f1: 0.65 | Train precision: 0.70 | Train recall: 0.65\n",
      "\t Val. Loss: 0.288 |  Val. f1: 0.57 |  Val. precision: 0.61 | Val. recall: 0.58\n",
      "Epoch: 13 | Epoch Time: 0m 8s\n",
      "\tTrain Loss: 0.088 | Train f1: 0.67 | Train precision: 0.71 | Train recall: 0.67\n",
      "\t Val. Loss: 0.304 |  Val. f1: 0.56 |  Val. precision: 0.60 | Val. recall: 0.58\n",
      "Epoch: 14 | Epoch Time: 0m 8s\n",
      "\tTrain Loss: 0.083 | Train f1: 0.69 | Train precision: 0.72 | Train recall: 0.69\n",
      "\t Val. Loss: 0.293 |  Val. f1: 0.53 |  Val. precision: 0.59 | Val. recall: 0.54\n",
      "Epoch: 15 | Epoch Time: 0m 8s\n",
      "\tTrain Loss: 0.078 | Train f1: 0.70 | Train precision: 0.73 | Train recall: 0.69\n",
      "\t Val. Loss: 0.296 |  Val. f1: 0.56 |  Val. precision: 0.60 | Val. recall: 0.58\n",
      "Epoch: 16 | Epoch Time: 0m 9s\n",
      "\tTrain Loss: 0.075 | Train f1: 0.70 | Train precision: 0.73 | Train recall: 0.70\n",
      "\t Val. Loss: 0.320 |  Val. f1: 0.57 |  Val. precision: 0.63 | Val. recall: 0.59\n",
      "Epoch: 17 | Epoch Time: 0m 8s\n",
      "\tTrain Loss: 0.072 | Train f1: 0.71 | Train precision: 0.75 | Train recall: 0.71\n",
      "\t Val. Loss: 0.312 |  Val. f1: 0.58 |  Val. precision: 0.62 | Val. recall: 0.60\n",
      "Epoch: 18 | Epoch Time: 0m 8s\n",
      "\tTrain Loss: 0.071 | Train f1: 0.72 | Train precision: 0.75 | Train recall: 0.72\n",
      "\t Val. Loss: 0.294 |  Val. f1: 0.57 |  Val. precision: 0.61 | Val. recall: 0.59\n",
      "Epoch: 19 | Epoch Time: 0m 8s\n",
      "\tTrain Loss: 0.065 | Train f1: 0.73 | Train precision: 0.76 | Train recall: 0.73\n",
      "\t Val. Loss: 0.323 |  Val. f1: 0.57 |  Val. precision: 0.62 | Val. recall: 0.58\n",
      "\n",
      "Performance of best found Model:\n",
      "Val. Loss: 0.271 |  Val. f1: 0.53 | Val. precision: 0.58 | Val. recall: 0.54\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "model = gru_model1\n",
    "model_name = gru_model_name1\n",
    "criterion = baseline_criterion\n",
    "\n",
    "model.apply(init_weights)\n",
    "\n",
    "pre_trained_emb = torch.FloatTensor(TEXT.vocab.vectors)\n",
    "model.load_pretrained_embeddings(pre_trained_emb, False)\n",
    "print(f'El modelo actual tiene {count_parameters(model):,} parámetros entrenables.')\n",
    "\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "model = model.to(device)\n",
    "criterion = criterion.to(device)\n",
    "\n",
    "optimize_model(model, train_iterator, valid_iterator, optimizer, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "Di-xCFoSN7VU",
    "outputId": "e9e80593-1b78-4505-af9b-9d9b2e39f8ac"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El modelo actual tiene 2,044,938 parámetros entrenables.\n",
      "Epoch: 01 | Epoch Time: 0m 9s\n",
      "\tTrain Loss: 0.279 | Train f1: 0.31 | Train precision: 0.36 | Train recall: 0.31\n",
      "\t Val. Loss: 0.359 |  Val. f1: 0.44 |  Val. precision: 0.49 | Val. recall: 0.46\n",
      "Epoch: 02 | Epoch Time: 0m 9s\n",
      "\tTrain Loss: 0.188 | Train f1: 0.43 | Train precision: 0.49 | Train recall: 0.44\n",
      "\t Val. Loss: 0.425 |  Val. f1: 0.42 |  Val. precision: 0.47 | Val. recall: 0.46\n",
      "Epoch: 03 | Epoch Time: 0m 9s\n",
      "\tTrain Loss: 0.170 | Train f1: 0.47 | Train precision: 0.53 | Train recall: 0.47\n",
      "\t Val. Loss: 0.372 |  Val. f1: 0.48 |  Val. precision: 0.53 | Val. recall: 0.51\n",
      "Epoch: 04 | Epoch Time: 0m 9s\n",
      "\tTrain Loss: 0.157 | Train f1: 0.50 | Train precision: 0.55 | Train recall: 0.50\n",
      "\t Val. Loss: 0.372 |  Val. f1: 0.46 |  Val. precision: 0.50 | Val. recall: 0.49\n",
      "Epoch: 05 | Epoch Time: 0m 9s\n",
      "\tTrain Loss: 0.149 | Train f1: 0.52 | Train precision: 0.58 | Train recall: 0.53\n",
      "\t Val. Loss: 0.324 |  Val. f1: 0.52 |  Val. precision: 0.56 | Val. recall: 0.54\n",
      "Epoch: 06 | Epoch Time: 0m 9s\n",
      "\tTrain Loss: 0.139 | Train f1: 0.54 | Train precision: 0.60 | Train recall: 0.55\n",
      "\t Val. Loss: 0.319 |  Val. f1: 0.51 |  Val. precision: 0.56 | Val. recall: 0.53\n",
      "Epoch: 07 | Epoch Time: 0m 9s\n",
      "\tTrain Loss: 0.133 | Train f1: 0.56 | Train precision: 0.61 | Train recall: 0.56\n",
      "\t Val. Loss: 0.316 |  Val. f1: 0.54 |  Val. precision: 0.60 | Val. recall: 0.56\n",
      "Epoch: 08 | Epoch Time: 0m 9s\n",
      "\tTrain Loss: 0.129 | Train f1: 0.57 | Train precision: 0.62 | Train recall: 0.57\n",
      "\t Val. Loss: 0.314 |  Val. f1: 0.54 |  Val. precision: 0.58 | Val. recall: 0.56\n",
      "Epoch: 09 | Epoch Time: 0m 10s\n",
      "\tTrain Loss: 0.123 | Train f1: 0.58 | Train precision: 0.63 | Train recall: 0.58\n",
      "\t Val. Loss: 0.292 |  Val. f1: 0.52 |  Val. precision: 0.58 | Val. recall: 0.53\n",
      "Epoch: 10 | Epoch Time: 0m 9s\n",
      "\tTrain Loss: 0.119 | Train f1: 0.59 | Train precision: 0.64 | Train recall: 0.59\n",
      "\t Val. Loss: 0.292 |  Val. f1: 0.55 |  Val. precision: 0.60 | Val. recall: 0.58\n",
      "Epoch: 11 | Epoch Time: 0m 9s\n",
      "\tTrain Loss: 0.117 | Train f1: 0.59 | Train precision: 0.64 | Train recall: 0.59\n",
      "\t Val. Loss: 0.303 |  Val. f1: 0.55 |  Val. precision: 0.60 | Val. recall: 0.57\n",
      "Epoch: 12 | Epoch Time: 0m 9s\n",
      "\tTrain Loss: 0.112 | Train f1: 0.61 | Train precision: 0.65 | Train recall: 0.61\n",
      "\t Val. Loss: 0.293 |  Val. f1: 0.55 |  Val. precision: 0.60 | Val. recall: 0.57\n",
      "Epoch: 13 | Epoch Time: 0m 9s\n",
      "\tTrain Loss: 0.111 | Train f1: 0.60 | Train precision: 0.65 | Train recall: 0.60\n",
      "\t Val. Loss: 0.271 |  Val. f1: 0.57 |  Val. precision: 0.62 | Val. recall: 0.60\n",
      "Epoch: 14 | Epoch Time: 0m 9s\n",
      "\tTrain Loss: 0.106 | Train f1: 0.62 | Train precision: 0.66 | Train recall: 0.62\n",
      "\t Val. Loss: 0.276 |  Val. f1: 0.56 |  Val. precision: 0.61 | Val. recall: 0.58\n",
      "Epoch: 15 | Epoch Time: 0m 9s\n",
      "\tTrain Loss: 0.105 | Train f1: 0.62 | Train precision: 0.67 | Train recall: 0.63\n",
      "\t Val. Loss: 0.279 |  Val. f1: 0.57 |  Val. precision: 0.62 | Val. recall: 0.59\n",
      "Epoch: 16 | Epoch Time: 0m 9s\n",
      "\tTrain Loss: 0.102 | Train f1: 0.64 | Train precision: 0.69 | Train recall: 0.64\n",
      "\t Val. Loss: 0.279 |  Val. f1: 0.56 |  Val. precision: 0.60 | Val. recall: 0.58\n",
      "Epoch: 17 | Epoch Time: 0m 9s\n",
      "\tTrain Loss: 0.098 | Train f1: 0.64 | Train precision: 0.69 | Train recall: 0.64\n",
      "\t Val. Loss: 0.295 |  Val. f1: 0.56 |  Val. precision: 0.61 | Val. recall: 0.58\n",
      "Epoch: 18 | Epoch Time: 0m 9s\n",
      "\tTrain Loss: 0.097 | Train f1: 0.65 | Train precision: 0.69 | Train recall: 0.65\n",
      "\t Val. Loss: 0.300 |  Val. f1: 0.56 |  Val. precision: 0.60 | Val. recall: 0.58\n",
      "Epoch: 19 | Epoch Time: 0m 9s\n",
      "\tTrain Loss: 0.093 | Train f1: 0.66 | Train precision: 0.70 | Train recall: 0.66\n",
      "\t Val. Loss: 0.315 |  Val. f1: 0.58 |  Val. precision: 0.62 | Val. recall: 0.61\n",
      "Epoch: 20 | Epoch Time: 0m 9s\n",
      "\tTrain Loss: 0.093 | Train f1: 0.66 | Train precision: 0.70 | Train recall: 0.66\n",
      "\t Val. Loss: 0.315 |  Val. f1: 0.58 |  Val. precision: 0.62 | Val. recall: 0.60\n",
      "Epoch: 21 | Epoch Time: 0m 9s\n",
      "\tTrain Loss: 0.092 | Train f1: 0.66 | Train precision: 0.70 | Train recall: 0.66\n",
      "\t Val. Loss: 0.305 |  Val. f1: 0.58 |  Val. precision: 0.62 | Val. recall: 0.59\n",
      "Epoch: 22 | Epoch Time: 0m 9s\n",
      "\tTrain Loss: 0.089 | Train f1: 0.67 | Train precision: 0.71 | Train recall: 0.67\n",
      "\t Val. Loss: 0.305 |  Val. f1: 0.58 |  Val. precision: 0.63 | Val. recall: 0.59\n",
      "Epoch: 23 | Epoch Time: 0m 9s\n",
      "\tTrain Loss: 0.087 | Train f1: 0.67 | Train precision: 0.71 | Train recall: 0.67\n",
      "\t Val. Loss: 0.343 |  Val. f1: 0.56 |  Val. precision: 0.59 | Val. recall: 0.59\n",
      "\n",
      "Performance of best found Model:\n",
      "Val. Loss: 0.271 |  Val. f1: 0.57 | Val. precision: 0.62 | Val. recall: 0.60\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "model = gru_model2\n",
    "model_name = gru_model_name2\n",
    "criterion = baseline_criterion\n",
    "\n",
    "model.apply(init_weights)\n",
    "\n",
    "pre_trained_emb = torch.FloatTensor(TEXT.vocab.vectors)\n",
    "model.load_pretrained_embeddings(pre_trained_emb, False)\n",
    "print(f'El modelo actual tiene {count_parameters(model):,} parámetros entrenables.')\n",
    "\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "model = model.to(device)\n",
    "criterion = criterion.to(device)\n",
    "\n",
    "optimize_model(model, train_iterator, valid_iterator, optimizer, criterion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zhcRxXlhM7Dr"
   },
   "source": [
    "##### *Fine-tunning* de los Embeddings Pre-entrenados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 799
    },
    "colab_type": "code",
    "id": "YSDWN5mHN_KC",
    "outputId": "08308f1f-2ccd-44ca-f8c9-2dcd0e2928d3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El modelo actual tiene 10,555,174 parámetros entrenables.\n",
      "Epoch: 01 | Epoch Time: 0m 11s\n",
      "\tTrain Loss: 0.224 | Train f1: 0.39 | Train precision: 0.44 | Train recall: 0.40\n",
      "\t Val. Loss: 0.307 |  Val. f1: 0.50 |  Val. precision: 0.54 | Val. recall: 0.53\n",
      "Epoch: 02 | Epoch Time: 0m 11s\n",
      "\tTrain Loss: 0.102 | Train f1: 0.66 | Train precision: 0.71 | Train recall: 0.67\n",
      "\t Val. Loss: 0.272 |  Val. f1: 0.61 |  Val. precision: 0.65 | Val. recall: 0.64\n",
      "Epoch: 03 | Epoch Time: 0m 10s\n",
      "\tTrain Loss: 0.063 | Train f1: 0.77 | Train precision: 0.80 | Train recall: 0.77\n",
      "\t Val. Loss: 0.276 |  Val. f1: 0.66 |  Val. precision: 0.69 | Val. recall: 0.68\n",
      "Epoch: 04 | Epoch Time: 0m 10s\n",
      "\tTrain Loss: 0.046 | Train f1: 0.81 | Train precision: 0.83 | Train recall: 0.81\n",
      "\t Val. Loss: 0.263 |  Val. f1: 0.63 |  Val. precision: 0.67 | Val. recall: 0.65\n",
      "Epoch: 05 | Epoch Time: 0m 10s\n",
      "\tTrain Loss: 0.037 | Train f1: 0.84 | Train precision: 0.86 | Train recall: 0.84\n",
      "\t Val. Loss: 0.278 |  Val. f1: 0.63 |  Val. precision: 0.66 | Val. recall: 0.66\n",
      "Epoch: 06 | Epoch Time: 0m 10s\n",
      "\tTrain Loss: 0.030 | Train f1: 0.87 | Train precision: 0.88 | Train recall: 0.87\n",
      "\t Val. Loss: 0.302 |  Val. f1: 0.65 |  Val. precision: 0.68 | Val. recall: 0.68\n",
      "Epoch: 07 | Epoch Time: 0m 10s\n",
      "\tTrain Loss: 0.024 | Train f1: 0.89 | Train precision: 0.90 | Train recall: 0.89\n",
      "\t Val. Loss: 0.338 |  Val. f1: 0.63 |  Val. precision: 0.67 | Val. recall: 0.65\n",
      "Epoch: 08 | Epoch Time: 0m 10s\n",
      "\tTrain Loss: 0.021 | Train f1: 0.90 | Train precision: 0.91 | Train recall: 0.90\n",
      "\t Val. Loss: 0.312 |  Val. f1: 0.67 |  Val. precision: 0.69 | Val. recall: 0.70\n",
      "Epoch: 09 | Epoch Time: 0m 10s\n",
      "\tTrain Loss: 0.019 | Train f1: 0.91 | Train precision: 0.92 | Train recall: 0.91\n",
      "\t Val. Loss: 0.344 |  Val. f1: 0.67 |  Val. precision: 0.69 | Val. recall: 0.69\n",
      "Epoch: 10 | Epoch Time: 0m 10s\n",
      "\tTrain Loss: 0.016 | Train f1: 0.92 | Train precision: 0.93 | Train recall: 0.92\n",
      "\t Val. Loss: 0.361 |  Val. f1: 0.64 |  Val. precision: 0.68 | Val. recall: 0.67\n",
      "Epoch: 11 | Epoch Time: 0m 10s\n",
      "\tTrain Loss: 0.014 | Train f1: 0.93 | Train precision: 0.94 | Train recall: 0.93\n",
      "\t Val. Loss: 0.373 |  Val. f1: 0.64 |  Val. precision: 0.68 | Val. recall: 0.67\n",
      "Epoch: 12 | Epoch Time: 0m 10s\n",
      "\tTrain Loss: 0.012 | Train f1: 0.94 | Train precision: 0.95 | Train recall: 0.94\n",
      "\t Val. Loss: 0.376 |  Val. f1: 0.69 |  Val. precision: 0.71 | Val. recall: 0.71\n",
      "Epoch: 13 | Epoch Time: 0m 10s\n",
      "\tTrain Loss: 0.011 | Train f1: 0.94 | Train precision: 0.95 | Train recall: 0.94\n",
      "\t Val. Loss: 0.360 |  Val. f1: 0.67 |  Val. precision: 0.70 | Val. recall: 0.69\n",
      "Epoch: 14 | Epoch Time: 0m 10s\n",
      "\tTrain Loss: 0.011 | Train f1: 0.95 | Train precision: 0.95 | Train recall: 0.95\n",
      "\t Val. Loss: 0.376 |  Val. f1: 0.66 |  Val. precision: 0.69 | Val. recall: 0.68\n",
      "\n",
      "Performance of best found Model:\n",
      "Val. Loss: 0.263 |  Val. f1: 0.63 | Val. precision: 0.67 | Val. recall: 0.65\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "model = lstm_model\n",
    "model_name = lstm_model_name\n",
    "criterion = baseline_criterion\n",
    "\n",
    "model.apply(init_weights)\n",
    "\n",
    "pre_trained_emb = torch.FloatTensor(TEXT.vocab.vectors)\n",
    "model.load_pretrained_embeddings(pre_trained_emb, True)\n",
    "print(f'El modelo actual tiene {count_parameters(model):,} parámetros entrenables.')\n",
    "\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "model = model.to(device)\n",
    "criterion = criterion.to(device)\n",
    "\n",
    "optimize_model(model, train_iterator, valid_iterator, optimizer, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 887
    },
    "colab_type": "code",
    "id": "RT0u-cCjOFNg",
    "outputId": "9b0d6075-5238-4124-acf7-2a4a55aa4ca6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El modelo actual tiene 8,041,766 parámetros entrenables.\n",
      "Epoch: 01 | Epoch Time: 0m 9s\n",
      "\tTrain Loss: 0.284 | Train f1: 0.29 | Train precision: 0.33 | Train recall: 0.30\n",
      "\t Val. Loss: 0.303 |  Val. f1: 0.46 |  Val. precision: 0.51 | Val. recall: 0.49\n",
      "Epoch: 02 | Epoch Time: 0m 9s\n",
      "\tTrain Loss: 0.136 | Train f1: 0.57 | Train precision: 0.62 | Train recall: 0.58\n",
      "\t Val. Loss: 0.275 |  Val. f1: 0.59 |  Val. precision: 0.63 | Val. recall: 0.61\n",
      "Epoch: 03 | Epoch Time: 0m 9s\n",
      "\tTrain Loss: 0.088 | Train f1: 0.71 | Train precision: 0.75 | Train recall: 0.71\n",
      "\t Val. Loss: 0.281 |  Val. f1: 0.61 |  Val. precision: 0.66 | Val. recall: 0.64\n",
      "Epoch: 04 | Epoch Time: 0m 9s\n",
      "\tTrain Loss: 0.064 | Train f1: 0.77 | Train precision: 0.80 | Train recall: 0.77\n",
      "\t Val. Loss: 0.249 |  Val. f1: 0.63 |  Val. precision: 0.66 | Val. recall: 0.65\n",
      "Epoch: 05 | Epoch Time: 0m 9s\n",
      "\tTrain Loss: 0.051 | Train f1: 0.81 | Train precision: 0.83 | Train recall: 0.81\n",
      "\t Val. Loss: 0.287 |  Val. f1: 0.64 |  Val. precision: 0.67 | Val. recall: 0.66\n",
      "Epoch: 06 | Epoch Time: 0m 9s\n",
      "\tTrain Loss: 0.044 | Train f1: 0.82 | Train precision: 0.85 | Train recall: 0.83\n",
      "\t Val. Loss: 0.257 |  Val. f1: 0.63 |  Val. precision: 0.66 | Val. recall: 0.66\n",
      "Epoch: 07 | Epoch Time: 0m 9s\n",
      "\tTrain Loss: 0.037 | Train f1: 0.85 | Train precision: 0.86 | Train recall: 0.85\n",
      "\t Val. Loss: 0.320 |  Val. f1: 0.66 |  Val. precision: 0.69 | Val. recall: 0.67\n",
      "Epoch: 08 | Epoch Time: 0m 9s\n",
      "\tTrain Loss: 0.032 | Train f1: 0.86 | Train precision: 0.88 | Train recall: 0.86\n",
      "\t Val. Loss: 0.318 |  Val. f1: 0.61 |  Val. precision: 0.66 | Val. recall: 0.63\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/numpy/lib/function_base.py:393: RuntimeWarning: Mean of empty slice.\n",
      "  avg = a.mean(axis)\n",
      "/usr/local/lib/python3.6/dist-packages/numpy/core/_methods.py:161: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 09 | Epoch Time: 0m 9s\n",
      "\tTrain Loss: 0.031 | Train f1: nan | Train precision: nan | Train recall: nan\n",
      "\t Val. Loss: 0.350 |  Val. f1: 0.65 |  Val. precision: 0.68 | Val. recall: 0.67\n",
      "Epoch: 10 | Epoch Time: 0m 9s\n",
      "\tTrain Loss: 0.026 | Train f1: nan | Train precision: nan | Train recall: nan\n",
      "\t Val. Loss: 0.336 |  Val. f1: 0.65 |  Val. precision: 0.68 | Val. recall: 0.68\n",
      "Epoch: 11 | Epoch Time: 0m 9s\n",
      "\tTrain Loss: 0.025 | Train f1: 0.89 | Train precision: 0.90 | Train recall: 0.89\n",
      "\t Val. Loss: 0.343 |  Val. f1: 0.66 |  Val. precision: 0.68 | Val. recall: 0.68\n",
      "Epoch: 12 | Epoch Time: 0m 9s\n",
      "\tTrain Loss: 0.022 | Train f1: 0.89 | Train precision: 0.90 | Train recall: 0.89\n",
      "\t Val. Loss: 0.353 |  Val. f1: 0.64 |  Val. precision: 0.67 | Val. recall: 0.67\n",
      "Epoch: 13 | Epoch Time: 0m 9s\n",
      "\tTrain Loss: 0.019 | Train f1: 0.91 | Train precision: 0.91 | Train recall: 0.91\n",
      "\t Val. Loss: 0.331 |  Val. f1: 0.66 |  Val. precision: 0.69 | Val. recall: 0.69\n",
      "Epoch: 14 | Epoch Time: 0m 9s\n",
      "\tTrain Loss: 0.020 | Train f1: 0.91 | Train precision: 0.91 | Train recall: 0.91\n",
      "\t Val. Loss: 0.391 |  Val. f1: 0.65 |  Val. precision: 0.68 | Val. recall: 0.67\n",
      "\n",
      "Performance of best found Model:\n",
      "Val. Loss: 0.249 |  Val. f1: 0.63 | Val. precision: 0.66 | Val. recall: 0.65\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "model = elman_model\n",
    "model_name = elman_model_name\n",
    "criterion = baseline_criterion\n",
    "\n",
    "model.apply(init_weights)\n",
    "\n",
    "pre_trained_emb = torch.FloatTensor(TEXT.vocab.vectors)\n",
    "model.load_pretrained_embeddings(pre_trained_emb, True)\n",
    "print(f'El modelo actual tiene {count_parameters(model):,} parámetros entrenables.')\n",
    "\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "model = model.to(device)\n",
    "criterion = criterion.to(device)\n",
    "\n",
    "optimize_model(model, train_iterator, valid_iterator, optimizer, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 697
    },
    "colab_type": "code",
    "id": "Dbu4sdJrOI9s",
    "outputId": "5b453127-0b2b-4386-c33a-4fadcd1a05ad"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El modelo actual tiene 9,875,238 parámetros entrenables.\n",
      "Epoch: 01 | Epoch Time: 0m 9s\n",
      "\tTrain Loss: 0.223 | Train f1: 0.42 | Train precision: 0.47 | Train recall: 0.43\n",
      "\t Val. Loss: 0.309 |  Val. f1: 0.54 |  Val. precision: 0.58 | Val. recall: 0.58\n",
      "Epoch: 02 | Epoch Time: 0m 10s\n",
      "\tTrain Loss: 0.095 | Train f1: 0.69 | Train precision: 0.74 | Train recall: 0.69\n",
      "\t Val. Loss: 0.253 |  Val. f1: 0.62 |  Val. precision: 0.66 | Val. recall: 0.64\n",
      "Epoch: 03 | Epoch Time: 0m 9s\n",
      "\tTrain Loss: 0.059 | Train f1: 0.79 | Train precision: 0.81 | Train recall: 0.79\n",
      "\t Val. Loss: 0.274 |  Val. f1: 0.62 |  Val. precision: 0.65 | Val. recall: 0.65\n",
      "Epoch: 04 | Epoch Time: 0m 9s\n",
      "\tTrain Loss: 0.042 | Train f1: 0.83 | Train precision: 0.85 | Train recall: 0.83\n",
      "\t Val. Loss: 0.327 |  Val. f1: 0.69 |  Val. precision: 0.71 | Val. recall: 0.71\n",
      "Epoch: 05 | Epoch Time: 0m 10s\n",
      "\tTrain Loss: 0.035 | Train f1: 0.86 | Train precision: 0.87 | Train recall: 0.85\n",
      "\t Val. Loss: 0.299 |  Val. f1: 0.67 |  Val. precision: 0.70 | Val. recall: 0.69\n",
      "Epoch: 06 | Epoch Time: 0m 9s\n",
      "\tTrain Loss: 0.027 | Train f1: 0.88 | Train precision: 0.89 | Train recall: 0.88\n",
      "\t Val. Loss: 0.293 |  Val. f1: 0.67 |  Val. precision: 0.71 | Val. recall: 0.69\n",
      "Epoch: 07 | Epoch Time: 0m 9s\n",
      "\tTrain Loss: 0.023 | Train f1: 0.89 | Train precision: 0.90 | Train recall: 0.89\n",
      "\t Val. Loss: 0.317 |  Val. f1: 0.66 |  Val. precision: 0.69 | Val. recall: 0.68\n",
      "Epoch: 08 | Epoch Time: 0m 9s\n",
      "\tTrain Loss: 0.020 | Train f1: 0.90 | Train precision: 0.91 | Train recall: 0.90\n",
      "\t Val. Loss: 0.354 |  Val. f1: 0.66 |  Val. precision: 0.69 | Val. recall: 0.69\n",
      "Epoch: 09 | Epoch Time: 0m 9s\n",
      "\tTrain Loss: 0.018 | Train f1: 0.92 | Train precision: 0.93 | Train recall: 0.92\n",
      "\t Val. Loss: 0.315 |  Val. f1: 0.67 |  Val. precision: 0.70 | Val. recall: 0.69\n",
      "Epoch: 10 | Epoch Time: 0m 10s\n",
      "\tTrain Loss: 0.017 | Train f1: 0.92 | Train precision: 0.93 | Train recall: 0.92\n",
      "\t Val. Loss: 0.453 |  Val. f1: 0.65 |  Val. precision: 0.69 | Val. recall: 0.68\n",
      "Epoch: 11 | Epoch Time: 0m 9s\n",
      "\tTrain Loss: 0.015 | Train f1: 0.93 | Train precision: 0.94 | Train recall: 0.93\n",
      "\t Val. Loss: 0.428 |  Val. f1: 0.66 |  Val. precision: 0.69 | Val. recall: 0.68\n",
      "Epoch: 12 | Epoch Time: 0m 9s\n",
      "\tTrain Loss: 0.014 | Train f1: 0.93 | Train precision: 0.94 | Train recall: 0.93\n",
      "\t Val. Loss: 0.383 |  Val. f1: 0.66 |  Val. precision: 0.69 | Val. recall: 0.68\n",
      "\n",
      "Performance of best found Model:\n",
      "Val. Loss: 0.253 |  Val. f1: 0.62 | Val. precision: 0.66 | Val. recall: 0.64\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "model = gru_model1\n",
    "model_name = gru_model_name1\n",
    "criterion = baseline_criterion\n",
    "\n",
    "model.apply(init_weights)\n",
    "\n",
    "pre_trained_emb = torch.FloatTensor(TEXT.vocab.vectors)\n",
    "model.load_pretrained_embeddings(pre_trained_emb, True)\n",
    "print(f'El modelo actual tiene {count_parameters(model):,} parámetros entrenables.')\n",
    "\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "model = model.to(device)\n",
    "criterion = criterion.to(device)\n",
    "\n",
    "optimize_model(model, train_iterator, valid_iterator, optimizer, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 799
    },
    "colab_type": "code",
    "id": "6I9n5yzaOQP8",
    "outputId": "7e074815-b519-49d8-d073-36daa25dfbbe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El modelo actual tiene 9,875,238 parámetros entrenables.\n",
      "Epoch: 01 | Epoch Time: 0m 11s\n",
      "\tTrain Loss: 0.253 | Train f1: 0.37 | Train precision: 0.43 | Train recall: 0.37\n",
      "\t Val. Loss: 0.393 |  Val. f1: 0.50 |  Val. precision: 0.53 | Val. recall: 0.54\n",
      "Epoch: 02 | Epoch Time: 0m 11s\n",
      "\tTrain Loss: 0.113 | Train f1: 0.65 | Train precision: 0.69 | Train recall: 0.65\n",
      "\t Val. Loss: 0.281 |  Val. f1: 0.62 |  Val. precision: 0.66 | Val. recall: 0.64\n",
      "Epoch: 03 | Epoch Time: 0m 11s\n",
      "\tTrain Loss: 0.072 | Train f1: 0.75 | Train precision: 0.78 | Train recall: 0.75\n",
      "\t Val. Loss: 0.267 |  Val. f1: 0.63 |  Val. precision: 0.67 | Val. recall: 0.65\n",
      "Epoch: 04 | Epoch Time: 0m 11s\n",
      "\tTrain Loss: 0.053 | Train f1: 0.80 | Train precision: 0.83 | Train recall: 0.80\n",
      "\t Val. Loss: 0.258 |  Val. f1: 0.65 |  Val. precision: 0.68 | Val. recall: 0.66\n",
      "Epoch: 05 | Epoch Time: 0m 11s\n",
      "\tTrain Loss: 0.043 | Train f1: 0.83 | Train precision: 0.85 | Train recall: 0.83\n",
      "\t Val. Loss: 0.324 |  Val. f1: 0.65 |  Val. precision: 0.68 | Val. recall: 0.68\n",
      "Epoch: 06 | Epoch Time: 0m 11s\n",
      "\tTrain Loss: 0.035 | Train f1: 0.85 | Train precision: 0.86 | Train recall: 0.84\n",
      "\t Val. Loss: 0.352 |  Val. f1: 0.65 |  Val. precision: 0.68 | Val. recall: 0.67\n",
      "Epoch: 07 | Epoch Time: 0m 11s\n",
      "\tTrain Loss: 0.031 | Train f1: 0.86 | Train precision: 0.88 | Train recall: 0.86\n",
      "\t Val. Loss: 0.345 |  Val. f1: 0.66 |  Val. precision: 0.68 | Val. recall: 0.67\n",
      "Epoch: 08 | Epoch Time: 0m 11s\n",
      "\tTrain Loss: 0.026 | Train f1: 0.88 | Train precision: 0.90 | Train recall: 0.88\n",
      "\t Val. Loss: 0.375 |  Val. f1: 0.67 |  Val. precision: 0.70 | Val. recall: 0.70\n",
      "Epoch: 09 | Epoch Time: 0m 11s\n",
      "\tTrain Loss: 0.024 | Train f1: 0.89 | Train precision: 0.90 | Train recall: 0.89\n",
      "\t Val. Loss: 0.338 |  Val. f1: 0.66 |  Val. precision: 0.70 | Val. recall: 0.68\n",
      "Epoch: 10 | Epoch Time: 0m 11s\n",
      "\tTrain Loss: 0.022 | Train f1: 0.90 | Train precision: 0.91 | Train recall: 0.90\n",
      "\t Val. Loss: 0.347 |  Val. f1: 0.67 |  Val. precision: 0.70 | Val. recall: 0.69\n",
      "Epoch: 11 | Epoch Time: 0m 11s\n",
      "\tTrain Loss: 0.020 | Train f1: 0.91 | Train precision: 0.92 | Train recall: 0.90\n",
      "\t Val. Loss: 0.373 |  Val. f1: 0.67 |  Val. precision: 0.70 | Val. recall: 0.69\n",
      "Epoch: 12 | Epoch Time: 0m 11s\n",
      "\tTrain Loss: 0.020 | Train f1: 0.90 | Train precision: 0.91 | Train recall: 0.90\n",
      "\t Val. Loss: 0.393 |  Val. f1: 0.67 |  Val. precision: 0.69 | Val. recall: 0.69\n",
      "Epoch: 13 | Epoch Time: 0m 11s\n",
      "\tTrain Loss: 0.017 | Train f1: 0.92 | Train precision: 0.93 | Train recall: 0.92\n",
      "\t Val. Loss: 0.401 |  Val. f1: 0.65 |  Val. precision: 0.68 | Val. recall: 0.67\n",
      "Epoch: 14 | Epoch Time: 0m 11s\n",
      "\tTrain Loss: 0.017 | Train f1: 0.92 | Train precision: 0.93 | Train recall: 0.92\n",
      "\t Val. Loss: 0.426 |  Val. f1: 0.67 |  Val. precision: 0.69 | Val. recall: 0.69\n",
      "\n",
      "Performance of best found Model:\n",
      "Val. Loss: 0.258 |  Val. f1: 0.65 | Val. precision: 0.68 | Val. recall: 0.66\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "model = gru_model2\n",
    "model_name = gru_model_name2\n",
    "criterion = baseline_criterion\n",
    "\n",
    "model.apply(init_weights)\n",
    "\n",
    "pre_trained_emb = torch.FloatTensor(TEXT.vocab.vectors)\n",
    "model.load_pretrained_embeddings(pre_trained_emb, True)\n",
    "print(f'El modelo actual tiene {count_parameters(model):,} parámetros entrenables.')\n",
    "\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "model = model.to(device)\n",
    "criterion = criterion.to(device)\n",
    "\n",
    "optimize_model(model, train_iterator, valid_iterator, optimizer, criterion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uF1ysw_Kw6zz"
   },
   "source": [
    "\n",
    "### Predecir datos para la competencia\n",
    "\n",
    "Ahora, a partir de los datos de **test** y nuestro modelo entrenado, predeciremos las etiquetas que serán evaluadas en la competencia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-23T22:31:56.776563Z",
     "start_time": "2020-06-23T22:31:39.654525Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "1RBs3UU4wLk3"
   },
   "outputs": [],
   "source": [
    "def predict_labels(model, iterator, criterion, fields=fields):\n",
    "\n",
    "    # Extraemos los vocabularios.\n",
    "    text_field = fields[0][1]\n",
    "    nertags_field = fields[1][1]\n",
    "    tags_vocab = nertags_field.vocab.itos\n",
    "    words_vocab = text_field.vocab.itos\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    predictions = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        for batch in iterator:\n",
    "\n",
    "            text_batch = batch.text\n",
    "            text_batch = torch.transpose(text_batch, 0, 1).tolist()\n",
    "\n",
    "            # Predecir los tags de las sentences del batch\n",
    "            predictions_batch = model(batch.text)\n",
    "            predictions_batch = torch.transpose(predictions_batch, 0, 1)\n",
    "\n",
    "            # por cada oración predicha:\n",
    "            for sentence, sentence_prediction in zip(text_batch,\n",
    "                                                     predictions_batch):\n",
    "                for word_idx, word_predictions in zip(sentence,\n",
    "                                                      sentence_prediction):\n",
    "                    # Obtener el indice del tag con la probabilidad mas alta.\n",
    "                    argmax_index = word_predictions.topk(1)[1]\n",
    "\n",
    "                    current_tag = tags_vocab[argmax_index]\n",
    "                    # Obtenemos la palabra\n",
    "                    current_word = words_vocab[word_idx]\n",
    "\n",
    "                    if current_word != '<pad>':\n",
    "                        predictions.append([current_word, current_tag])\n",
    "\n",
    "\n",
    "    return predictions\n",
    "\n",
    "\n",
    "predictions = predict_labels(model, test_iterator, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "GyAOTPQp1S2r",
    "outputId": "595c8b0f-12ae-4ec8-f60b-f1b45d72ca10"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "51533"
      ]
     },
     "execution_count": 241,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YwQp1Ru8Oht8"
   },
   "source": [
    "### Generar el archivo para la submission\n",
    "\n",
    "No hay problema si aparecen unk en la salida. Estos no son relevantes para evaluarlos, usamos solo los tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-23T22:33:41.845955Z",
     "start_time": "2020-06-23T22:33:41.731717Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "RPfZkjJGkWyq"
   },
   "outputs": [],
   "source": [
    "import os, shutil\n",
    "\n",
    "if (os.path.isfile('./predictions.zip')):\n",
    "    os.remove('./predictions.zip')\n",
    "\n",
    "if (not os.path.isdir('./predictions')):\n",
    "    os.mkdir('./predictions')\n",
    "\n",
    "else:\n",
    "    # Eliminar predicciones anteriores:\n",
    "    shutil.rmtree('./predictions')\n",
    "    os.mkdir('./predictions')\n",
    "\n",
    "f = open('predictions/predictions.txt', 'w')\n",
    "for word, tag in predictions:\n",
    "    f.write(word + ' ' + tag + '\\n')\n",
    "f.write('\\n')\n",
    "f.close()\n",
    "\n",
    "a = shutil.make_archive('predictions', 'zip', './predictions')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-23T21:49:19.575711Z",
     "start_time": "2020-06-23T21:49:19.100486Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "k2PqvJAmTFWR"
   },
   "outputs": [],
   "source": [
    "%%script false\n",
    "# A veces no funciona a la primera. Ejecutar mas de una vez para obtener el archivo...\n",
    "from google.colab import files\n",
    "files.download('predictions.zip')  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LZEWJXrNaSIf"
   },
   "source": [
    "## Conclusiones\n",
    "\n",
    "\n",
    "\n",
    "..."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Tarea_2.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "12265109990b4fd0baf30cbd9b1d8dd2": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "66ddd53c2cb647e7ac0fdbea7ae7f68a": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "873b40d4d05d496e99d6fe9d040fc3c0": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_ac9dae42dc594402b132cfc53569157e",
       "IPY_MODEL_dc755770a50a4b209c131b1d8a7ce865"
      ],
      "layout": "IPY_MODEL_66ddd53c2cb647e7ac0fdbea7ae7f68a"
     }
    },
    "ac9dae42dc594402b132cfc53569157e": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_12265109990b4fd0baf30cbd9b1d8dd2",
      "max": 29171,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_cc9affc16ca14e9a957c1abd3b52a411",
      "value": 29171
     }
    },
    "cc9affc16ca14e9a957c1abd3b52a411": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "dc755770a50a4b209c131b1d8a7ce865": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f311d9d2b57d4619ab9b3c748d2ee15a",
      "placeholder": "​",
      "style": "IPY_MODEL_e63a97dc96fa434fbba5dd99650c2b2a",
      "value": " 29171/29171 [00:00&lt;00:00, 65274.41it/s]"
     }
    },
    "e63a97dc96fa434fbba5dd99650c2b2a": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "f311d9d2b57d4619ab9b3c748d2ee15a": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
