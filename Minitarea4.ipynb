{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "X4lL5hGw07yP"
   },
   "source": [
    "# Minitarea 4\n",
    "\n",
    "Por favor, en las respuestas de desarrollo expliquen lo que están haciendo. En las preguntas que son con desarrollo matemático pongan todos los pasos del cálculo (déjenlo bonito porfis :D).\n",
    "\n",
    "Usen $\\LaTeX$ para las fórmulas matemáticas.\n",
    "\n",
    "En la parte de programación pueden usar lo que quieran, pero la auxiliar 3 les puede servir de *inspiración*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bWXD3D7RYKJ-"
   },
   "source": [
    "# Hidden Markov Models (HMM), Maximum Entropy Markov Models (MEMM) and Conditional Random Field(CRF)\n",
    "\n",
    "### Pregunta 1 (1 pt)\n",
    "Para un problema de POS tagging se define el conjunto de etiquetas $S = \\{ D, N, V, P \\}$ y se tiene un Hidden Markov Model con los siguientes parámetros estimados a partir de un corpus de entrenamiento:\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "q(D|N,P) &= 0.4 \\\\\n",
    "q(D|w,P) &= 0 \\qquad \\forall w \\in S, w \\neq N \\\\\n",
    "e(the|D) &= 0.6\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "\n",
    "Luego para la oración: `Ella walks to the red house`, se tiene una tabla de programación dinámica con los siguientes valores:\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "\\pi(3,D,P)&=0.1\\\\\n",
    "\\pi(3,N,P)&=0.2\\\\\n",
    "\\pi(3,V,P)&=0.01\\\\\n",
    "\\pi(3,P,P)&=0.5\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "\n",
    "Con esta información, calcule el valor de $\\pi(4,P,D)$. Puede dejar el resultado expresado como una fracción.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5EzgysW9kGi-"
   },
   "source": [
    "**Respuesta**\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "\\pi(4,P,D)&=\\underset{w\\in S_2}{\\max} \\pi(3,w,P)\\cdot q(D|w,P)\\cdot e(the|D)\\\\\n",
    "&=\\pi(3,N,P)\\cdot q(D|N,P)\\cdot e(the|D)\\\\\n",
    "&=0.2\\cdot 0.4\\cdot 0.6\\\\\n",
    "&=0.048\n",
    "\\end{split}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oiwJb_vmkKLZ"
   },
   "source": [
    "### Pregunta 2 (0.5 pts)\n",
    "Comente  sobre las similitudes o diferencias entre los HMMs, MEMMs y CRFs. Para esto, responda las siguientes preguntas.\n",
    "* ¿Para qué tipo de tarea sirven? Dé dos ejemplo de este tipo de tarea y descríbalos brevemente.\n",
    "* ¿Qué modelos usan features? ¿Qué ventajas conlleva esto?\n",
    "* ¿Cómo maneja cada uno de los modelos las palabras con baja frecuencia en el set de train?\n",
    "* ¿Qué le permite a los CRF realizar decisiones globales? ¿Qué diferencia con respecto a los MEMMs permite lograr esto? ¿Por qué los HMMs tampoco son capaces de tomar decisiones globales?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "P9h5ow8OWF7y"
   },
   "source": [
    "**Respuesta**\n",
    "\n",
    "1. Los modelos HMMs, MEMMs y CRFs son utilizados en la tarea de *sequence tagging*, la cual consisiste en la asignación de etiquetas discretas a elementos discretos en una secuencia. Un ejemplo conocido de *sequence tagging* es el *Part-of-Speech tagging*, el cual consiste en asignar etiquetas gramáticas a las palabras de acuerdo a su definición y contexto (ej. sustantivos, adjetivos, verbos, advervios, etc.). Otro ejemplo de *sequence tagging* es el *Name Entity Recognition*, el cual consiste en detectar y clasificar entidades en una oración (ej. nombres de personas, organizaciones, lugares, etc.).\n",
    "2. Los modelos que utilizan feaures son el MEMM y el CRF, los cuales permiten utilizar representaciones mucho más enriquecidas, pudiendo incorporar dependencias de las etiquetas respecto a la secuencia completa de palabras o incluso permitiendo incorporar características ortogŕaficas de una palabra o de su vecindad próxima de manera explícita. \n",
    "3. El modelo HMM particiona el vocabulario en un conjunto de palabras frecuentes (de recurrencia mayor o igual a 5) y en uno de palabras infrecuentes, mapeando cada palabra infrecuente a un conjunto pequeño y finito de palabras dependiendo de los prefijos, sufijos, etc., en cambio, los modelos MEMM y CRF no requieren de un manejo especial de palabras de baja frecuencia ya que utilizan los features o características de las palabras como representación de la mismas, por tanto, su frecuencia no presentará una injerencia totalmente determinante en la representación de estas (a excepción de posibles características específicas que dependendan de ello), y por ende, en la etapa de entrenamiento de los modelos.\n",
    "4. Sea $x_j$ y $s_j$ el elemento *j*-ésima de la secuencia y su respectivo *tag*, el modelo MEMM utiliza vectores de características dependientes de la transición inmediata de los *tags* $\\vec{\\phi}(x_{1:m},s_{j-1},s_j)$, codificando la información relevante de la transición de *tags* $s_{j-1}$ a $s_{j}$ (información local), mientras que el modelo CRF utiliza vectores de características correspondientes a la suma de todos los vectores locales $\\vec{\\mathbb{\\Phi}}(x_{1:m},s_{1:m})=\\sum_{j=1}^m \\vec{\\phi}(x_{1:m},s_{j-1},s_j)$, codificando la información relevante de todas las transiciones (información global), permitiendo así, realizar decisiones globales. De manera similar, el modelo HMM tampoco es capaz de tomar decisiones globales debido a la hipotesis de independencia de los elementos la secuencia de *tags* modelados como una cadena de Markov."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ClRAHR95Y8aB"
   },
   "source": [
    "# Convolutional Neural Networks\n",
    "### Pregunta 3 (1 pt)\n",
    "\n",
    "Considere la frase $w_{1..7}=$ `El agua moja y el fuego quema` $=[El, agua, moja, y, el, fuego, quema]$.\n",
    "\n",
    "La siguiente matriz de embeddings, donde la i-ésima fila corresponde al vector de embedding de la i-ésima palabra, ordenadas segun aparecen en la frase. (vectores de largo 2).\n",
    "\\begin{equation}\n",
    "E = \\begin{pmatrix}\n",
    "2 & 2\\\\\n",
    "0 & -2\\\\\n",
    "0 & 1\\\\\n",
    "-2 & 1\\\\\n",
    "-2 & 0\\\\\n",
    "2 & -1\\\\\n",
    "0 & 2\n",
    "\\end{pmatrix}\n",
    "\\end{equation}\n",
    "\n",
    "Los siguientes 3 filtros\n",
    "\\begin{equation}\n",
    "U = \\begin{pmatrix}\n",
    "-1 & -1 & 0\\\\\n",
    "1 & 1 & 0\\\\\n",
    "0 & 0 & -1\\\\\n",
    "-1 & -1 & -1\\\\\n",
    "-1 & -1 & 1\\\\\n",
    "1 & 0 & -1\n",
    "\\end{pmatrix}\n",
    "\\end{equation}\n",
    "\n",
    "Y la función de activación\n",
    "\\begin{equation}\n",
    "tanh = \\frac{e^{2x} - 1}{e^{2x} + 1}\n",
    "\\end{equation}\n",
    "\n",
    "Usando estos paramátros calcule manualmente la representación (vector) resultante de aplicar la operación de convolución (sin padding) + max pooling.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SlQ30Arkq0u4"
   },
   "source": [
    "**Respuesta**\n",
    "\n",
    "Como $E\\in \\mathbb{R}^{7\\times 2}$ y $U\\in \\mathbb{R}^{6\\times 3}$, se deduce que $k=3$. De este modo $\\vec{x}_i=\\oplus (\\vec{w}_{i:i+k-1})=\\oplus (\\vec{w}_{i:i+2})\\in\\mathbb{R}^6$:\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "\\vec{x}_1=\\oplus (\\vec{w}_{1:3})&=[\\vec{w}_1,\\vec{w}_2,\\vec{w}_3]=\\begin{pmatrix}2 & 2 & 0 & -2 & 0 & 1\\end{pmatrix}\\\\\n",
    "\\vec{x}_2=\\oplus (\\vec{w}_{2:4})&=[\\vec{w}_2,\\vec{w}_3,\\vec{w}_4]=\\begin{pmatrix}0 & -2&0 & 1&-2 & 1\\end{pmatrix}\\\\\n",
    "\\vec{x}_3=\\oplus (\\vec{w}_{3:5})&=[\\vec{w}_3,\\vec{w}_4,\\vec{w}_5]=\\begin{pmatrix}0 & 1&-2 & 1&-2 & 0\\end{pmatrix}\\\\\n",
    "\\vec{x}_4=\\oplus (\\vec{w}_{4:6})&=[\\vec{w}_4,\\vec{w}_5,\\vec{w}_6]=\\begin{pmatrix}-2 & 1&-2 & 0&2 & -1\\end{pmatrix}\\\\\n",
    "\\vec{x}_5=\\oplus (\\vec{w}_{5:7})&=[\\vec{w}_5,\\vec{w}_6,\\vec{w}_7]=\\begin{pmatrix}-2 & 0&2 & -1&0 & 2\\end{pmatrix}\\\\\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "\n",
    "Luego, $\\vec{p}_i=tanh(\\vec{x}_i\\cdot U)$, por lo tanto:\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "\\vec{p}_1&=tanh(\\vec{x}_1\\cdot U)=tanh\\big(\\begin{pmatrix}3&2&1\\end{pmatrix}\\big)=\\begin{pmatrix}0.99505475&0.96402758 &0.76159416\\end{pmatrix}\\\\\n",
    "\\vec{p}_2&=tanh(\\vec{x}_2\\cdot U)=tanh\\big(\\begin{pmatrix}0&-1&-4\\end{pmatrix}\\big)=\\begin{pmatrix} 0   &-0.76159416& -0.9993293\\end{pmatrix}\\\\\n",
    "\\vec{p}_3&=tanh(\\vec{x}_3\\cdot U)=tanh\\big(\\begin{pmatrix}2&2&-1\\end{pmatrix}\\big)=\\begin{pmatrix}0.96402758  &0.96402758 &-0.76159416\\end{pmatrix}\\\\\n",
    "\\vec{p}_4&=tanh(\\vec{x}_4\\cdot U)=tanh\\big(\\begin{pmatrix}0&1&5\\end{pmatrix}\\big)=\\begin{pmatrix}0         &0.76159416& 0.9999092\\end{pmatrix}\\\\\n",
    "\\vec{p}_5&=tanh(\\vec{x}_5\\cdot U)=tanh\\big(\\begin{pmatrix}5&3&-3\\end{pmatrix}\\big)=\\begin{pmatrix} 0.9999092   &0.99505475 &-0.99505475\\end{pmatrix}\\\\\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "\n",
    "Finalmente, aplicando $maxpooling$:\n",
    "\n",
    "\\begin{equation}\n",
    "\\vec{c}=\\begin{pmatrix}\\max\\vec{p}_{1[1]},...,\\vec{p}_{5[1]}&\\max\\vec{p}_{1[2]},...,\\vec{p}_{5[2]}&\\max\\vec{p}_{1[3]},...,\\vec{p}_{5[3]}\\end{pmatrix}=\\begin{pmatrix}0.9999092&0.99505475&0.9999092\\end{pmatrix}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tj1V_sAzZCHY"
   },
   "source": [
    "# Recurrent Neural Networks\n",
    "### Pregunta 4 (1 pt)\n",
    "Sea la siguiente oración: `El perro ladra` representada como una secuencia de vectores $x1,x2,x3$. Donde cada vector corresponde al word embedding de cada palabra, que a la vez es un vector de dos dimensiones\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "x1 &= Embed(El)    &= [1, 0] \\\\\n",
    "x2 &= Embed(perro) &= [-1, 1] \\\\\n",
    "x3 &= Embed(ladra) &= [1,1]\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "\n",
    "Se tiene una red recurrente Elman: \n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "\\vec{s}_i &= R_{SRNN}\\left (\\vec{x}_i, \\vec{s}_{i-1}\\right ) = g \\left (\\vec{s}_{i-1}W^s + \\vec{x}_i W^x + \\vec{b}\\right ) \\\\\n",
    "\\vec{y}_i &= O_{SRNN}\\left(\\vec{s}_i\\right) = \\vec{s}_i \\\\\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "Con\n",
    "\\begin{equation}\n",
    "\\vec{s}_i, \\vec{y}_i \\in \\mathbb{R}^{d_s}, \\quad \\vec{x}_i \\in \\mathbb{R}^{d_x}, \\quad W^x \\in \\mathbb{R}^{d_x \\times d_s}, \\quad W^s \\in \\mathbb{R}^{d_s \\times d_s}, \\quad \\vec{b} \\in \\mathbb{R}^{d_s}\n",
    "\\end{equation}\n",
    "y donde los vectores de estado $s_i$ son de tres dimensiones, $ds= 3$.\n",
    "\n",
    "Sea\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "\\vec{s}_0 &= [0,0,0]\\\\\n",
    "W^x &= \\begin{pmatrix}\n",
    "0 &  0 & 1\\\\\n",
    "1 & -1 & 0\n",
    "\\end{pmatrix} \\\\\n",
    "W^s &= \\begin{pmatrix}\n",
    "1 & 0 &  1\\\\\n",
    "0 & 1 & -1\\\\\n",
    "1 & 1 &  1\n",
    "\\end{pmatrix} \\\\\n",
    "\\vec{b} &= [0, 0, 0] \\\\\n",
    "g(x) &= ReLu(x) = max(0, x)\n",
    "\\end{split}\n",
    "\\end{equation}\n",
    "\n",
    "<br>\n",
    "\n",
    "Calcule manualmente los valores de $\\vec{s}_1, \\vec{s}_2,\\vec{s}_3$ y de $\\vec{y}_1, \\vec{y}_2,\\vec{y}_3$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7M7sqIQV-Q3a"
   },
   "source": [
    "**Respuesta**\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "\\vec{s}_1&=ReLU\\big(\\vec{s}_0W^s,\\vec{x}_1W^x,\\vec{b}\\big)=ReLU\\big([0,0,0]+[0,0,1]+[0,0,0]\\big)=ReLU\\big([0,0,1]\\big)=[0,0,1]\\\\\n",
    "\\vec{y}_1&=\\vec{s}_1=[0,0,1]\\\\\n",
    "\\vec{s}_2&=ReLU\\big(\\vec{s}_1W^s,\\vec{x}_2W^x,\\vec{b}\\big)=ReLU\\big([1,1,1]+[1,-1,-1]+[0,0,0]\\big)=ReLU\\big([2,0,0]\\big)=[2,0,0]\\\\\n",
    "\\vec{y}_2&=\\vec{s}_2=[2,0,0]\\\\\n",
    "\\vec{s}_3&=ReLU\\big(\\vec{s}_2W^s,\\vec{x}_3W^x,\\vec{b}\\big)=ReLU\\big([2,0,2]+[1,-1,1]+[0,0,0]\\big)=ReLU\\big([3,-1,3]\\big)=[3,0,3]\\\\\n",
    "\\vec{y}_3&=\\vec{s}_3=[3,0,3]\\\\\n",
    "\\end{split}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "W4rAT6ELxRZW"
   },
   "source": [
    "### Pregunta 5 (0.5 pts)\n",
    "¿De qué forma las RNN y las CNN logran aprender representaciones específicas\n",
    "para la tarea objetivo? Compare la forma en que las RNN y las CNN aprenden con los modelos que usan *features* diseñadas manualmente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "b6AXbQSgA_t8"
   },
   "source": [
    "**Respuesta**\n",
    "\n",
    "Los modelos RNN y CNN aprenden representaciones específicas directamente de los datos incorporando las dependencias temporales de las secuencias de entrenamiento para generar *features* que codifican los patrones de mayor relevancia del conjunto de datos utilizado para la resolución de una determinada tarea (mediante operaciones de convolución con kernels de tamaño fijo para el caso CNN y operaciones de recurrencia mediante actualización de estados para el caso RNN), sin necedidad de diseñar o especificar manualmente los *features* relevantes de manera previa (modelos de que utilizan *features* manuales), eliminando el sesgo de diseño. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qxQIuO8axTUa"
   },
   "source": [
    "# Redes neuronales con PyTorch\n",
    "### Pregunta 6 (2 pts)\n",
    "En esta parte van a tener que implementar una red neuronal Feed Forward. Además, deberán entrenar el modelo usando uno de los datasets de TorchText. En la sección de la respuesta hay un esqueleto de lo que deben hacer, deberán completar los metodos del modelo y la parte asociada al entrenamiento la deben implementar ustedes. De todas formas, como les mencionamos en las auxiliares, el proceso de entrenamiento es bastante estándar, así que se pueden guiar en gran medida por los ejemplos que hemos visto y los que vamos a ver en las próximas auxiliares.\n",
    "\n",
    "### Bonus (0.5 pts)\n",
    "Agregue a la arquitectura una capa convolucional, para esto debe registrar el parametro $U$ en la red y realizar el computo de la convolución en el metodo forward de la red.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LVKEaQXZ3eGl"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install --upgrade torchtext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 243
    },
    "colab_type": "code",
    "id": "dJ-wrzFO5mCC",
    "outputId": "ae7d3d70-ba98-4d73-9556-86cca13e4a26"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "120000lines [00:05, 23599.73lines/s]\n",
      "120000lines [00:11, 10694.99lines/s]\n",
      "7600lines [00:00, 10995.61lines/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Un ejemplo aleatorio\n",
      " (3, tensor([  206,   155,   225,   531,   158,   541,  2305,    21, 12120,    24,\n",
      "           74,    14,    28,    15,    16,   206,    42,     6,   260,   256,\n",
      "            2,    28,     2,   219,     2,  2533,     2,  2510,    81, 10243,\n",
      "            2,   138,  2530,  2526,  2529,   206,     2,   502,   258,   155,\n",
      "            5,   374, 40539,    20,     6,  5583,   119,   158,     4,  1211,\n",
      "            5,   679,   637,     9, 10560,   134,   517,   224,   608,    11,\n",
      "           47,  4872,   920,     8,     3,   127,     4,     3,    55,    27,\n",
      "           11,    58,     2]))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "from random import choice\n",
    "from pprint import pprint\n",
    "from itertools import zip_longest\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchtext.datasets import AG_NEWS\n",
    "\n",
    "os.makedirs(\".data\", exist_ok=True)\n",
    "train_dataset, test_dataset = AG_NEWS(root=\".data\")\n",
    "\n",
    "print(\"\\nUn ejemplo aleatorio\\n\", choice(train_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 104
    },
    "colab_type": "code",
    "id": "wSHn-hYi8XN8",
    "outputId": "aab971b0-ed6f-446b-f951-98a6780d7069"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0, 1, 2, 3}\n",
      "('anti-government strike cripples bangladesh shops and schools were closed '\n",
      " 'across bangladesh on tuesday after the country #39 s main opposition party '\n",
      " 'called a general strike to protest a weekend grenade attack that killed 20 '\n",
      " 'people and wounded more than 300 .')\n"
     ]
    }
   ],
   "source": [
    "# Informacion relevante del dataset\n",
    "vocab = train_dataset.get_vocab()\n",
    "labels = train_dataset.get_labels()\n",
    "# El nombre de cada label lo pueden encontrar aqui\n",
    "# https://pytorch.org/text/datasets.html#ag-news, aunque no es necesario para \n",
    "# clasificar\n",
    "print(labels)\n",
    "# Un ejemplo convertido a texto\n",
    "pprint(\" \".join(vocab.itos[token] for token in choice(train_dataset)[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IXngUm9HxKvA"
   },
   "outputs": [],
   "source": [
    "# Model\n",
    "class CNNClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, num_class, hidden_size, pad_idx):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim, pad_idx)\n",
    "        self.conv = nn.Conv1d(\n",
    "            in_channels=1,\n",
    "            out_channels=hidden_size,\n",
    "            kernel_size=3*embed_dim,\n",
    "            stride=embed_dim\n",
    "        )\n",
    "        self.fc = nn.Linear(hidden_size, num_class)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Embedding\n",
    "        x = self.embedding(x)\n",
    "        # Words concatenation\n",
    "        x = x.view(x.shape[0], 1, -1)\n",
    "        # Convolution\n",
    "        x = F.relu(self.conv(x))\n",
    "        # Max-pooling\n",
    "        x = x.max(dim=-1).values\n",
    "        # FC\n",
    "        x = self.fc(x)\n",
    "        # Softmax\n",
    "        x = torch.softmax(x, -1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mPHyqgId96ra"
   },
   "outputs": [],
   "source": [
    "# Utils\n",
    "from itertools import zip_longest\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def generate_batch(batch):\n",
    "    return (\n",
    "        torch.tensor([item[0] for item in batch]),\n",
    "        torch.tensor(\n",
    "            list(\n",
    "                zip(\n",
    "                    *zip_longest(\n",
    "                        *[item[1] for item in batch], fillvalue=vocab[\"<pad>\"]\n",
    "                    )\n",
    "                )\n",
    "            )\n",
    "        ),\n",
    "    )\n",
    "\n",
    "def train_func(train_dataset):\n",
    "    train_loss = 0\n",
    "    train_acc = 0\n",
    "    data = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=generate_batch)\n",
    "    for i, (cls, text) in enumerate(data):\n",
    "        optimizer.zero_grad()\n",
    "        cls, text = cls.to(device), text.to(device)\n",
    "        output = model(text)\n",
    "        loss = criterion(output, cls)\n",
    "        train_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_acc += (output.argmax(1) == cls).sum().item()\n",
    "    return train_loss / len(train_dataset), train_acc / len(train_dataset)\n",
    "\n",
    "def test(test_dataset):\n",
    "    test_loss = 0\n",
    "    acc = 0\n",
    "    data = DataLoader(test_dataset, batch_size=BATCH_SIZE, collate_fn=generate_batch)\n",
    "    for cls, text in data:\n",
    "        cls, text = cls.to(device), text.to(device)\n",
    "        with torch.no_grad():\n",
    "            output = model(text)\n",
    "            loss = criterion(output, cls)\n",
    "            test_loss += loss.item()\n",
    "            acc += (output.argmax(1) == cls).sum().item()\n",
    "    return test_loss / len(test_dataset), acc / len(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 278
    },
    "colab_type": "code",
    "id": "KxCHuhfAIPY3",
    "outputId": "419e831f-edd3-4f5d-ff95-bb16ce6f0313"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1  | time in 0 minutes, 34 seconds\n",
      "\tLoss: 0.0313(train)\t| \tAcc: 74.4%(train)\n",
      "\tLoss: 0.0283(valid)\t| \tAcc: 84.5%(valid)\n",
      "Epoch: 2  | time in 0 minutes, 34 seconds\n",
      "\tLoss: 0.0274(train)\t| \tAcc: 86.8%(train)\n",
      "\tLoss: 0.0282(valid)\t| \tAcc: 83.9%(valid)\n",
      "Epoch: 3  | time in 0 minutes, 34 seconds\n",
      "\tLoss: 0.0267(train)\t| \tAcc: 89.2%(train)\n",
      "\tLoss: 0.0278(valid)\t| \tAcc: 85.6%(valid)\n",
      "Epoch: 4  | time in 0 minutes, 34 seconds\n",
      "\tLoss: 0.0262(train)\t| \tAcc: 90.7%(train)\n",
      "\tLoss: 0.0277(valid)\t| \tAcc: 85.7%(valid)\n",
      "Epoch: 5  | time in 0 minutes, 34 seconds\n",
      "\tLoss: 0.0259(train)\t| \tAcc: 91.7%(train)\n",
      "\tLoss: 0.0274(valid)\t| \tAcc: 86.6%(valid)\n"
     ]
    }
   ],
   "source": [
    "N_EPOCHS = 5\n",
    "LEARN_RATE = 0.1\n",
    "STEP_SIZE = 1\n",
    "BATCH_SIZE = 32\n",
    "EMBED_DIM = 100\n",
    "HIDDEN_SIZE = 200\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = CNNClassifier(\n",
    "    vocab_size=len(vocab),\n",
    "    embed_dim=EMBED_DIM,\n",
    "    num_class=len(labels),\n",
    "    hidden_size=HIDDEN_SIZE,\n",
    "    pad_idx=vocab[\"<pad>\"],\n",
    ").to(device)\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss().to(device)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=LEARN_RATE)\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    start_time = time.time()\n",
    "    train_loss, train_acc = train_func(train_dataset)\n",
    "    valid_loss, valid_acc = test(test_dataset)\n",
    "    secs = int(time.time() - start_time)\n",
    "    mins = secs // 60\n",
    "    secs = secs % 60\n",
    "    print(f\"Epoch: {epoch + 1}\", f\" | time in {mins} minutes, {secs} seconds\")\n",
    "    print(f\"\\tLoss: {train_loss:.4f}(train)\\t|\", f\"\\tAcc: {train_acc * 100:.1f}%(train)\")\n",
    "    print(f\"\\tLoss: {valid_loss:.4f}(valid)\\t|\", f\"\\tAcc: {valid_acc * 100:.1f}%(valid)\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "minitarea4.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
